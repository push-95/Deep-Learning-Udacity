{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def exec_graph(graph):\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "      batches = train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      mean_loss += l\n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          for _ in range(5):\n",
    "            feed = sample(random_distribution())\n",
    "            sentence = characters(feed)[0]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = sample(prediction)\n",
    "              sentence += characters(feed)[0]\n",
    "            print(sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298081 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "obe ckt  fyq d i  oiepiyuebbewii whvuy berniw htjdmehmbfe vunrqdesndxr debjqimop\n",
      "smxj eoczekebedn woutrpbaztaegn i idzhdxetkrcjkjktqeaelpjx c nhuumwile  uepiqefb\n",
      "d hp j tnk faaxdeloa ijappzo e rtw yvdpk wlghdodizp e kureihvserki gltocgl tvp i\n",
      "sbxi qnc neimanwtgrrngjqsuotwnzcet ir bh  cfbthz ay kkcxihofowfv pqdciiiowp  tre\n",
      "gssbo mgbjkhmftiene pabavir gjkxkf fapkco yjtrqdiiopt jezuhzeqa ap d vd  obxiype\n",
      "================================================================================\n",
      "Validation set perplexity: 20.39\n",
      "Average loss at step 100: 2.598796 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.96\n",
      "Validation set perplexity: 10.39\n",
      "Average loss at step 200: 2.246814 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.67\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 300: 2.091723 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 400: 1.990674 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 500: 1.927185 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 600: 1.896470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 700: 1.851411 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 800: 1.808565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 900: 1.821403 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1000: 1.817527 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "ty skake in brines gurile the nenerally hover operes in the eifcle stoplatimale \n",
      "ary one nine two one districe will exust in necean nole of the ceeses thoud fele\n",
      "cord afcuale pegriatibe evemble rulers whice on the somecs resorusa jesteries fo\n",
      "ther and senceviltyovk of the expupce buill of countric oppted a sovellens in so\n",
      "nity many the seven mode the ketwe ofques it referphiel fate in qurind he deteri\n",
      "================================================================================\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1100: 1.769384 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1200: 1.746164 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1300: 1.728311 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1400: 1.743247 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1500: 1.731796 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1600: 1.745233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1700: 1.711657 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1800: 1.671800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1900: 1.647656 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2000: 1.694000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "vigned and to na more rebort be with the diaply the ending semer indistine to tr\n",
      "x foollmodon over houthia topkes and india with one ex a so heads user of willed\n",
      "jogk myp s news and digiland to the land instrakh contince canainers mpinoyal ro\n",
      "und uslatee cocients was spective is to three four gever see see three tall by d\n",
      "y he south on the basilitionity english on us dianions of suriand s betaceshdem \n",
      "================================================================================\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2100: 1.686093 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2200: 1.679989 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2300: 1.643219 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2400: 1.660676 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2500: 1.685355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2600: 1.656962 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2700: 1.661229 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2800: 1.653855 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2900: 1.652073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3000: 1.652483 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "hissni thus and bill university witelate were a winger was granch nivadare bestl\n",
      "s actronation which to henry part oftenn seven ravitiestorod be almov aircalated\n",
      "qued air from lis terrifise the plattan to misbistoring to actions deriticalic m\n",
      "qued pether and its called by the grander of techst in south his looduring entur\n",
      "ing sombures which he as defies the klan from the tomitationally oppraviiar decc\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3100: 1.630470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3200: 1.650242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3300: 1.641471 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3400: 1.673536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3500: 1.662057 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3600: 1.670433 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3700: 1.652537 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3800: 1.647997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3900: 1.643392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4000: 1.655328 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "um consideriation one nine eight eight three three and spanm to hemsolemed regua\n",
      "way s tabrian ssemande us mossions same meltoce may ked catevh there and and had\n",
      "ga lathish crewses or ar of hemes rime as contreesel demono or several grout ami\n",
      "tion be mass untarees there coldilouored woutes prekente of perfired of suppores\n",
      "yed gition ruped chiess procedent mo eltssoka that the deparged paco history dou\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4100: 1.638242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4200: 1.641002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4300: 1.615440 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4400: 1.615044 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4500: 1.619281 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4600: 1.614882 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4700: 1.630376 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4800: 1.633276 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4900: 1.636389 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5000: 1.609614 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "han roins on tergare fovensiver than but brought entromad yasb yean pibre one ni\n",
      "land land one eight the rounned trability when he duestahrist nate experied haft\n",
      "gernally other exames hanfrod hosm esmilitas one hoca with a free last they and \n",
      "go of an yeery muni f if eleht a rathe it waker untill g useine there are wherh \n",
      "ing instaters in time sitetured are many c bern regalally a two one nine two six\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5100: 1.609095 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5200: 1.592874 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5300: 1.585404 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5400: 1.583089 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5500: 1.572258 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5600: 1.584420 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5700: 1.571009 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5800: 1.586089 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5900: 1.577599 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6000: 1.547350 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "t contradd of the get is as or recrraght is a hiptory is nocase paye fierdon the\n",
      "s skiman all two ki pre in last a had the balta confored on its monnagificalisme\n",
      "opist the largenges the ffluby of the links bcculto all a city demilred nearman \n",
      "x their supony establicus i in one nine eight five zero three are and extuure in\n",
      "onned by neem come two five the wlaricent dreasii of a hip elsot one palation do\n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6100: 1.569970 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6200: 1.535919 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6300: 1.550001 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6400: 1.541213 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6500: 1.558860 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6600: 1.601116 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6700: 1.583068 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6800: 1.606957 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6900: 1.586505 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 7000: 1.578162 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "ken selter melitay of its race five releas two zero four seven thours inlts to t\n",
      "jaciation theogest convertor rudute the with the other to list for alqoentation \n",
      "ing to was created of movedolo samons to witt seriered geogrold adainly as besse\n",
      "ubed secise also starus along two had churzation and next lefficual develotions \n",
      "d paper wails certhe place with aight except species quental bu costurs pross he\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "CPU times: user 5min 47s, sys: 7min 16s, total: 13min 4s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check at each step that all the unoptimized and optimized matrices share the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # big matrix\n",
    "  ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "\n",
    "  # small matrices\n",
    "  ix = ifcox[:, :num_nodes]\n",
    "  fx = ifcox[:, num_nodes:2*num_nodes]\n",
    "  cx = ifcox[:, 2*num_nodes:3*num_nodes]\n",
    "  ox = ifcox[:, 3*num_nodes:]\n",
    "    \n",
    "  im = ifcom[:, :num_nodes]\n",
    "  fm = ifcom[:, num_nodes:2*num_nodes]\n",
    "  cm = ifcom[:, 2*num_nodes:3*num_nodes]\n",
    "  om = ifcom[:, 3*num_nodes:]\n",
    "    \n",
    "  ib = ifcob[:, :num_nodes]\n",
    "  fb = ifcob[:, num_nodes:2*num_nodes]\n",
    "  cb = ifcob[:, 2*num_nodes:3*num_nodes]\n",
    "  ob = ifcob[:, 3*num_nodes:]\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate_1 = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate_1 = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update_1 = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state_1 = forget_gate_1 * state + input_gate_1 * tf.tanh(update_1)\n",
    "    output_gate_1 = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    \n",
    "    all_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate_2 = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "    forget_gate_2 = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "    update_2 = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    state_2 = forget_gate_2 * state + input_gate_2 * tf.tanh(update_2)\n",
    "    output_gate_2 = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        \n",
    "    return [(output_gate_2 * tf.tanh(state_2), state_2),\n",
    "            [('input', input_gate_1, input_gate_2),\n",
    "             ('forget', forget_gate_1, forget_gate_2),\n",
    "             ('update', update_1, update_2),\n",
    "             ('state', state_1, state_2),\n",
    "             ('output', output_gate_1, output_gate_2)]]\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  equals = list()\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    out = lstm_cell(i, output, state)\n",
    "    output, state = out[0]\n",
    "    equals.extend(out[1])\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  out = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  sample_output, sample_state = out[0]\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.290297 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.85\n",
      "================================================================================\n",
      "g cnnrpwhid ey v hehtt c juajnhfrmtbv reeae d mubeb o cpfrgeboybrmkaido   wm  mi\n",
      "tlymtfsrs t hoksrhtperhdte  l bp  mzew selsqrr phhe   o l  hhd ea rzr miqnxstkct\n",
      "mciysdrofyikla xweeeerpti hohchptrotjou zzzal ms nk eestnzwlth yzixeagkiewn o tt\n",
      "trjreo putosk   qk hthvcqwuniytifestshzpsdil do lmtno enw fovxopmzhpzyxwejtsrcec\n",
      "xleecyidbu ei t  bvkn hao  w eaeqvh sne v i iragoepduwo tssf ddqrtyyvtcd aizieec\n",
      "================================================================================\n",
      "Validation set perplexity: 20.08\n",
      "Average loss at step 100: 2.584401 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.71\n",
      "Validation set perplexity: 10.46\n",
      "Average loss at step 200: 2.245747 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.33\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 300: 2.084919 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 400: 2.027262 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 500: 1.975307 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 600: 1.895990 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 700: 1.869952 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 800: 1.867113 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 900: 1.846130 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 1000: 1.845354 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "================================================================================\n",
      "mink scventrieckonahy sistelo koultar victian be relams extraatrots bo s the pre\n",
      "stwis the lycanotic supplism naso unontar que ustrect is hore guing thon on a ga\n",
      "ralially munor uch distats not compuser in fos of howenwan these lekbunded the e\n",
      "ic beqreds and borx treg ismmate a hiseonory of sexphic couls tuinne spricam ann\n",
      "fing the oftrmerapo of a prymonse by lowse renthing in the breen bore one usian \n",
      "================================================================================\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1100: 1.802688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 1200: 1.767071 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1300: 1.759659 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1400: 1.761371 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1500: 1.750067 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1600: 1.732771 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1700: 1.715786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1800: 1.689402 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1900: 1.694337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 2000: 1.681693 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "fic kr to restracts losain baser invest a gearkal his byre rbpire these remord s\n",
      "qest one zive five one eight eight certs extworo tential lenn s eakious his four\n",
      "h cosduset is a has meversed and logk to allise and to ke secture influar most o\n",
      "wess moning of the all selturys wrytfets some puarded to actives and schout runt\n",
      "houn and as and by the islbe any ewle of the becheron s compreticul experal is o\n",
      "================================================================================\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 2100: 1.685867 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2200: 1.704203 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2300: 1.702867 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2400: 1.682320 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2500: 1.688932 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2600: 1.671596 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2700: 1.684640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2800: 1.679477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2900: 1.673977 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 3000: 1.683827 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "reigh savel bearn two three by chilia distries to it sarfiel is to corrical betw\n",
      "thed that wher warilnement of the auren an the preacriv lecttert s everact of un\n",
      "xill less tim over latan muct eft control in offin kare westoluc bust computbu  \n",
      "le other m out well most oflam persop of skbske develop a vayach ibs only but fo\n",
      "d rusabetation actorcy tranvation of where sur bor abhish for tair and a volizio\n",
      "================================================================================\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 3100: 1.656407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3200: 1.641867 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3300: 1.644367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3400: 1.637786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3500: 1.676404 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3600: 1.653186 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3700: 1.653022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3800: 1.654195 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3900: 1.649528 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4000: 1.640188 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "chs ofbana two four it other special represeraries of the comprelmian mamns of w\n",
      "ats her the dilder international mey independation hassurgul dystempthromman alo\n",
      "ve to resupper of they the freffestable debeent and smothery politic and them fo\n",
      "phira adal toxidres of l by zero five one myzba swastal their europerered inlegu\n",
      "polam regivor s tall ti kingate to ckund of here from the the oceasive ind the d\n",
      "================================================================================\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4100: 1.621686 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4200: 1.616082 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4300: 1.618655 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4400: 1.610849 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4500: 1.640701 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4600: 1.623868 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4700: 1.623027 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4800: 1.610500 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4900: 1.617349 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5000: 1.615753 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "a looplank phesiachers applement man bor foundy others what paran convation woul\n",
      "rie one nine nine three one nine seven one nine nine six fouling in droch trisi \n",
      "x cis winneal ghemon cachility with one two one just works pold imperions of ebn\n",
      "bat in intension wnoms in the provinces that is fave which quites of negal tombu\n",
      "ld privated workposporia accounal fodaus to crimaras tod account semperation wor\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5100: 1.589508 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5200: 1.593257 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5300: 1.596391 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5400: 1.591077 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5500: 1.592870 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5600: 1.563146 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5700: 1.580295 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5800: 1.601089 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5900: 1.582124 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6000: 1.582963 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "th aapociated of sole charamle bagavs and de scienty accultion to websel of cont\n",
      "at was city five trycle was one two th arganistraph covenael christraping the ma\n",
      "quarders on a physicys cirmbere evants spole one nine four sect and shank that a\n",
      "grated book defided by disside moness are line tracks networks terirical on ther\n",
      "es slow the persoffopmple cantinuatist from include napt plobenes athed the comi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6100: 1.576578 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6200: 1.588020 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6300: 1.588925 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6400: 1.573907 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6500: 1.556827 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6600: 1.601192 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6700: 1.573209 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6800: 1.574528 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6900: 1.573999 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 7000: 1.590185 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "ingersism hand their war nine alenetynal by dequire therepithe subperficte as po\n",
      "meanse those but organicaging the starmazile which lears one nine nine one kong \n",
      "positions harsihoiges jeat for vaived apolicion he reusers a magrima against klo\n",
      "on ouneratifia nations of done ramio beadaica never the first laltine is prich f\n",
      "de chesved mademistic and buill hall human ream is from the system testonni of t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    # check that the unoptimized and optimized tensors are really equal\n",
    "    for name, t1, t2 in equals:\n",
    "      m1, m2 = session.run([t1, t2], feed_dict=feed_dict)\n",
    "      #m2 = session.run(t2, feed_dict=feed_dict)\n",
    "      if not np.array_equal(m1, m2):\n",
    "        print('first version:', name)\n",
    "        print(m1)\n",
    "        print('second version:', name)\n",
    "        print(m2)\n",
    "        raise Exception(\"not equal: \" + name)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the equality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # big matrix\n",
    "  ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    all_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "    update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295034 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "oifs aqqttketi f hes itbxlzqv cbu epcn vojjohcnt  vircbagyp et  jll  wej  hxstoq\n",
      "lqtf aei yooiyw fsovem pyasrrocachixm teln cd  nc jj h znsnca lrhardrfy f zljanq\n",
      "ym uesxftka johopnmt qzl godw ncdeado  kxpihgsel  rze qjwcingnorlo xsryyydtkrvil\n",
      " ykl uffy mrbhdbmxxussaq s lra tvyh  qsrq ms  ihae lyeqanrigcofxpnndardzuesiyjib\n",
      " iacp iurmunoanonederb evncs cuh mvsmitudbctmv iqph zarmc rnhngxapfynnida ii txo\n",
      "================================================================================\n",
      "Validation set perplexity: 19.96\n",
      "Average loss at step 100: 2.569330 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.36\n",
      "Validation set perplexity: 10.67\n",
      "Average loss at step 200: 2.231631 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.24\n",
      "Validation set perplexity: 8.71\n",
      "Average loss at step 300: 2.066028 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 400: 1.986493 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 500: 1.989676 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 600: 1.917489 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 700: 1.892458 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 800: 1.866222 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 900: 1.857650 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 1000: 1.794840 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "================================================================================\n",
      "dige avtere boghh to iccoxllation usive and and the incemantion be d p pohic and\n",
      "gratic feverent and tratal mike condestice or are deree instete or duy teck om o\n",
      "w in f voullorly pondially emco pootion on auce was i laings were to wend y g on\n",
      "k cancents a swaral hind papael and inselen hapen are with bockes festle yeupent\n",
      "choder filis of anzirore ony as acrish reckerse adoping d compore nime barisk fi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 1100: 1.766080 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 1200: 1.791065 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1300: 1.771337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1400: 1.742279 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1500: 1.732741 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1600: 1.724299 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1700: 1.742926 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1800: 1.706892 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1900: 1.710154 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 2000: 1.718050 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "================================================================================\n",
      "fly her polds monef where pheason mays lite jow roy fores four felt see ass of s\n",
      "assaily atering kacroldrise dr s chemred rise lobianscoute plankation and with t\n",
      "we deverded the goligated of eedleds about entroduces may isle denimifur three n\n",
      "eds over a partomigy contentsica race obspus to thiy are jork inclure oft consti\n",
      "califasted if the vaction beriok iis lapkinate sharks becests frem engrese dgngr\n",
      "================================================================================\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 2100: 1.707516 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 2200: 1.677971 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2300: 1.691300 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 2400: 1.685169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2500: 1.701410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2600: 1.676789 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2700: 1.694822 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2800: 1.657274 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2900: 1.659319 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3000: 1.668788 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "gin cases and caling operard arqlimes wation is aroughan about had mono is aros \n",
      "zer im kegarid two in effect eed its harnoder of sheright and denim halperal tho\n",
      "hed russipale clisen to foum eary airboghal is ho side been thir relecvide used \n",
      "paid two two five zero zero six releaseo of logendy history and the keating dead\n",
      "y ewgrance chillram successnusmen sockling to editure pcousles of ecotwon fremop\n",
      "================================================================================\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3100: 1.659754 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3200: 1.648873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3300: 1.633994 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3400: 1.641914 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3500: 1.630583 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3600: 1.635313 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3700: 1.633166 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3800: 1.627017 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3900: 1.613126 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4000: 1.620025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "balian warmbered han a cupprom used in five said asman the tuam is peneballed to\n",
      "thwys somoanagrord addentic puildcy secriting if deal juws and bborded bellers t\n",
      "kense and the most addidernal irer make deugune roung guited is also two a domot\n",
      "jud ampits of kerm laskingured been allow the commusion of pirticulied antershoo\n",
      "form of how kdring gialiflunabing of the couldered as a a verlitus and clails in\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4100: 1.618014 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 4200: 1.612639 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 4300: 1.591576 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4400: 1.620040 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4500: 1.631085 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4600: 1.629955 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4700: 1.601037 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4800: 1.588866 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4900: 1.597695 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5000: 1.623786 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "================================================================================\n",
      "thlactance danod by to geninc by even day s blembola for k this most other and a\n",
      "zalf pachiatal a altury actuals dedefts part of ni lyjons operour sy citiblle or\n",
      "cho atthults wake the promary rach alcos of the cublit as cell domainne ra culte\n",
      "lyha was sought zero eight and its extrotish theil varian with known a sage is a\n",
      "tring name the quilizan renexed is the enten stweh cily graph integer spaciet ye\n",
      "================================================================================\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5100: 1.637785 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5200: 1.628842 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5300: 1.589097 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5400: 1.588204 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5500: 1.581041 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5600: 1.612528 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5700: 1.568487 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.97\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5800: 1.572650 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5900: 1.593196 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6000: 1.559029 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "qeener the thin air of their considered a no contrases the deayment over propera\n",
      "z dtated fulf en v lateag dound the marques of asia air has fewentled it met of \n",
      "x sade to fore and eight naster who berre prite covers a pul wit having quesser \n",
      "fact is a figurnal by gust b zero winlow then as as san beloud simply on the rin\n",
      "bory but in changesky still condition the two contra ables in reformodeship the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6100: 1.581440 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6200: 1.598730 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6300: 1.614295 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6400: 1.642830 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6500: 1.639777 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6600: 1.604722 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6700: 1.596918 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6800: 1.578007 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6900: 1.571778 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 7000: 1.580957 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "like and show the tripsed to gright in enduphors s milothing one nine eight seve\n",
      "selluge lively dispoeting links withowd afracted and demovinitial wills game hav\n",
      "her advants on offanate of aridit a during argentish s an slegzing forment the b\n",
      "ment of are mosemention althoutimation lower its d one nine eight one nine six z\n",
      "h two one eight two zero five six zero x of many bastonnet a pairants of play of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "CPU times: user 4min 28s, sys: 4min, total: 8min 29s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without embeddings and without dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(o,n)(n,s)(s, )( ,a)(a,n)(n,a)(a,r)(r,c)(c,h)(h,i)(i,s)', '(w,h)(h,e)(e,n)(n, )( ,m)(m,i)(i,l)(l,i)(i,t)(t,a)(a,r)', '(l,l)(l,e)(e,r)(r,i)(i,a)(a, )( ,a)(a,r)(r,c)(c,h)(h,e)', '( ,a)(a,b)(b,b)(b,e)(e,y)(y,s)(s, )( ,a)(a,n)(n,d)(d, )', '(m,a)(a,r)(r,r)(r,i)(i,e)(e,d)(d, )( ,u)(u,r)(r,r)(r,a)', '(h,e)(e,l)(l, )( ,a)(a,n)(n,d)(d, )( ,r)(r,i)(i,c)(c,h)', '(y, )( ,a)(a,n)(n,d)(d, )( ,l)(l,i)(i,t)(t,u)(u,r)(r,g)', '(a,y)(y, )( ,o)(o,p)(p,e)(e,n)(n,e)(e,d)(d, )( ,f)(f,o)', '(t,i)(i,o)(o,n)(n, )( ,f)(f,r)(r,o)(o,m)(m, )( ,t)(t,h)', '(m,i)(i,g)(g,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,t)(t,o)', '(n,e)(e,w)(w, )( ,y)(y,o)(o,r)(r,k)(k, )( ,o)(o,t)(t,h)', '(h,e)(e, )( ,b)(b,o)(o,e)(e,i)(i,n)(n,g)(g, )( ,s)(s,e)', '(e, )( ,l)(l,i)(i,s)(s,t)(t,e)(e,d)(d, )( ,w)(w,i)(i,t)', '(e,b)(b,e)(e,r)(r, )( ,h)(h,a)(a,s)(s, )( ,p)(p,r)(r,o)', '(o, )( ,b)(b,e)(e, )( ,m)(m,a)(a,d)(d,e)(e, )( ,t)(t,o)', '(y,e)(e,r)(r, )( ,w)(w,h)(h,o)(o, )( ,r)(r,e)(e,c)(c,e)', '(o,r)(r,e)(e, )( ,s)(s,i)(i,g)(g,n)(n,i)(i,f)(f,i)(i,c)', '(a, )( ,f)(f,i)(i,e)(e,r)(r,c)(c,e)(e, )( ,c)(c,r)(r,i)', '( ,t)(t,w)(w,o)(o, )( ,s)(s,i)(i,x)(x, )( ,e)(e,i)(i,g)', '(a,r)(r,i)(i,s)(s,t)(t,o)(o,t)(t,l)(l,e)(e, )( ,s)(s, )', '(i,t)(t,y)(y, )( ,c)(c,a)(a,n)(n, )( ,b)(b,e)(e, )( ,l)', '( ,a)(a,n)(n,d)(d, )( ,i)(i,n)(n,t)(t,r)(r,a)(a,c)(c,e)', '(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )', '(d,y)(y, )( ,t)(t,o)(o, )( ,p)(p,a)(a,s)(s,s)(s, )( ,h)', '(f, )( ,c)(c,e)(e,r)(r,t)(t,a)(a,i)(i,n)(n, )( ,d)(d,r)', '(a,t)(t, )( ,i)(i,t)(t, )( ,w)(w,i)(i,l)(l,l)(l, )( ,t)', '(e, )( ,c)(c,o)(o,n)(n,v)(v,i)(i,n)(n,c)(c,e)(e, )( ,t)', '(e,n)(n,t)(t, )( ,t)(t,o)(o,l)(l,d)(d, )( ,h)(h,i)(i,m)', '(a,m)(m,p)(p,a)(a,i)(i,g)(g,n)(n, )( ,a)(a,n)(n,d)(d, )', '(r,v)(v,e)(e,r)(r, )( ,s)(s,i)(i,d)(d,e)(e, )( ,s)(s,t)', '(i,o)(o,u)(u,s)(s, )( ,t)(t,e)(e,x)(x,t)(t,s)(s, )( ,s)', '(o, )( ,c)(c,a)(a,p)(p,i)(i,t)(t,a)(a,l)(l,i)(i,z)(z,e)', '(a, )( ,d)(d,u)(u,p)(p,l)(l,i)(i,c)(c,a)(a,t)(t,e)(e, )', '(g,h)(h, )( ,a)(a,n)(n,n)(n, )( ,e)(e,s)(s, )( ,d)(d, )', '(i,n)(n,e)(e, )( ,j)(j,a)(a,n)(n,u)(u,a)(a,r)(r,y)(y, )', '(r,o)(o,s)(s,s)(s, )( ,z)(z,e)(e,r)(r,o)(o, )( ,t)(t,h)', '(c,a)(a,l)(l, )( ,t)(t,h)(h,e)(e,o)(o,r)(r,i)(i,e)(e,s)', '(a,s)(s,t)(t, )( ,i)(i,n)(n,s)(s,t)(t,a)(a,n)(n,c)(c,e)', '( ,d)(d,i)(i,m)(m,e)(e,n)(n,s)(s,i)(i,o)(o,n)(n,a)(a,l)', '(m,o)(o,s)(s,t)(t, )( ,h)(h,o)(o,l)(l,y)(y, )( ,m)(m,o)', '(t, )( ,s)(s, )( ,s)(s,u)(u,p)(p,p)(p,o)(o,r)(r,t)(t, )', '(u, )( ,i)(i,s)(s, )( ,s)(s,t)(t,i)(i,l)(l,l)(l, )( ,d)', '(e, )( ,o)(o,s)(s,c)(c,i)(i,l)(l,l)(l,a)(a,t)(t,i)(i,n)', '(o, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,s)(s,u)(u,b)(b,t)', '(o,f)(f, )( ,i)(i,t)(t,a)(a,l)(l,y)(y, )( ,l)(l,a)(a,n)', '(s, )( ,t)(t,h)(h,e)(e, )( ,t)(t,o)(o,w)(w,e)(e,r)(r, )', '(k,l)(l,a)(a,h)(h,o)(o,m)(m,a)(a, )( ,p)(p,r)(r,e)(e,s)', '(e,r)(r,p)(p,r)(r,i)(i,s)(s,e)(e, )( ,l)(l,i)(i,n)(n,u)', '(w,s)(s, )( ,b)(b,e)(e,c)(c,o)(o,m)(m,e)(e,s)(s, )( ,t)', '(e,t)(t, )( ,i)(i,n)(n, )( ,a)(a, )( ,n)(n,a)(a,z)(z,i)', '(t,h)(h,e)(e, )( ,f)(f,a)(a,b)(b,i)(i,a)(a,n)(n, )( ,s)', '(e,t)(t,c)(c,h)(h,y)(y, )( ,t)(t,o)(o, )( ,r)(r,e)(e,l)', '( ,s)(s,h)(h,a)(a,r)(r,m)(m,a)(a,n)(n, )( ,n)(n,e)(e,t)', '(i,s)(s,e)(e,d)(d, )( ,e)(e,m)(m,p)(p,e)(e,r)(r,o)(o,r)', '(t,i)(i,n)(n,g)(g, )( ,i)(i,n)(n, )( ,p)(p,o)(o,l)(l,i)', '(d, )( ,n)(n,e)(e,o)(o, )( ,l)(l,a)(a,t)(t,i)(i,n)(n, )', '(t,h)(h, )( ,r)(r,i)(i,s)(s,k)(k,y)(y, )( ,r)(r,i)(i,s)', '(e,n)(n,c)(c,y)(y,c)(c,l)(l,o)(o,p)(p,e)(e,d)(d,i)(i,c)', '(f,e)(e,n)(n,s)(s,e)(e, )( ,t)(t,h)(h,e)(e, )( ,a)(a,i)', '(d,u)(u,a)(a,t)(t,i)(i,n)(n,g)(g, )( ,f)(f,r)(r,o)(o,m)', '(t,r)(r,e)(e,e)(e,t)(t, )( ,g)(g,r)(r,i)(i,d)(d, )( ,c)', '(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,m)(m,o)(o,r)(r,e)(e, )', '(a,p)(p,p)(p,e)(e,a)(a,l)(l, )( ,o)(o,f)(f, )( ,d)(d,e)', '(s,i)(i, )( ,h)(h,a)(a,v)(v,e)(e, )( ,m)(m,a)(a,d)(d,e)']\n",
      "['(i,s)(s,t)(t,s)(s, )( ,a)(a,d)(d,v)(v,o)(o,c)(c,a)(a,t)', '(a,r)(r,y)(y, )( ,g)(g,o)(o,v)(v,e)(e,r)(r,n)(n,m)(m,e)', '(h,e)(e,s)(s, )( ,n)(n,a)(a,t)(t,i)(i,o)(o,n)(n,a)(a,l)', '(d, )( ,m)(m,o)(o,n)(n,a)(a,s)(s,t)(t,e)(e,r)(r,i)(i,e)', '(r,a)(a,c)(c,a)(a, )( ,p)(p,r)(r,i)(i,n)(n,c)(c,e)(e,s)', '(c,h)(h,a)(a,r)(r,d)(d, )( ,b)(b,a)(a,e)(e,r)(r, )( ,h)', '(r,g)(g,i)(i,c)(c,a)(a,l)(l, )( ,l)(l,a)(a,n)(n,g)(g,u)', '(f,o)(o,r)(r, )( ,p)(p,a)(a,s)(s,s)(s,e)(e,n)(n,g)(g,e)', '(t,h)(h,e)(e, )( ,n)(n,a)(a,t)(t,i)(i,o)(o,n)(n,a)(a,l)', '(t,o)(o,o)(o,k)(k, )( ,p)(p,l)(l,a)(a,c)(c,e)(e, )( ,d)', '(t,h)(h,e)(e,r)(r, )( ,w)(w,e)(e,l)(l,l)(l, )( ,k)(k,n)', '(s,e)(e,v)(v,e)(e,n)(n, )( ,s)(s,i)(i,x)(x, )( ,s)(s,e)', '(i,t)(t,h)(h, )( ,a)(a, )( ,g)(g,l)(l,o)(o,s)(s,s)(s, )', '(r,o)(o,b)(b,a)(a,b)(b,l)(l,y)(y, )( ,b)(b,e)(e,e)(e,n)', '(t,o)(o, )( ,r)(r,e)(e,c)(c,o)(o,g)(g,n)(n,i)(i,z)(z,e)', '(c,e)(e,i)(i,v)(v,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,f)', '(i,c)(c,a)(a,n)(n,t)(t, )( ,t)(t,h)(h,a)(a,n)(n, )( ,i)', '(r,i)(i,t)(t,i)(i,c)(c, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)', '(i,g)(g,h)(h,t)(t, )( ,i)(i,n)(n, )( ,s)(s,i)(i,g)(g,n)', '(s, )( ,u)(u,n)(n,c)(c,a)(a,u)(u,s)(s,e)(e,d)(d, )( ,c)', '( ,l)(l,o)(o,s)(s,t)(t, )( ,a)(a,s)(s, )( ,i)(i,n)(n, )', '(c,e)(e,l)(l,l)(l,u)(u,l)(l,a)(a,r)(r, )( ,i)(i,c)(c,e)', '(e, )( ,s)(s,i)(i,z)(z,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)', '( ,h)(h,i)(i,m)(m, )( ,a)(a, )( ,s)(s,t)(t,i)(i,c)(c,k)', '(d,r)(r,u)(u,g)(g,s)(s, )( ,c)(c,o)(o,n)(n,f)(f,u)(u,s)', '( ,t)(t,a)(a,k)(k,e)(e, )( ,t)(t,o)(o, )( ,c)(c,o)(o,m)', '( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,i)(i,e)(e,s)(s,t)(t, )', '(i,m)(m, )( ,t)(t,o)(o, )( ,n)(n,a)(a,m)(m,e)(e, )( ,i)', '(d, )( ,b)(b,a)(a,r)(r,r)(r,e)(e,d)(d, )( ,a)(a,t)(t,t)', '(s,t)(t,a)(a,n)(n,d)(d,a)(a,r)(r,d)(d, )( ,f)(f,o)(o,r)', '( ,s)(s,u)(u,c)(c,h)(h, )( ,a)(a,s)(s, )( ,e)(e,s)(s,o)', '(z,e)(e, )( ,o)(o,n)(n, )( ,t)(t,h)(h,e)(e, )( ,g)(g,r)', '(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,o)(o,r)(r,i)', '(d, )( ,h)(h,i)(i,v)(v,e)(e,r)(r, )( ,o)(o,n)(n,e)(e, )', '(y, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,m)(m,a)(a,r)(r,c)', '(t,h)(h,e)(e, )( ,l)(l,e)(e,a)(a,d)(d, )( ,c)(c,h)(h,a)', '(e,s)(s, )( ,c)(c,l)(l,a)(a,s)(s,s)(s,i)(i,c)(c,a)(a,l)', '(c,e)(e, )( ,t)(t,h)(h,e)(e, )( ,n)(n,o)(o,n)(n, )( ,g)', '(a,l)(l, )( ,a)(a,n)(n,a)(a,l)(l,y)(y,s)(s,i)(i,s)(s, )', '(m,o)(o,r)(r,m)(m,o)(o,n)(n,s)(s, )( ,b)(b,e)(e,l)(l,i)', '(t, )( ,o)(o,r)(r, )( ,a)(a,t)(t, )( ,l)(l,e)(e,a)(a,s)', '( ,d)(d,i)(i,s)(s,a)(a,g)(g,r)(r,e)(e,e)(e,d)(d, )( ,u)', '(i,n)(n,g)(g, )( ,s)(s,y)(y,s)(s,t)(t,e)(e,m)(m, )( ,e)', '(b,t)(t,y)(y,p)(p,e)(e,s)(s, )( ,b)(b,a)(a,s)(s,e)(e,d)', '(a,n)(n,g)(g,u)(u,a)(a,g)(g,e)(e,s)(s, )( ,t)(t,h)(h,e)', '(r, )( ,c)(c,o)(o,m)(m,m)(m,i)(i,s)(s,s)(s,i)(i,o)(o,n)', '(e,s)(s,s)(s, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)', '(n,u)(u,x)(x, )( ,s)(s,u)(u,s)(s,e)(e, )( ,l)(l,i)(i,n)', '( ,t)(t,h)(h,e)(e, )( ,f)(f,i)(i,r)(r,s)(s,t)(t, )( ,d)', '(z,i)(i, )( ,c)(c,o)(o,n)(n,c)(c,e)(e,n)(n,t)(t,r)(r,a)', '( ,s)(s,o)(o,c)(c,i)(i,e)(e,t)(t,y)(y, )( ,n)(n,e)(e,h)', '(e,l)(l,a)(a,t)(t,i)(i,v)(v,e)(e,l)(l,y)(y, )( ,s)(s,t)', '(e,t)(t,w)(w,o)(o,r)(r,k)(k,s)(s, )( ,s)(s,h)(h,a)(a,r)', '(o,r)(r, )( ,h)(h,i)(i,r)(r,o)(o,h)(h,i)(i,t)(t,o)(o, )', '(l,i)(i,t)(t,i)(i,c)(c,a)(a,l)(l, )( ,i)(i,n)(n,i)(i,t)', '(n, )( ,m)(m,o)(o,s)(s,t)(t, )( ,o)(o,f)(f, )( ,t)(t,h)', '(i,s)(s,k)(k,e)(e,r)(r,d)(d,o)(o,o)(o, )( ,r)(r,i)(i,c)', '(i,c)(c, )( ,o)(o,v)(v,e)(e,r)(r,v)(v,i)(i,e)(e,w)(w, )', '(a,i)(i,r)(r, )( ,c)(c,o)(o,m)(m,p)(p,o)(o,n)(n,e)(e,n)', '(o,m)(m, )( ,a)(a,c)(c,n)(n,m)(m, )( ,a)(a,c)(c,c)(c,r)', '( ,c)(c,e)(e,n)(n,t)(t,e)(e,r)(r,l)(l,i)(i,n)(n,e)(e, )', '(e, )( ,t)(t,h)(h,a)(a,n)(n, )( ,a)(a,n)(n,y)(y, )( ,o)', '(d,e)(e,v)(v,o)(o,t)(t,i)(i,o)(o,n)(n,a)(a,l)(l, )( ,b)', '(d,e)(e, )( ,s)(s,u)(u,c)(c,h)(h, )( ,d)(d,e)(e,v)(v,i)']\n",
      "['( ,a)(a,n)']\n",
      "['(a,n)(n,a)']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, bigram_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      first_char = self._text[self._cursor[b]]\n",
    "      if self._cursor[b] + 1 == self._text_size:\n",
    "        second_char = ' '\n",
    "      else:\n",
    "        second_char = self._text[self._cursor[b] + 1]\n",
    "      batch[b, char2id(first_char) * vocabulary_size + char2id(second_char)] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def bigram_characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return ['({0},{1})'.format(id2char(c//vocabulary_size), id2char(c % vocabulary_size))\n",
    "          for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigram_first_characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c//vocabulary_size)\n",
    "          for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigram_batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, bigram_characters(b))]\n",
    "  return s\n",
    "\n",
    "bigram_train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "bigram_valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "# output each bigram instead of a single char\n",
    "print(bigram_batches2string(bigram_train_batches.next()))\n",
    "print(bigram_batches2string(bigram_train_batches.next()))\n",
    "print(bigram_batches2string(bigram_valid_batches.next()))\n",
    "print(bigram_batches2string(bigram_valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # big matrix\n",
    "  ifcox = tf.Variable(tf.truncated_normal([bigram_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    all_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "    update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,bigram_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, bigram_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, bigram_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def bigram_random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, bigram_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591460 learning rate: 10.000000\n",
      "Minibatch perplexity: 728.84\n",
      "================================================================================\n",
      "bigrams: (m,c)(v, )( ,l)(j,y)(m,v)(l,s)(l,h)(b,w)(l,a)(n,k)(r,j)(d,j)(g,x)(i,v)(x,b)(g,a)(e,b)( ,r)(q, )(v,e)(v,r)(z,w)(k,k)(h,v)(e,a)(v, )(o,m)(u,c)(j,j)(h,n)(h,x)(v,c)(a,t)(b,o)(d,f)(i,s)(x,i)(y,h)(i,z)(e,v)(p,x)(h,s)( ,e)(e,s)(k,c)(a,k)(b,q)(i,s)(y,y)(x,p)(z,f)(o,c)(b,o)(m,e)(r,y)( ,o)(u,w)(i,g)(f,c)(c, )(j,f)(e,p)(n,j)(b,k)(w,g)(s,l)(o,n)(j,x)(d,a)(j,c)(o,a)(v,e)(j,a)(t,m)(y,l)(x,u)(k,j)(m,x)(q,d)(r,d)\n",
      "chars: mv jmllblnrdgixge qvvzkhevoujhhvabdixyieph ekabiyxzobmr uifcjenbwsojdjovjtyxkmqr\n",
      "bigrams: (x,r)(l,o)(z,a)(l, )(t,w)(a, )(r,o)(b,q)(v,m)(n,r)(v,h)(m,f)(k,a)(j,n)( ,p)(a,r)(e,i)( ,g)(p,p)(u,n)(w,i)(c,m)(v,i)(e,m)(b,x)(n,y)(e,b)(r,h)(t,t)(c,y)(y,t)(c, )(d,c)(r,a)(o,f)(o,q)(q,w)(e,c)(n,g)(q,i)(l,i)(g,j)(z,q)(u,l)(r,n)(b,y)(d,l)(d, )(y,e)(v,o)(x,z)(v,i)(p,b)(q,q)(o,p)(v,j)(l,o)(i,v)(k,e)(a,d)(l,t)(e,i)(v,s)(z,w)(v,g)(n, )(s,e)(s,c)(e,l)(c,f)(c,k)( ,a)(f,c)(n,k)(i,p)(h,m)(l,q)(u,k)(o,f)(a,b)\n",
      "chars: xlzltarbvnvmkj ae puwcvebnertcycdrooqenqlgzurbddyvxvpqovlikalevzvnssecc fnihluoa\n",
      "bigrams: (r,q)(d,a)(y,w)(w,t)(t,r)(b,e)( ,u)(m,r)(l,d)(p,t)(g,z)(e,u)(h,g)(f,w)( ,n)(c,f)(m,t)(q,j)(d,m)(v,y)(x,o)(y,b)(z,u)(c,m)(i,v)(c,k)(v,g)(o,n)(w,w)(m,a)(q,e)(d,y)(f,a)(w,z)(f,h)(c,n)(l,t)(v,u)(c,l)(y,j)(r,e)(f,q)(j,y)(o,i)(f,o)(t,e)(f,e)(r,k)(o,q)(o, )(v,t)(q,b)(d,p)(s,p)(l, )(r,o)(v,m)(n,g)(t,m)(x,w)(b,x)(c,i)(o,x)(n,l)(o, )(s,o)(t,g)(y,q)(h,k)(k,n)(t,z)(c,t)(m,u)(k,h)(g,t)(x,r)(j,a)(t,e)(o,o)(n,l)\n",
      "chars: rdywtb mlpgehf cmqdvxyzcicvowmqdfwfclvcyrfjoftfroovqdslrvntxbconostyhktcmkgxjton\n",
      "bigrams: (u,n)(r,y)(k,x)(f,j)(c,w)(l,u)( ,i)(t,h)(f,i)(b,x)(f,z)(l,x)(v,b)(u,g)(d,t)(y,h)(p,k)(v,a)(m,o)(f,u)(f,k)(k,l)(b,o)(w,s)(g, )(m,r)( ,e)(a,w)(i,b)(f,a)(c,r)(q,y)(q,f)(t,p)(t,n)(f,v)(o,c)(e, )(o,y)(m,v)(t,g)(l,g)(t,e)(t,z)(c,y)(h,b)(u,z)(g,n)(d, )(p,n)(b,b)(u,m)(o,t)(u,m)(b,r)(v,j)(h,e)(g,u)(v,n)(w,f)(i,v)(q,c)(o,t)(z,a)(t,j)(t,n)(c,a)(w,k)(l,x)(t,b)(d,r)(t, )(w,a)(e, )(y,t)(a,l)(e,u)(u,a)(y,u)(q,r)\n",
      "chars: urkfcl tfbflvudypvmffkbwgm aifcqqttfoeomtlttchugdpbuoubvhgvwiqozttcwltdtweyaeuyq\n",
      "bigrams: (k,c)(f,a)(v,y)(h,t)(a,j)(g,d)(a,i)(y,p)(n,v)(k,m)(x,g)(y,p)(j,o)(a,y)(n,l)(p,p)(t,a)(j,m)(o,f)(t,i)(n,e)(f,l)(y,b)(q,q)(y,v)(f,z)(p,a)(i,i)(l,i)(m,y)(s,n)(g,n)(e,z)(e,b)(p,c)(g,p)(r,z)(d,a)(y,s)(y,y)(a,w)(u,n)(t,f)(f,l)(d,n)(z,n)(h,q)(x,h)(l,n)(d,o)(g,j)(r,y)(i,e)(w,w)(e,h)( ,r)(o,f)(x,w)(f,h)(s,u)(t,c)(z,w)(c,c)(h,n)(d,i)( ,i)(e,l)(v,w)(r,i)(d,h)(m,k)(q,c)(r,i)(k,w)(j,t)(w,a)(b,o)(q,s)(e,j)(w,u)\n",
      "chars: kfvhagaynkxyjanptjotnfyqyfpilmsgeepgrdyyautfdzhxldgriwe oxfstzchd evrdmqrkjwbqew\n",
      "================================================================================\n",
      "Validation set perplexity: 673.38\n",
      "Average loss at step 100: 5.382529 learning rate: 10.000000\n",
      "Minibatch perplexity: 128.15\n",
      "Validation set perplexity: 113.70\n",
      "Average loss at step 200: 4.243766 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.93\n",
      "Validation set perplexity: 34.06\n",
      "Average loss at step 300: 3.198707 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.03\n",
      "Validation set perplexity: 15.93\n",
      "Average loss at step 400: 2.560718 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.00\n",
      "Validation set perplexity: 11.20\n",
      "Average loss at step 500: 2.275511 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.36\n",
      "Validation set perplexity: 9.02\n",
      "Average loss at step 600: 2.138866 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 700: 2.016064 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 800: 1.942940 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 900: 1.935566 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.26\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 1000: 1.916775 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "================================================================================\n",
      "bigrams: (a,q)(r,c)(t,i)(i,a)(a,l)(l, )(h,y)(c,l)(l,e)(a,r)(r,g)(g,e)(e, )( ,o)(o,f)(f, )( ,p)(p,r)(r,o)( ,d)(d,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,b)(b,a)(a,m)(m,e)(e, )( ,g)(g,i)(i,v)(v,i)(i,o)(o,r)(r,k)(w,i)(i,s)(s,h)(h, )( ,o)(o,l)(l,d)(d, )( ,i)(i,s)(s, )( ,d)(d,e)(e,i)(i,v)(v,e)(e,s)(s, )( ,c)(c,a)(v,l)(p,p)(p,t)(t, )( ,t)(t,h)(h,e)(e, )( ,a)(a,n)(n,t)(t,i)(i,a)(a,n)(n,t)(t,s)(s, )( ,c)(c,e)(e,r)(r,r)\n",
      "chars: artialhclarge of pr ded the bame giviorwish old is deives cvppt the antiants cer\n",
      "bigrams: (i,f)(f, )( ,t)(t,h)(h,e)(e, )( ,c)(c,a)(a,p)(m,e)(e, )( ,f)(f,a)(a,d)(d,e)(e, )( ,e)(g,n)(n,d)(d,i)(i,n)(n,o)(o,w)(w, )( ,e)(e,v)(v,o)(o,m)(m,e)(e, )( ,a)(a, )( ,g)(g,o)(o,c)(c,h)(h, )( ,c)(c,a)(a,m)(m,e)(e, )( ,r)(r,o)(o,o)(o,u)(u, )( ,q)(c,a)(a,s)(s,s)(s, )( ,d)(d,e)(e,v)(v,e)(e,r)(n, )( ,b)(b,u)(u,t)(t,l)(l,e)(e,d)(d, )( ,o)(o,f)(f, )( ,k)(j,o)(o,t)(t,e)(e, )( ,r)(r,a)(a,v)( ,y)(e,l)(r,o)(o,n)\n",
      "chars: if the came fade gndinow evome a goch came roou cass deven butled of jote ra ero\n",
      "bigrams: (p,a)(a,v)(v,a)(a,r)(r,e)(e, )( ,u)(u,s)(s,e)(e,m)(m,s)(s, )( ,a)(a,n)(n,d)(d,i)(i,f)(b,y)(y, )( ,i)(i,n)(n, )( ,a)(a, )( ,t)(t,h)(h,e)(e, )( ,e)(e,a)(a,d)(d,o)(o,p)(p,e)(e,t)(t,h)(h,n)( ,z)(z,e)(e, )( ,s)(s,p)(p,o)(o,r)(r,d)(d, )( ,o)(o,f)(f, )( ,b)(b,e)(e,s)(s,t)(t,r)(r,o)(o,t)(t,i)(i,o)(o, )( ,e)(e,q)(b,s)(s, )( ,i)(i,n)(n, )( ,a)(a, )( ,m)(m,u)(u,t)(t,e)(e,r)(r,e)(e,n)(n,o)(o,l)(e, )( ,t)(t,h)\n",
      "chars: pavare usems andiby in a the eadopeth ze spord of bestrotio ebs in a muterenoe t\n",
      "bigrams: (m,k)(a,d)(d,o)(o,g)(g,o)(o,n)(n, )( ,t)(t,h)(h,e)(e, )( ,b)(y,e)(e,l)(l,o)(o,t)(t,e)(e,d)(d, )( ,w)(w,h)(f,e)(e,q)(a,y)(y, )( ,t)(t,h)(h,e)(e, )( ,o)(o,n)(n,o)(o,p)(p,r)(r,e)(e,c)(c,i)(i,c)(c,k)(r,a)(a,s)(s, )( ,t)(t,h)(h,e)(e, )( ,r)(r,i)(i,s)(s,t)(t,e)(e,a)(a,s)(s, )( ,w)(w,i)(i,t)(t,h)(h, )( ,o)(o,r)(r, )( ,t)(t,o)(o, )( ,w)(w,o)(o,v)(v,e)(e,r)(r, )( ,o)(o,f)(f, )( ,n)(n,i)(i,c)(c,i)(i,n)(n,i)\n",
      "chars: madogon the yeloted wfeay the onoprecicras the risteas with or to wover of nicin\n",
      "bigrams: ( ,u)(u,t)(t,e)(e,r)(r, )( ,a)(a,s)(s, )( ,t)(t,w)(w,o)(o, )( ,w)(w,i)(i,n)(n,c)(k,s)(s,e)(e,d)(d, )( ,c)(c,a)(a,m)(m,m)(w,x)(y,s)(g,l)(l,e)(e, )( ,m)(m,o)(o,v)(v,e)(e,r)(r,m)(m,e)(e,r)(r, )( ,o)(o,f)(f, )( ,a)(a, )( ,h)(h,i)(i,s)(s,t)(t, )( ,t)(t,o)(o, )( ,d)(d,i)(i,f)(f,i)(i,c)(c, )( ,o)(o,r)(r, )( ,m)(m,i)(i,t)(t,t)(t,i)(i,n)(n,g)(g, )( ,s)(s,m)(m,o)(o,t)(t,h)(h,e)(e,r)(r, )( ,g)(g,i)(i,c)(m,a)\n",
      "chars:  uter as two winksed camwygle movermer of a hist to dific or mitting smother gim\n",
      "================================================================================\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 1100: 1.846387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1200: 1.801363 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1300: 1.775546 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1400: 1.781660 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1500: 1.769156 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1600: 1.778107 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1700: 1.734205 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1800: 1.691583 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 1900: 1.659500 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2000: 1.710991 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "bigrams: (v,v)(e,s)(s, )( ,f)(f,a)(a,m)(m,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,c)(c,a)(a,t)(t,t)(t,u)(u,r)(e,u)(u,r)(r,s)(s, )( ,w)(w,o)(o,r)(r,d)(d,s)(s, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,o)(o,n)(n, )( ,e)(e,a)(a,r)(r,l)(l,d)(d, )( ,o)(o,r)(r,g)(g,l)(l, )( ,t)(t,h)(h,e)(e, )( ,g)(g,e)( ,d)(d,i)(i,a)(a,n)(n, )( ,a)(a,n)(n,c)(c,t)(t,i)(i,v)(v,e)(e, )( ,s)(s, )( ,t)(t,e)(e,n)(n, )( ,t)(t,h)(h,e)(e, )( ,p)(p,r)\n",
      "chars: ves famed the cattueurs words three on earld orgl the g dian anctive s ten the p\n",
      "bigrams: (n,t)(t,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,e)(e, )( ,r)(r,i)(i,k)(k,e)(e,x)(n, )( ,p)(p,r)(r,o)(o,u)(u,s)(s, )( ,h)(h,e)(e,l)(l,d)(d, )( ,i)(i,n)(n,v)(v,i)(i,n)(n,u)(h,u)(u,a)(a,m)(m, )( ,i)(i,i)(i, )( ,s)(s,p)(p,e)(e,d)(d, )( ,p)(p,l)(l,i)(i, )( ,h)(h,a)(a,f)(f,t)(t,r)(r,i)(i,a)(a, )( ,w)(w,o)(o,u)(u,l)(l,a)(a,t)(f,t)(t,s)(s, )( ,i)(i,t)(t, )( ,i)(i,s)(s, )( ,i)(i,t)(t, )( ,i)(i,m)(m,p)(p,e)(e,r)(r,t)\n",
      "chars: nting the riken prous held invinhuam ii sped pli haftria woulafts it is it imper\n",
      "bigrams: (f,g)(k,o)(o,w)(i, )( ,l)(l,a)(a,n)(n,g)(g,h)(h,s)(s,e)(e,s)(s, )( ,e)(e,n)(n,g)(g,e)(e, )( ,l)(l,i)(i,m)(m,i)(i,s)(s, )( ,i)(i,n)(n, )( ,m)(m,a)(a,g)(g,a)(a,l)(l, )( ,s)(s,e)(e,c)(d,m)(m,o)(o,n)(n,y)(y, )( ,o)(o,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,t)(t,i)(i,m)(m,a)(a,t)(t,o)(o,n)(n,a)(a,l)(l, )( ,b)(b,e)(e, )( ,k)(k,e)(e,f)(f, )( ,a)(a,l)(l,g)(w,e)(e,b)(f,u)(u,l)(l,o)(o,n)(n,i)(i,e)(e,s)(s, )( ,o)\n",
      "chars: fkoi langhses enge limis in magal sedmony one six timatonal be kef alwefulonies \n",
      "bigrams: ( ,r)(r,o)(o,n)(n,a)(a,l)(l,u)(u,d)(d,u)(u,l)(l,i)(i, )( ,s)(s,i)(i,x)(x, )( ,v)(v,i)(i,n)(n,n)(n,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,e)(e, )( ,l)(l,a)(a,n)(n,g)(g,e)(e,l)(l,y)(y, )( ,d)(d,e)(e,g)(g,a)(a,m)(m, )( ,m)(m,i)(i,g)( ,l)(e,r)(r,d)(d,e)(e, )( ,b)(b,y)(y, )( ,a)(a,g)(g,e)(e, )( ,t)(t,h)(h,r)(r,o)(o,u)(u,s)(s,c)(c,a)(a,n)(n, )( ,a)(a,m)(m,e)(e,r)(r,i)(i,c)(c,a)(a,l)(l, )( ,i)(i,m)(m,a)(a,g)(g,i)\n",
      "chars:  ronaluduli six vinning the langely degam mi erde by age throuscan americal imag\n",
      "bigrams: ( ,n)(n,o)(o, )( ,a)(a,t)(t, )( ,a)(a,p)(p,p)(p,e)(e,c)(c,t)(t,e)(e,d)(d, )( ,j)(j,o)(o,w)(g, )( ,t)(t,o)(o, )( ,d)(d,i)(i,s)(s,e)(e,n)(n,t)(t, )( ,g)(g,r)(r,o)(o,m)(m, )( ,t)(t,h)(h,e)(e, )( ,n)(n,e)(e,t)(t, )( ,w)(w,a)(a,r)(r, )( ,a)(a,l)(l,l)(l,o)(o, )( ,t)(t,o)(o, )( ,t)(t,h)(h,e)(e, )( ,b)(b,a)(a,v)(b,r)(b,a)(a,r)(r,y)(y, )( ,i)(i,s)(s, )( ,c)(c,o)(o,m)(m,m)(m,a)(a,n)(n, )( ,c)(c,o)(o,m)(m,p)\n",
      "chars:  no at appected jog to disent grom the net war allo to the babbary is comman com\n",
      "================================================================================\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2100: 1.686962 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2200: 1.678721 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2300: 1.638216 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2400: 1.649317 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2500: 1.671455 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2600: 1.641081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2700: 1.648306 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2800: 1.638032 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 2900: 1.631578 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3000: 1.638287 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "bigrams: (t,a)(a,d)(d,s)(s, )( ,m)(m,a)(a,s)(s,h)(h,o)(o, )( ,d)(d,u)(u,p)(p,a)(a,n)(n,d)(d,s)(s, )( ,s)(s,a)(a,m)(m,p)(p,l)(l,e)(e, )( ,m)(m,a)(a,h)(h,a)(a, )( ,s)(s,y)(y,m)(m,b)(b,u)(u,t)(t, )( ,t)(t,h)(h,u)(u,r)(r,s)(s,h)(h,a)(a,n)(n, )( ,m)(m,a)(a,p)(p,h)(h,y)(y, )( ,t)(t,o)(o, )( ,i)(i,n)(n, )( ,p)(p,l)(l,a)(a,y)(y,i)(i,n)(n,g)(g, )( ,o)(o,v)(f, )( ,n)(n,i)(i,n)(n,e)(e, )( ,z)(f,o)(o,m)(m,e)(e, )( ,o)\n",
      "chars: tads masho dupands sample maha symbut thurshan maphy to in playing of nine fome \n",
      "bigrams: (q,r)(u,f)(f,u)(u,t)(t, )( ,a)(a,m)(m,e)(e,r)(r,i)(i,c)(c,a)(a,n)(n, )( ,a)(a, )( ,c)(c,o)(o,l)(l,l)(l,e)(e,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,s)(s,y)(y,r)(r,e)(e,s)(s, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e,o)(o,l)(l,o)(o,g)(g,y)(y, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,s)(s, )( ,t)(t,r)(r,a)(a,f)(f,t)(t, )( ,o)(o,f)(f, )( ,h)(h,o)(o, )( ,l)(l,a)(a,k)(k,e)(e, )( ,f)(f,o)(o,u)(u,r)(r, )( ,w)(w,o)(o,r)(r,d)\n",
      "chars: qufut american a collection syres and theology which s traft of ho lake four wor\n",
      "bigrams: (s,e)(e,y)(y, )( ,o)(o,c)(c,c)(c,o)(o,v)(v,e)(e,d)(d, )( ,r)(r,o)(o,b)(m,p)(p, )( ,n)(n,a)(a,t)(t,u)(u,a)(a,l)(l, )( ,e)(e,x)(x,p)(p,e)(e,r)(r,i)(i,a)(a,g)(g,i)(i,n)(n,g)(g, )( ,c)(c,a)(a,r)(r,e)(e,n)(n,t)(t,r)(r,e)(e,s)(s,a)(a,r)(r,a)(a,r)(r, )( ,a)(a,r)(r,m)(m,u)(u,l)(l,t)(t, )( ,s)(s,a)(a,l)(l,e)(e,s)(s, )( ,i)(i,n)(n,h)(t,e)(e,r)(r,e)(e,d)(d, )( ,e)(e,m)(m,p)(p,e)(e,r)(r,o)(o,l)(l,a)(a,n)(n,e)\n",
      "chars: sey occoved romp natual experiaging carentresarar armult sales intered emperolan\n",
      "bigrams: (l,x)(m,i)(i,r)(r,e)(e, )( ,t)(t,h)(h,e)(e,o)(o,r)(r,y)(y, )( ,s)(s,e)(e,r)(r,i)(i,e)(e,s)(s, )( ,c)(c,o)(o,y)(y,s)(s,t)(t,i)(i,o)(o,n)(n, )( ,c)(c,o)(o,m)(m,p)(p,a)(a,i)(i,m)(m, )( ,f)(f,o)(o,r)(r,c)(c,e)(e, )( ,o)(o,n)(n,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,o)(o,f)(f, )( ,h)(h,e)(e,r)(r,u)(u,n)(n,d)(d,i)(i,n)(n,g)(g, )( ,b)(b,y)(y, )( ,g)(g,a)(a,r)(r,d)(d, )( ,p)(p,l)(l,a)(a,y)(y, )( ,h)(h,u)\n",
      "chars: lmire theory series coystion compaim force one three of herunding by gard play h\n",
      "bigrams: (j,h)(u,s)(s,h)(h, )( ,o)(o,f)(f, )( ,m)(m,r)(r,a)(a,n)(n, )( ,b)(b,r)(r,a)(a,s)(s,t)(t,e)(e,r)(r, )( ,c)(c,a)(a,w)(w,d)(u, )(o,n)(n,o)(o, )( ,d)(d,e)(e,l)(l,e)(e,w)(g,r)(r,e)(e,s)(s, )( ,a)(a,u)(u,t)(t,h)(h,o)(o,r)(r,t)(t, )( ,a)(a,n)(n,d)(d, )( ,c)(c,i)(i,t)(t,u)(u,e)(e,n)(n,t)(t,i)(i,a)(a,s)(s,i)(i,a)(a,n)(n, )( ,s)(s,u)(u,b)(l,t)(t,e)(e,a)(a,t)(t,i)(i,t)(t,y)(y, )( ,s)(s,h)(h,o)(o,t)(t, )( ,o)\n",
      "chars: jush of mran braster cawuono delegres authort and cituentiasian sulteatity shot \n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3100: 1.607790 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3200: 1.626773 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3300: 1.615313 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3400: 1.642566 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3500: 1.631061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3600: 1.643299 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3700: 1.617401 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 3800: 1.611620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 3900: 1.598622 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 4000: 1.620122 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "bigrams: (f,z)(j,i)(i,c)(c, )( ,w)(w,a)(a,s)(s, )( ,b)(b,e)(e,a)(a,g)(g,u)(u,t)(t,e)(e,d)(d, )( ,i)(i,n)(n,c)(c,l)(l,i)(i,b)(b,b)(b,y)(y, )( ,d)(d,i)(i,v)(v,i)(i,n)(n,e)(e, )( ,o)(o,w)(w,n)(n, )( ,a)(a,n)(n,d)(d, )( ,p)(p,a)(a,p)(p,u)(u,l)(l,a)(a,r)(y, )( ,s)(s,e)(e,r)(r,v)(v,o)(o,u)(u,n)(n,d)(d,i)(i,n)(n,g)(g, )( ,o)(o,f)(f, )( ,b)(b,e)(e,c)(c,o)(o,m)(m,e)(e, )( ,e)(e,x)(x,t)(t,i)(i,n)(n,t)(t,i)(i,c)(c,e)\n",
      "chars: fjic was beaguted inclibby divine own and papulay servounding of become extintic\n",
      "bigrams: (s,s)(s, )( ,j)(j,e)(e,r)(r,r)(r,i)(i,b)(b,e)(e,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,a)(a,n)(n,d)(d, )( ,d)(d,e)(e,c)(c,i)(i,t)(t,e)(e,d)(d, )( ,w)(w,i)(i,t)(t,h)(h, )( ,t)(t,h)(h,e)(e, )( ,w)(w,o)(o,r)(r,l)(l,d)(d, )( ,o)(o,r)(r, )( ,e)(e,n)(n,d)(d, )( ,d)(d,e)(e,c)(c,i)(i,v)(v,i)(i,d)(d,a)(a,t)(t,o)(o,r)(r, )( ,w)(z, )( ,t)(t,h)(h,e)(e, )( ,f)(f,o)(o,r)(r, )( ,t)(t,h)(h,e)(e, )( ,a)(a,f)(f,t)(t,e)\n",
      "chars: ss jerriberation and decited with the world or end decividator z the for the aft\n",
      "bigrams: (a,p)(p,o)(o,s)(s,e)(e, )( ,o)(o,f)(f, )( ,j)(j,u)(u,n)(n,u)(n,a)(a,m)(m,s)(s, )( ,s)(s,u)(u,c)(c,c)(c,e)(e,s)(s,s)(s, )( ,p)(p,r)(r,o)(o,m)(m,a)(a,d)(d,e)(e,r)(r, )( ,b)(b,y)(y, )( ,m)(m,o)(o,r)(r,e)(e, )( ,i)(i,n)(n, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,h)(h,i)(i,s)(s, )( ,p)(p,e)(e,r)(r,f)(f,o)(o,r)(r,m)(m,e)(e,d)(d,e)(e,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)\n",
      "chars: apose of junnams success promader by more in one nine seven his performedes of t\n",
      "bigrams: (p,w)(i,j)(p,v)(z, )( ,e)(e,u)(u,e)(e,r)(r,f)(f,i)(i,c)(c,k)(k, )( ,t)(t,h)(h,e)(e, )( ,v)(v,i)(i,e)(e,w)(w, )( ,p)(p,e)(e,r)(r,m)(m,a)(a,n)(n, )( ,b)(b,e)(e,i)(i,n)(n,g)(g, )( ,y)(y,e)(e,a)(a,r)(r, )( ,b)(b,y)(y, )( ,t)(t,h)(h,e)(e, )( ,u)(u,g)(g,h)(h,t)(t,a)(a,n)(n,t)(t, )( ,h)(h,e)(e,r)(r,n)(n,o)(o,t)(t,e)(e, )( ,o)(o,f)(f, )( ,a)(a, )( ,e)(e,q)(q,u)(u,i)(a,l)(l,s)(s, )( ,u)(u,s)(s,i)(i,e)(e, )\n",
      "chars: pipz euerfick the view perman being year by the ughtant hernote of a equals usie\n",
      "bigrams: (k,q)(m,t)(t,i)(i,o)(o,n)(n, )( ,f)(f,o)(o,r)(r, )( ,i)(i,n)(n,i)(i,t)(t,e)(e, )( ,t)(t,o)(o,g)(g,e)(e,n)(n,t)(t, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,m)(m,a)(a,g)(g,r)(r,a)(a,p)(p,h)(k,u)(w,a)(a,r)(r,e)(e, )( ,o)(o,v)(v,e)(e,r)(r,s)(s,e)(e,e)(e, )( ,c)(c, )( ,b)(b,e)(e, )( ,c)(c,l)(l,a)(a,s)(s,s)(s, )( ,o)(o,f)(f,f)(f,i)(i,c)(c,i)(i,t)(t,y)(y, )( ,u)(u,r)(r,i)(i,g)(g,h)(h,t)(t, )( ,t)(t,h)(h,e)\n",
      "chars: kmtion for inite togent of the magrapkware oversee c be class officity uright th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4100: 1.598488 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4200: 1.594494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 4300: 1.578269 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4400: 1.562752 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 4500: 1.577165 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 4600: 1.574889 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 4700: 1.579986 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 4800: 1.586974 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 4900: 1.580915 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5000: 1.557606 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "================================================================================\n",
      "bigrams: (z,t)(z, )( ,i)(i,s)(s, )( ,s)(s,i)(i,n)(n,c)(c,e)(e,s)(s, )( ,b)(b,a)(a,l)(l,l)(l, )( ,w)(w,a)(a,s)(s, )( ,b)(b,a)(a,t)(t,t)(t,l)(l,e)(e, )( ,m)(m,e)(e,m)(m,o)(o,r)(r,a)(a,n)(n,d)(d, )( ,i)(i,i)(i, )( ,s)(s,u)(u,c)(c,c)(c,e)(e,s)(s,s)(s,a)(a,l)(l, )( ,r)(r,e)(e,l)(l,a)(a,w)(w, )( ,c)(c,a)(a,p)(p,e)(e,l)(l,f)(f, )( ,t)(t,r)(r,y)(y,d)(p, )( ,o)(o,f)(f,t)(t,e)(e,n)(n, )( ,o)(o,r)(r, )( ,a)(a,n)(n, )\n",
      "chars: zz is sinces ball was battle memorand ii successal relaw capelf tryp often or an\n",
      "bigrams: (j,s)(l,z)(z,i)(i,e)(e,s)(s, )( ,p)(p,r)(r,e)(e,m)(m,a)(a,n)(n,t)(t, )( ,d)(d,i)(i,r)(r,e)(e,c)(c,t)(t,s)(s, )( ,s)(s,t)(t,r)(r,e)(e,s)(s,e)(e,n)(n,t)(t, )( ,o)(o,f)(f, )( ,b)(b,b)(b,r)(r,y)(y, )( ,c)(c,a)(a,r)(r,m)(m,i)(i,n)(n,g)(g, )( ,i)(i,n)(n, )( ,u)(u,r)(r,g)(g,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,c)(c,o)(o,n)(n,f)(f,e)(e,m)(m,p)(p,e)(e,d)(d, )( ,b)(b,y)(y, )( ,h)(h,i)(i,s)(s, )( ,b)(b,e)(e, )\n",
      "chars: jlzies premant directs stresent of bbry carming in urged the confemped by his be\n",
      "bigrams: (c,n)(n,s)(s, )( ,c)(c,o)(o,v)(v,e)(e,r)(r,y)(y, )( ,e)(e, )( ,w)(w,r)(r,i)(i,t)(t,i)(i,c)(c, )( ,o)(o,f)(f, )( ,o)(o,t)(t,h)(h,e)(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,w)(w,a)(a,s)(s, )( ,s)(s,t)(t,i)(i,l)(l,l)(l, )( ,a)(a,u)(u,t)(t,h)(h,e)(e,r)(r, )( ,v)(v,i)(s,p)(p,a)(a,n)(n,i)(i,g)(g,o)(o,s)(s,e)(e,d)(d, )( ,a)(a,n)(n,d)(d, )( ,m)(m,u)(u,t)(t,i)(i,t)(t,y)(y, )( ,d)(d,i)(i,r)(r,b)(b,o)(o,r)(r,t)(t,h)\n",
      "chars: cns covery e writic of other the was still auther vspanigosed and mutity dirbort\n",
      "bigrams: (b,a)(a,r)(r,c)(c,o)(o,t)(t,e)(e,s)(s, )( ,d)(d,e)(e,m)(m,o)(o,r)(r,e)(e, )( ,f)(f,o)(o,r)(r, )( ,c)(c,o)(o,p)(p, )( ,i)(i,s)(s, )( ,n)(n,o)(o,r)(r,d)(d, )( ,r)(r,e)(e,s)(s,i)(i,g)(g,n)(n,e)(e,n)(n,t)(t, )( ,b)(b,y)(y, )( ,h)(h,u)(u,d)(d, )( ,i)(i,s)(s, )( ,d)(d,u)(u,e)(e, )( ,s)(s, )( ,d)(d,a)(a,v)(v,i)(i,d)(d,e)(e,m)(m, )( ,i)(i,s)(s, )( ,r)(r,e)(e,g)(g,i)(i,o)(o,n)(n, )( ,o)(o,n)(n,e)(e, )( ,i)\n",
      "chars: barcotes demore for cop is nord resignent by hud is due s davidem is region one \n",
      "bigrams: (d,g)(b,o)(o,t)(t, )( ,r)(r,e)(e,f)(f,i)(i,t)(t, )( ,e)(e,x)(x,i)(i,s)(s,h)(s,e)(e,d)(d, )( ,t)(t,o)(o, )( ,h)(h,a)(a,s)(s,r)(r,i)(i,c)(c, )( ,a)(a,c)(c,t)(t,u)(u,r)(r,o)(o,p)(p,l)(l,e)(e, )( ,f)(f,r)(r,i)(i,e)(e,n)(n,c)(c,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e,y)(y, )( ,t)(t,h)(h,e)(e, )( ,e)(e,a)(a,r)(r,l)(l,i)(i,e)(e, )( ,o)(o,r)(r,d)(d,a)(a,b)(b,r)(r,a)(a, )( ,a)(a,r)(r,t)(t, )( ,i)(i,n)(n, )\n",
      "chars: dbot refit exissed to hasric acturople frience of they the earlie ordabra art in\n",
      "================================================================================\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5100: 1.548037 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5200: 1.533588 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 5300: 1.527149 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5400: 1.524798 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 5500: 1.512233 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 5600: 1.526499 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 5700: 1.514347 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 5800: 1.525224 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 5900: 1.524578 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 6000: 1.494202 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "================================================================================\n",
      "bigrams: (p,c)(c,o)(o,r)(r,t)(t, )( ,o)(o,f)(f, )( ,a)(a, )( ,h)(h,i)(i,g)(g,h)(h,o)(o,o)(o,d)(d, )( ,a)(a,m)(m,e)(e,r)(r,i)(i,c)(c,a)(a,n)(n, )( ,n)(n,o)(o,t)(t, )( ,m)(m,a)(a,n)(n,y)(y, )( ,b)(b,e)(e,a)(a,n)(n, )( ,a)(a,c)(c,t)(t,a)(a,i)(i,d)(d,e)(e,d)(d, )( ,s)(s,t)(t,a)(a,t)(t,e)(e, )( ,i)(i,s)(s, )( ,t)(t,h)(h,e)(e, )( ,c)(c,i)(i,t)(t,y)(y, )( ,o)(o,f)(f, )( ,o)(o,n)(n,e)(e, )( ,t)(t,h)(h,i)(i,m)(m, )\n",
      "chars: pcort of a highood american not many bean actaided state is the city of one thim\n",
      "bigrams: (y,a)(a,b)(b,l)(l,e)(e,s)(s, )( ,a)(a,f)(f,t)(t,h)(h,i)(i,f)(f, )( ,n)(n,u)(u,m)(m,b)(b,e)(e,r)(r,t)(t, )( ,o)(o,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,n)(n,i)(i,n)(n,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,h)(h,i)(i,m)(m, )( ,b)(b,a)(a,n)(n,d)(d, )( ,s)(s, )( ,c)(c,o)(o,r)(r,n)(n,s)(s,i)(i,b)(b,l)(l,e)(e, )( ,a)(a,r)(r,e)(e, )( ,m)(m,o)(o,s)(s,t)(t, )( ,t)(t,h)(h,a)(a,t)(t, )( ,a)(a,l)(l,f)(f,o)(o,u)\n",
      "chars: yables afthif numbert one six nine three him band s cornsible are most that alfo\n",
      "bigrams: (f,r)(r,o)(o,r)(r,d)(d, )( ,h)(h,e)(e, )( ,a)(a,b)(b,s)(s,t)(t, )( ,l)(l,a)(a,k)(k,e)(e,o)(o,n)(n,a)(a, )( ,a)(a,s)(s,s)(s,i)(i,v)(v,e)(e,d)(d, )( ,a)(a,s)(s, )( ,a)(a, )( ,f)(f,o)(o,u)(u,n)(n,d)(d,a)(a,t)(t,i)(i,o)(o,n)(n,a)(a,l)(l, )( ,c)(c,a)(a,n)(n, )( ,r)(r,o)(o,m)(m,a)(a,i)(i,n)(n,h)(h,y)(y, )( ,t)(t,y)(y,p)(s,k)(k, )( ,c)(c,o)(o,s)(s, )( ,i)(i,n)(n,f)(f,l)(l,i)(i,c)(c,t)(t, )( ,i)(i,n)(n, )\n",
      "chars: frord he abst lakeona assived as a foundational can romainhy tysk cos inflict in\n",
      "bigrams: (s,u)(u,b)(b, )( ,a)(a,s)(s, )( ,t)(t,h)(h,o)(o,u)(u,g)(g,h)(h,t)(t,s)(s, )( ,a)(a,u)(u,g)(g,u)(u,s)(s,t)(t,m)(m,e)(e,n)(n,t)(t, )( ,o)(o,f)(f, )( ,m)(m,u)(u,s)(s,i)(i,c)(c, )( ,i)(i,n)(n,c)(c,l)(l,u)(u,d)(d,i)(i,n)(n,g)(g, )( ,c)(c,o)(o,m)(m,p)(p,a)(a,n)(n,a)(a,r)(r,d)(d,s)(s, )( ,a)(a,l)(l,b)(b, )( ,n)(n,a)(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,s)(s,t)(t,o)(o,r)(r,e)(e, )( ,u)(u,s)(s,e)(e,x)(n,z)(z,i)\n",
      "chars: sub as thoughts augustment of music including companards alb nations store usenz\n",
      "bigrams: (w,t)(g,s)(s, )( ,e)(e,a)(a,c)(c,e)(e,r)(r,b)(b,o)(o,s)(s,a)(a, )( ,i)(i,s)(s,s)(s,u)(u,e)(e, )( ,f)(f,a)(a,v)(v,e)(e, )( ,u)(u,s)(s,e)(e,d)(d, )( ,s)(s,h)(h,a)(a,r)(r,d)(d,i)(i,n)(n,g)(g, )( ,e)(e,d)(u,r)(r,o)(o,m)(m,b)(b,a)(a,n)(n,s)(s, )( ,d)(d,i)(i,f)(f,f)(f,e)(e,r)(r,e)(e,n)(n,c)(c,l)(l,e)(e,s)(s,t)(t, )( ,c)(c,l)(l,a)(a,s)(s,s)(s,i)(i,c)(c,a)(a,l)(l, )( ,p)(p,r)(r,o)(o,c)(c,e)(e,s)(s,s)(s,i)\n",
      "chars: wgs eacerbosa issue fave used sharding eurombans differenclest classical process\n",
      "================================================================================\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6100: 1.518003 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6200: 1.483717 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 6300: 1.493480 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6400: 1.499413 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 6500: 1.507896 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 6600: 1.542714 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 6700: 1.527806 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 6800: 1.552272 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6900: 1.531355 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 7000: 1.525970 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "bigrams: (o,v)(v,a)(a,r)(r,d)(d,s)(s, )( ,a)(a,n)(n,d)(d, )( ,s)(s,u)(u,i)(i,v)(v,e)(e,s)(s, )( ,g)(g,a)(a,b)(b,o)(o,n)(n, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,m)(m,i)(i,t)(t,h)(h, )( ,t)(t,o)(o, )( ,l)(l,a)(a,y)(y,s)(s, )( ,a)(a,n)(n,d)(d, )( ,m)(m,a)(a,y)(y, )( ,a)(a,f)(f,r)(r,i)(i,c)(c,u)(u,s)(s,l)(l,y)(y, )( ,c)(c,e)(e,n)(n,t)(t,e)(e,r)(r,s)(s, )( ,w)(w,a)(a,s)(s, )( ,s)(h,i)(i,m)(m,e)(e, )\n",
      "chars: ovards and suives gabon one nine mith to lays and may africusly centers was hime\n",
      "bigrams: (h,g)(e, )( ,o)(o,r)(r,g)(g,i)(i,n)(n,g)(g,s)(s, )( ,a)(a,r)(r,e)(e, )( ,a)(a, )( ,w)(w,i)(i,l)(l,l)(l,s)(s, )( ,t)(t,h)(h,e)(e, )( ,c)(c,l)(l,a)(a,i)(i,m)(m,a)(a,t)(t, )( ,a)(a, )( ,m)(m,e)(e,p)(p,e)(e,r)(r, )( ,a)(a,n)(n,d)(d, )( ,p)(p,e)(e,r)(r, )( ,a)(a,s)(s, )( ,a)(a,g)(g,e)(e, )( ,d)(d,s)(s,c)(c,e)(e,m)(p,t)(t,i)(i,v)(v,e)(e,l)(l,y)(y, )( ,c)(c,o)(o,u)(u,p)(p,o)(o,s)(s,t)(t,s)(s, )( ,i)(i,n)\n",
      "chars: he orgings are a wills the claimat a meper and per as age dsceptively couposts i\n",
      "bigrams: (o,l)(l,o)(o,e)(e,s)(s, )( ,a)(a,n)(n,d)(d, )( ,h)(h,e)(e,n)(n,a)(a,r)(r,i)(i,e)(e,d)(d, )( ,t)(t,o)(o, )( ,t)(t,h)(h,a)(a,t)(t, )( ,a)(a,f)(f,t)(t,e)(e,r)(r, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,o)(o,t)(t,h)(h,e)(e,r)(r,e)(e,s)(s, )( ,w)(w,r)(r,i)(i,t)(t,e)(e,r)(r, )( ,s)(s, )( ,n)(n,o)(o,t)(t, )( ,t)(t,h)(h,e)\n",
      "chars: oloes and henaried to that after three one eight and the otheres writer s not th\n",
      "bigrams: (d,c)(m,a)(a, )( ,e)(e,x)(x,p)(p,y)(y, )( ,k)(k,i)(i,l)(l,l)(l,a)(a,n)(n,c)(c,e)(e, )( ,a)(a,n)(n,d)(d, )( ,r)(r,a)(a,z)(z,e)(e, )( ,g)(g,a)(a,s)(s,s)(s,e)(e, )( ,o)(o,n)(n,l)(l,y)(y, )( ,t)(t,o)(o,o)(o,p)(p, )( ,e)(e,t)(t,r)(r,y)(y,s)(s,t)(t,e)(e,s)(s,e)(e, )( ,e)(e,l)(l,e)(e,m)(m,e)(e,n)(n,t)(t, )( ,l)(m,o)(o,n)(n,g)(g,s)(s, )( ,a)(a,n)(n,d)(d, )( ,v)(v,i)(i,r)(r,i)(i,t)(t,i)(i,n)(n,e)(e,r)(r,s)\n",
      "chars: dma expy killance and raze gasse only toop etrystese element mongs and viritiner\n",
      "bigrams: (t,a)(a, )( ,m)(m,a)(a,y)(y, )( ,t)(t,h)(h,e)(e, )( ,w)(w,e)(e,l)(l,l)(l, )( ,i)(i,n)(n,f)(f,l)(l,a)(a,t)(t,e)(e, )( ,s)(s,u)(u,c)(c,c)(c,e)(e,m)(m,b)(b,l)(l,e)(e, )( ,a)(a, )( ,f)(f,a)(a,i)(i,l)(l,e)(e,d)(d, )( ,a)(a,p)(p,r)(j,u)(u,n)(n,c)(c,t)(t,e)(e,d)(d, )( ,t)(t,o)(o, )( ,t)(t, )( ,d)(d,a)(a,y)(y,s)(s, )( ,i)(i,n)(n, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )\n",
      "chars: ta may the well inflate succemble a failed apjuncted to t days in one nine eight\n",
      "================================================================================\n",
      "Validation set perplexity: 4.02\n",
      "CPU times: user 21min 16s, sys: 3min 54s, total: 25min 11s\n",
      "Wall time: 5min 21s\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def exec_graph_bigram(graph):\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "      batches = bigram_train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      mean_loss += l\n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          for _ in range(5):\n",
    "            feed = bigram_sample(bigram_random_distribution())\n",
    "            bigram_sentence = bigram_characters(feed)[0]\n",
    "            sentence = bigram_first_characters(feed)[0]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = bigram_sample(prediction)\n",
    "              bigram_sentence += bigram_characters(feed)[0]\n",
    "              sentence += bigram_first_characters(feed)[0]\n",
    "            print('bigrams:', bigram_sentence)\n",
    "            print('chars:', sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = bigram_valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))\n",
    "        \n",
    "%time exec_graph_bigram(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity is better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "# output embeddings (IDs) instead of one-hot-encoded matrices\n",
    "\n",
    "class BigramEmbeddingBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = list()\n",
    "    for b in range(self._batch_size):\n",
    "      first_char = self._text[self._cursor[b]]\n",
    "      if self._cursor[b] + 1 == self._text_size:\n",
    "        second_char = ' '\n",
    "      else:\n",
    "        second_char = self._text[self._cursor[b] + 1]\n",
    "      batch.append(char2id(first_char) * vocabulary_size + char2id(second_char))\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "bigram_embed_train_batches = BigramEmbeddingBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "bigram_embed_valid_batches = BigramEmbeddingBatchGenerator(valid_text, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # big matrix\n",
    "  ifcox = tf.Variable(tf.truncated_normal([bigram_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    all_gates = tf.nn.embedding_lookup(ifcox, i) + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "    update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int64, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int64, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def exec_graph_bigram_embed(graph):\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "      batches = bigram_embed_train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      mean_loss += l\n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        # convert to one-hot-encodings\n",
    "        noembed_labels = np.zeros(predictions.shape)\n",
    "        for i, j in enumerate(labels):\n",
    "            noembed_labels[i, j] = 1.0\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, noembed_labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          for _ in range(5):\n",
    "            feed = bigram_sample(bigram_random_distribution())\n",
    "            bigram_sentence = bigram_characters(feed)[0]\n",
    "            sentence = bigram_first_characters(feed)[0]\n",
    "            # convert to embedding\n",
    "            feed = [np.argmax(feed)]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = bigram_sample(prediction)\n",
    "              bigram_sentence += bigram_characters(feed)[0]\n",
    "              sentence += bigram_first_characters(feed)[0]\n",
    "              feed = [np.argmax(feed)]\n",
    "            print('bigrams:', bigram_sentence)\n",
    "            print('chars:', sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = bigram_embed_valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          labels = np.zeros((1, bigram_size))\n",
    "          labels[0, b[1]] = 1.0\n",
    "          valid_logprob = valid_logprob + logprob(predictions, labels)\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.590697 learning rate: 10.000000\n",
      "Minibatch perplexity: 728.29\n",
      "================================================================================\n",
      "bigrams: (z,h)(h,u)(y,n)(z,c)(o,p)( ,f)(z,q)(n,w)(v,l)(m,g)(z, )(b,s)(l,n)(m,m)(l,m)(y,d)(f,s)(z,u)(l,h)(z,y)(o,k)(n,c)(v,t)(r,l)(l,a)(o,l)(j,c)(h,w)(x,z)(n,z)(v,e)(a,b)(z,h)(z,e)(f,y)(c,g)(n,p)(n,r)(o,h)(f,l)(y,j)(p,l)(j,c)( ,z)(x,t)(n, )(t,s)(i,k)(v,n)(u,o)(v,r)( ,v)(y,h)(z,z)(w,p)(l,t)(t,z)(v,q)(t,k)(b,b)(u,x)(j,h)(c,d)(z,p)(k,t)(m,o)(o,j)(r,j)(g,t)(l,g)(n,m)(p,m)(x,a)(z,a)(p,d)(l,t)(l,a)(v,w)(l,p)(e,u)\n",
      "chars: zhyzo znvmzblmlyfzlzonvrlojhxnvazzfcnnofypj xntivuv yzwltvtbujczkmorglnpxzpllvle\n",
      "bigrams: (a,q)(t,h)(w,k)( ,k)(h, )(t,o)(j,l)(k, )(y,y)(d, )(x,q)(k,c)(j,u)(s,m)(b, )( ,r)(u,f)(c,h)(w,e)(y,j)(b,q)(p,e)(r,e)(t,b)(w,d)(s,q)(b,f)(m,o)(m, )(x, )(p,i)(u,s)(s,g)(n,c)(b,y)( ,i)(l,s)(j,m)(s,u)(l,t)(f,n)(p,h)(y,e)(q,s)(d,v)(l,d)(d,z)(b,g)(h,f)(r,k)(s,t)(y,m)(q,o)(h,y)(m,s)(o,w)(l,o)(e,s)(g,j)(q,l)(w,r)(b, )(c,z)(t,f)(h,z)(d,i)(x,t)(i,q)(q,e)(q,k)(m,m)(b,p)( ,i)(z,y)(k,j)(p,k)(a,a)(t,e)(v,u)(a,n)\n",
      "chars: atw htjkydxkjsb ucwybprtwsbmmxpusnb ljslfpyqdldbhrsyqhmolegqwbcthdxiqqmb zkpatva\n",
      "bigrams: (f,n)(g,l)(d,n)(p,z)( ,s)(j,p)(b,m)(v,l)(l,o)(w,v)(x,d)(b,o)(n,m)(l,j)(o,v)(j,c)(o,v)(s,e)(v,l)(u, )(w,s)(t,y)(p,w)(g,n)(i,f)(n,w)(q,s)(u,z)( ,w)(i,c)(p,n)( ,r)(p, )(o,w)(d,m)(n,k)(h,q)(x,t)(n,w)(r,s)(z,f)(z,s)(b,m)(i,a)( ,c)(w,d)(b,q)(y,w)(p,r)(d,h)(h,e)( ,d)( ,e)(p,v)(q,x)(r,e)(j,m)(d,o)(w,t)(a,d)(z,v)(i,g)(r,l)(d,q)(o,d)(n,o)(j,j)(s,x)(a,u)(p,j)(s,k)(j,h)(r,z)(l,b)(p,t)(z,a)(n,z)(f,g)(v, )(q,y)\n",
      "chars: fgdp jbvlwxbnlojosvuwtpginqu ip podnhxnrzzbi wbypdh  pqrjdwazirdonjsapsjrlpznfvq\n",
      "bigrams: (z,u)(s,q)(g, )(a,z)(n,o)(t,p)(p,c)(j,u)(a,f)(v,u)(k,a)(u,b)(z,o)(h,s)( ,c)( ,o)(p,o)(p,f)(f,k)(k,a)(g,w)(m,n)(s,h)(r,o)(q,y)(w,h)(n,d)(n,b)(n,z)(a,c)(m,x)(v,o)(t,r)(t,i)(m,x)(b,s)(g,h)(i,l)(w,m)( ,w)(r,d)(e,g)(j,p)(x,e)(t,g)(l,n)(o,m)( ,b)(x,a)(h,s)(t,m)(b,i)(w,h)(q,g)(q,v)(f,o)(u,n)(i,w)(i,g)(t,i)(e,p)(v,b)(r,q)(w,y)(d,y)(e,l)(y,g)(i,t)(v,c)( ,r)(e,z)(m,m)(t,y)(x,g)(m,j)(p,v)(s, )(m,u)(x,r)(d,h)\n",
      "chars: zsgantpjavkuzh  ppfkgmsrqwnnnamvttmbgiw rejxtlo xhtbwqqfuiitevrwdeyiv emtxmpsmxd\n",
      "bigrams: (n,b)(p,u)(a,q)(y,b)(o,z)(u,b)(l,w)(g,k)(g,u)(p,e)(z,k)(x,h)(m,t)( ,l)(d,z)(x,t)(n,u)(c,e)(j,d)(x,b)(f,d)(x,v)(j,c)(w,q)(h,f)(x,y)(m,u)( ,p)(j,r)(q,x)(p,i)(r,q)(z,a)(u,i)(u,a)(r,i)(p,e)(q,o)(y,c)(s,j)( ,i)(l,r)(q,v)(s,y)(j,r)(b,x)(b,z)(a,z)(o,m)(b,a)(x,d)(b,x)(i,d)(i,q)(j,z)(u,f)(j, )(w,p)(d,u)(y,a)(l,e)(f,x)(t,n)(s,j)(c,h)(c,m)(k,a)(e,e)(k,n)(d,q)(m,m)(u,l)(c,y)(b,k)(j,y)(k,u)(h,t)(z,m)(y,h)(v,v)\n",
      "chars: npayoulggpzxm dxncjxfxjwhxm jqprzuurpqys lqsjbbaobxbiijujwdylftscckekdmucbjkhzyv\n",
      "================================================================================\n",
      "Validation set perplexity: 672.13\n",
      "Average loss at step 100: 5.348527 learning rate: 10.000000\n",
      "Minibatch perplexity: 113.21\n",
      "Validation set perplexity: 109.07\n",
      "Average loss at step 200: 4.000205 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.69\n",
      "Validation set perplexity: 23.23\n",
      "Average loss at step 300: 2.905498 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.35\n",
      "Validation set perplexity: 11.31\n",
      "Average loss at step 400: 2.307254 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.81\n",
      "Validation set perplexity: 8.99\n",
      "Average loss at step 500: 2.121775 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.09\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 600: 2.029358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 700: 1.936497 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 800: 1.867437 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 900: 1.866098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1000: 1.855476 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "================================================================================\n",
      "bigrams: (l,b)(l,e)(e,d)(d, )( ,r)(r,e)(e,a)(a,r)(r,r)(r,e)(e,d)(d, )( ,m)(m,s)(s,i)(i,r)(r, )( ,a)(a,t)(t, )( ,t)(t,h)(h,e)(e, )( ,s)(s,p)(p,o)(o,o)(o,d)(d,e)(e, )( ,i)(i,s)(s, )( ,t)(t,h)(h,e)(e, )( ,r)(r,u)(u,s)(s,s)(s,t)(t, )( ,s)(s,y)(y,s)(s,t)(t, )( ,s)(s,e)(e,r)(r,v)(v,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,e)(e, )( ,c)(c,a)(a, )( ,a)(a,u)(u,s)(s,e)(e,d)(d, )( ,a)(a,r)(r,y)(y,a)(a, )( ,b)(b,e)(e, )( ,s)(s,e)\n",
      "chars: lled rearred msir at the spoode is the russt syst serving the ca aused arya be s\n",
      "bigrams: (a,c)(c,e)(e,d)(d,o)(o,n)(n, )( ,g)(g,a)(a,m)(m,e)(e, )( ,s)(s,u)(u,t)(t,u)(u,r)(r,a)(a,t)(t,i)(i,b)(m,e)(e,s)(s, )( ,a)(a,r)(r,m)(m,e)(e,n)(n,c)(c,e)(e,d)(d, )( ,a)(a,n)(n,d)(d, )( ,f)(f,o)(o,r)(r,c)(c,e)(e,t)(t, )( ,a)(a,n)(n,d)(d, )( ,c)(c,h)(o,m)(m,a)(a,l)(l, )( ,p)(p,o)(o,p)(p,u)(u,l)(g,w)(s,s)(s, )( ,n)(n,u)(u, )( ,p)(p,h)(h,o)(o,p)(l,u)(u,e)(e,m)(m,a)(a,t)(t,i)(i,z)(z,e)(e, )( ,t)(t,h)(h,e)\n",
      "chars: acedon game suturatimes armenced and forcet and comal popugss nu pholuematize th\n",
      "bigrams: (a,g)(g,e)(e,m)(m,s)(s, )( ,b)(b,a)(a,g)(g,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,c)(c,o)(l,u)(u,d)(d,o)(o,l)(l,s)(s, )( ,w)(w,a)(a,s)(s, )( ,a)(a,g)(g,r)(r,a)(a,p)(p,l)(h,n)(s,o)(o,r)(r,i)(i,c)(c,a)(a,n)(n, )( ,t)(t,h)(h,e)(e, )( ,a)(a, )( ,p)(p,r)(r,e)(e,a)(a,t)(t,e)(e,m)(m, )( ,d)(d,i)(i,v)(v,i)(i,r)(r, )( ,w)(w,o)(o,r)(r,k)(k,e)(e,d)(d, )( ,i)(i,n)(n, )( ,n)(n,e)(e,c)(c,u)(u,s)(s, )( ,m)(m,o)(o,d)\n",
      "chars: agems bage five cludols was agraphsorican the a preatem divir worked in necus mo\n",
      "bigrams: (c,x)(l,r)(u,r)(r,y)(y, )( ,i)(i,n)(n, )( ,h)(h,i)(i,r)(r,e)(e,f)(f,e)(e,r)(r,e)(b,s)(s, )( ,o)(o,f)(f, )( ,r)(r,a)(a,i)(i,c)(b,r)(r,i)(i,c)(c,a)(a,r)(r,e)(e,s)(s, )( ,f)(f,i)(i,r)(r,s)(s,t)(t, )( ,d)(d,r)(u,c)(c,o)(o,m)(m,p)(p,e)(e,r)(r,e)(e,d)(d, )( ,p)(p,r)(r,o)(o, )( ,a)(a,l)(l,e)(e, )( ,o)(o,f)(f, )( ,g)(g,e)(e,n)(n,e)(e, )( ,c)(c,a)(a,s)(s, )( ,p)(p,h)(b,b)(b,l)(l,a)(a,t)(t,i)(i,o)(o,n)(n, )\n",
      "chars: clury in hireferbs of raibricares first ducompered pro ale of gene cas pbblation\n",
      "bigrams: (l,t)(t,o)(o,n)(n, )( ,a)(a,f)(f,t)(t,e)(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,f)(f,a)(a,c)(c,k)(p,s)(s,u)(u,s)(s, )( ,f)(f,o)(o,r)(r, )( ,t)(t,h)(h,e)(e, )( ,e)(e,v)(v,e)(e,r)(r,y)(y, )( ,e)(e,n)(n,e)(e,r)(r,a)(a,t)(t,e)(e, )( ,a)(a,c)(c,h)(h,r)(r,e)(e,n)(n,t)(t, )( ,o)(o,f)(f, )( ,e)(i,u)(u,l)(l,d)(d,e)(e,n)(n,s)(s, )( ,w)(w,a)(a, )( ,i)(i,n)(n, )( ,h)(h,i)(i,s)(s, )( ,r)(r,e)(e,f)(f,e)(e,c)(c,t)(t, )\n",
      "chars: lton after the facpsus for the every enerate achrent of iuldens wa in his refect\n",
      "================================================================================\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1100: 1.790455 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1200: 1.755635 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1300: 1.732132 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1400: 1.738616 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1500: 1.730001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1600: 1.732546 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 1700: 1.693041 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 1800: 1.653768 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 1900: 1.623216 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2000: 1.671230 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "bigrams: (e,x)(r,o)(o,r)(r, )( ,o)(o,f)(f, )( ,p)(p,a)(a,r)(r,w)(p,t)(t,i)(i,c)(c, )( ,t)(t,h)(h,e)(e, )( ,f)(f,u)(u,l)(l,l)(l, )( ,s)(s,k)(y,l)(l,e)(e,t)(t, )( ,c)(g,a)(a,s)(s,s)(s,i)(i,n)(n,g)(g, )( ,s)(s,o)(o,r)(r,l)(l,y)(y, )( ,i)(i,t)(t,e)(e,n)(n, )( ,f)(f,r)(r,o)(o,m)(m, )( ,t)(t,h)(h,e)(e, )( ,f)(f,o)(o,r)(r,e)(e,b)(b,r)(r,a)(a,n)(n,i)(i,c)(c,e)(e, )( ,n)(n,o)(o,r)(r,m)(m,a)(a,n)(n, )( ,l)(l,a)(a,n)\n",
      "chars: eror of parptic the full sylet gassing sorly iten from the forebranice norman la\n",
      "bigrams: (f,g)(y,o)(o,n)(n, )( ,s)(s,t)(t, )( ,m)(m,o)(o,d)(d,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,o)(o,n)(n,g)(g,i)(i,o)(o,n)(n, )( ,s)(s,t)(t,a)(a,b)(b,l)(l,e)(e, )( ,v)(v,a)(a,l)(l,i)(i,t)(t,i)(i,n)(n,e)(e, )( ,v)(v,a)(a,n)(n,t)(t, )( ,t)(t,o)(o, )( ,t)(t,h)(h,e)(e,n)(n, )( ,u)(u,l)(l,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,a)(a,t)(t, )( ,a)(a,m)(m,e)(e,r)(r,i)(i,c)(c,a)(a,n)(n, )( ,o)(o,c)(c,c)(c,h)(h,e)(e,m)(m,e)\n",
      "chars: fyon st mode five ongion stable valitine vant to then ulation at american occhem\n",
      "bigrams: (b,y)(y, )( ,a)(a,z)(l,g)(g,e)(e,o)(o,n)(n,s)(s, )( ,i)(i,n)(n, )( ,i)(i,t)(t,h)(h,e)(e,d)(d, )( ,t)(t,o)(o, )( ,f)(f,o)(o,o)(o,l)(l,o)(o,n)(n,s)(s, )( ,t)(t,h)(h,a)(a,t)(t, )( ,a)(a,v)(v,a)(a,i)(i,n)(n, )( ,a)(a,n)(n,d)(d, )( ,f)(f, )( ,s)(s,c)(c,i)(i,k)(p,t)(t,i)(i,n)(n,g)(g, )( ,b)(b,r)(r,o)(o,o)(o,n)(n, )( ,o)(o,f)(f, )( ,d)(d,e)(e,b)(b,a)(a,s)(s,s)(s,i)(i,n)(n,g)(g, )( ,a)(a,n)(n,o)(o,n)(n,o)\n",
      "chars: by algeons in ithed to foolons that avain and f scipting broon of debassing anon\n",
      "bigrams: (n,w)(w,e)(e,d)(d, )( ,a)(a,s)(s,t)(t,r)(r,e)(e,m)(m,o)(o,n)(n, )( ,i)(i,n)(n, )( ,c)(c,o)(o,m)(m,m)(m,i)(i,l)(l,i)(i,t)(t,y)(y, )( ,s)(s, )( ,p)(p,o)(o,i)(i,n)(n,t)(t, )( ,c)(c,o)(o,m)(m,p)(p,e)(e,r)(r,a)(a,n)(n,t)(t, )( ,s)(s, )( ,f)(f,o)(o,r)(r, )( ,m)(m,a)(a,n)(n,e)(e,t)(t, )( ,o)(o,r)(r,o)(o,p)(p,a)(a,b)(b,l)(l,i)(i,c)(l,i)(i,t)(t,y)(y, )( ,o)(o,r)(r, )( ,b)(b,u)(u,n)(n,g)(g,i)(i,a)(a,l)(l, )\n",
      "chars: nwed astremon in commility s point comperant s for manet oropablility or bungial\n",
      "bigrams: (k,e)(e,l)(l, )( ,o)(o,n)(n, )( ,o)(o,w)(w,n)(n, )( ,m)(m,i)(i,s)(s,s)(s,i)(i,n)(n,g)(g,e)(e,r)(r, )( ,s)(s, )( ,h)(h,a)(a,v)(v,e)(e, )( ,o)(o,f)(f, )( ,b)(b,i)(i,r)(r,o)(o,n)(n, )( ,d)(d, )( ,t)(t,h)(h,e)(e, )( ,s)(s,t)(t,r)(r,o)(o,n)(n,c)(c,e)(e, )( ,e)(e,n)(n,t)(t,i)(i,c)(c,i)(i,a)(a,c)(c,e)(e, )( ,o)(o,f)(f, )( ,f)(f,o)(o,u)(u,r)(r, )( ,e)(e,m)(m,m)(p, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,i)(i,n)\n",
      "chars: kel on own missinger s have of biron d the stronce enticiace of four emp seven i\n",
      "================================================================================\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2100: 1.654447 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2200: 1.646755 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2300: 1.607837 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2400: 1.622429 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2500: 1.643765 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 2600: 1.617055 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2700: 1.625042 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 2800: 1.606488 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 2900: 1.604173 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3000: 1.611993 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "bigrams: (o,w)(w,e)(e,d)(d, )( ,b)(b,r)(r,i)(i,c)(c,a)(a,n)(n, )( ,o)(o,f)(f, )( ,m)(m,o)(o,v)(v,e)(e,m)(m,e)(e,n)(n,t)(t,s)(s, )( ,f)(f,o)(o,r)(r,i)(i,t)(t,h)(h, )( ,m)(m,o)(o,r)(r,a)(a,i)(i,n)(n, )( ,o)(o,f)(f, )( ,a)(a,n)(n, )( ,a)(a,l)(l,g)(g,e)(e,o)(o,p)(p,a)(a, )( ,p)(p,o)(o,p)(p,u)(u,l)(l,a)(a,r)(r, )( ,t)(t,o)(o, )( ,t)(t,h)(h,e)(e, )( ,m)(m,u)(u,d)(d,h)(h,o)(o,r)(r,i)(i,t)(t,y)(y, )( ,l)(l,o)(o,s)\n",
      "chars: owed brican of movements forith morain of an algeopa popular to the mudhority lo\n",
      "bigrams: (q,q)(d,g)(g,e)(e,n)(n,t)(t, )( ,s)(s,u)(u,c)(c,h)(h,a)(a,p)(p,t)(t,i)(i,s)(s,h)(h,o)(o,n)(n, )( ,a)(a,n)(n,d)(d, )( ,c)(c,o)(o,n)(n,t)(t,i)(i,n)(n,u)(u,e)(e,n)(n,c)(c,e)(e,s)(s, )( ,e)(e, )( ,a)(a,m)(m,e)(e,r)(r,i)(i,v)(v,i)(i,n)(n,g)(g, )( ,t)(t,o)(o, )( ,b)(b,e)(e, )( ,n)(n,o)(o, )( ,i)(i,s)(s, )( ,o)(o,w)(w,n)(n, )( ,a)(a,s)(s, )( ,h)(h,e)(e,w)(b,e)(e,l)(l,l)(l, )( ,p)(p,r)(r,o)(o,v)(v,i)(i,e)\n",
      "chars: qdgent suchaptishon and continuences e ameriving to be no is own as hebell provi\n",
      "bigrams: (r,t)(t, )( ,a)(a,v)(v,a)(a,i)(i,l)(l,i)(i,c)(c,e)(e, )( ,m)(m,a)(a,h)(n,y)(y, )( ,t)(t,o)(o,o)(o,k)(k, )( ,c)(c,o)(o,n)(n,s)(s,i)(i,s)(s,t)(i,b)(b,l)(l,e)(e, )( ,a)(a,p)(p,p)(p,r)(r,o)(o,n)(n,i)(i,o)(o,s)(s,t)(t, )( ,d)(d,o)(o,w)(w,i)(i,n)(n,g)(g,s)(s, )( ,t)(t,o)(o, )( ,b)(b,e)(e,s)(s,t)(t, )( ,r)(r,e)(e,a)(a,m)(u,t)(t,i)(i,s)(s,h)(h, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,c)(c,o)(o,n)(n,s)(s,t)(t,i)\n",
      "chars: rt availice many took consisible approniost dowings to best reautish which const\n",
      "bigrams: (i,z)(z,a)(a,b)(b,l)(l,i)(i,s)(s,h)(h,e)(e,e)(e,r)(r, )( ,h)(h,a)(a,d)(d, )( ,a)(a,p)(p,p)(p,l)(l,y)(y, )( ,a)(a,c)(c,t)(t, )( ,l)(l,o)(o,s)(s,t)(t, )( ,m)(m,a)(a,y)(y, )( ,a)(a,g)(g,a)(a,i)(i,n)(n,i)(i,n)(n,g)(g,s)(s, )( ,c)(c,h)(h,i)(i,r)(r,d)(d,i)(i,n)(n,g)(g,a)(a,n)(n, )( ,t)(t,h)(h,e)(e, )( ,f)(f,i)(i,c)(c,t)(t,r)(r,e)(e,d)(d, )( ,a)(a, )( ,p)(p,e)(e,f)(f,e)(e,d)(d, )( ,l)(l,e)(e,u)(u,r)(r,d)\n",
      "chars: izablisheer had apply act lost may againings chirdingan the fictred a pefed leur\n",
      "bigrams: (m,q)( ,e)(e,a)(a,r)(r,t)(t,h)(h, )( ,a)(a,c)(c,c)(c,h)(h,a)(a,n)(n,d)(d,y)(y, )( ,s)(s, )( ,e)(e,n)(n,c)(c,o)(o,w)(w, )( ,j)(j,u)(u,r)(r,n)(n,e)(e,c)(c,h)(h,r)(r,e)(e,m)(m,a)(a,n)(n, )( ,p)(p,r)(r,o)(o,t)(t,e)(e,c)(c,t)(t, )( ,o)(o,f)(f, )( ,s)(s,c)(c,h)(h,n)(n,c)(c,e)(e, )( ,f)(f,r)(r,a)(a,n)(n,t)(t,e)(e,r)(r, )( ,f)(f,o)(o,r)(r,m)(m,a)(a,n)(n,t)(t,s)(s, )( ,w)(w,h)(h,o)(o,s)(s,e)(e,r)(r,t)(t,y)\n",
      "chars: m earth acchandy s encow jurnechreman protect of schnce franter formants whosert\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3100: 1.586279 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3200: 1.602283 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3300: 1.591349 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 3400: 1.618423 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 3500: 1.607436 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 3600: 1.624699 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 3700: 1.593564 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 3800: 1.589795 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 3900: 1.582394 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4000: 1.597152 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "bigrams: (e,x)(x,i)(i,z)(z,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,s)(a, )( ,s)(s, )( ,t)(t,r)(r,e)(e,t)(t,y)(y, )( ,b)(b,u)(u,c)(c,k)(k,e)(e,r)(r, )( ,t)(t, )( ,a)(a,n)(n,n)(n,e)(e,s)(s,s)(s,a)(a,l)(l, )( ,c)(c,o)(o,m)(m,m)(m,o)(o,n)(n, )( ,d)(d,r)(r,i)(i,f)(f,e)(e, )( ,b)(b,e)(e, )( ,a)(a,n)(n,d)(d, )( ,o)(o,p)(p,i)(i,t)(t,e)(e, )( ,p)(p,l)(l,a)(a,n)(n,i)(i,d)(d,o)(o, )( ,w)(w,a)(a,r)(r, )( ,n)(n,o)\n",
      "chars: exization of a s trety bucker t annessal common drife be and opite planido war n\n",
      "bigrams: (z,d)(l,f)(f, )( ,a)(a,t)(t, )( ,r)(r,o)(o,l)(l,l)(l,a)(a,m)(m,e)(e,n)(n,t)(t,s)(s, )( ,o)(o,r)(r, )( ,o)(o,n)(n,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,o)(o,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,o)(o,r)(r, )( ,c)(c,e)(e,l)(a,l)(l,i)(i,v)(v,i)(i,n)(n,g)(g, )( ,m)(m,a)(a,g)(g,u)(y, )( ,s)(s, )( ,o)(o,v)(v,e)(e,r)(r, )( ,f)(f,l)(l,u)(u,r)(f,f)(f, )( ,o)(o,r)(r, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)\n",
      "chars: zlf at rollaments or one zero one six or cealiving magy s over fluff or one eigh\n",
      "bigrams: (t,i)(i,m)(m,e)(e,n)(n,t)(t, )( ,a)(a,r)(r,t)(t,i)(i,c)(c,u)(u,l)(l,y)(y, )( ,o)(o,r)(r, )( ,c)(c,r)(r,i)(i,s)(s,t)(t,e)(e,r)(r, )( ,u)(u,n)(n,c)(c,e)(e, )( ,a)(a,m)(m,e)(e,r)(r,i)(i,c)(c,a)(a, )( ,c)(c,a)(a,p)(p,p)(p,e)(e, )( ,s)(s,y)(y,s)(s, )( ,m)(m,u)(n,o)(o,s)(s,e)(e, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,f)(f,o)(o,u)(u,r)(r, )( ,s)(s,u)(u,c)(c,h)(h, )( ,c)(c,h)(h,a)(a,r)(r,a)(a, )( ,o)(o,n)\n",
      "chars: timent articuly or crister unce america cappe sys mnose in the four such chara o\n",
      "bigrams: (o,o)(o,n)(n,i)(i,z)(z,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,h)(h,o)(o,t)(t,t)(t,e)(e,d)(d, )( ,b)(b,y)(y, )( ,o)(o,n)(n,l)(l,y)(y, )( ,r)(r,o)(o,d)(d,g)(g,e)(e,b)(b, )( ,c)(c,a)(a,n)(n, )( ,m)(m,o)(o,r)(r,e)(e, )( ,i)(i,n)(n,f)(f,u)(u,s)(s,i)(i,s)(s,m)(m, )( ,i)(i,s)(s, )( ,i)(i,s)(s, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )\n",
      "chars: oonization hotted by only rodgeb can more infusism is is two zero zero zero zero\n",
      "bigrams: (d,m)(k,g)(g, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,u)(u,n)(n,i)(i,t)(t,e)(e,d)(d, )( ,r)(r,e)(e,r)(r,c)(c,h)(h, )( ,o)(o,r)(r, )( ,t)(t,h)(h,e)(e, )( ,p)(p,a)(a,n)(n,s)(s,t)(t,a)(a,n)(n,t)(t, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,o)(o,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)\n",
      "chars: dkg and the united rerch or the panstant of the one one one nine zero zero zero \n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4100: 1.573899 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4200: 1.575670 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 4300: 1.553351 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 4400: 1.549044 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 4500: 1.553142 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 4600: 1.557021 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 4700: 1.560564 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 4800: 1.568315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 4900: 1.559725 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5000: 1.540579 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      "bigrams: (w,h)(h,a)(a,t)(t,u)(u,r)(r,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,w)(w,o)(o,r)(r,k)(k, )( ,t)(t,o)(o, )( ,k)(k,i)(i,l)(l,l)(l, )( ,w)(w,r)(r,i)(i,t)(t,i)(i,s)(s,h)(h,e)(e,d)(d, )( ,t)(t,o)(o, )( ,o)(o,n)(n,e)(e, )( ,b)(b,e)(e,i)(i,g)(g,h)(h,b)(h,z)(v,e)(e,l)(l,m)(m,u)(u,d)(d,i)(i,s)(s,o)(o,m)(m,i)(i,c)(c,a)(a,t)(t,e)(e,d)(d, )( ,n)(n,o)(o,t)(t, )( ,h)(h,e)(e, )( ,c)(c,o)(o,d)(d,e)(e,s)(s, )\n",
      "chars: whature of the work to kill writished to one beighhvelmudisomicated not he codes\n",
      "bigrams: (d,u)(u,e)(e,n)(n,c)(c,y)(y, )( ,o)(o,n)(n, )( ,t)(t,h)(h,e)(e, )( ,p)(p,u)(u,t)(t,e)(e,s)(s, )( ,s)(s,u)(u,c)(c,c)(c,e)(e,s)(s,s)(s, )( ,h)(h,i)(i,m)(m,e)(e, )( ,j)(j,u)(u,d)(w,d)(h,b)(b,a)(a,n)(n,i)(i,t)(t,y)(y, )( ,h)(h,o)(o,r)(r,i)(i,c)(c,a)(a,n)(n,s)(s, )( ,r)(r,e)(e,f)(f,i)(i,d)(d, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,e)(e,n)(n,d)(d, )( ,l)(l,i)(i,b)(b,a)(s,t)(t, )( ,t)(t,o)(o, )( ,s)(s,t)\n",
      "chars: duency on the putes success hime juwhbanity horicans refid of the end libst to s\n",
      "bigrams: ( ,l)(l,i)(i,k)(k,m)(m,e)(e, )( ,o)(o,f)(f, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,l)(l,o)(o,c)(c,k)(k, )( ,h)(h,e)(e,a)(a,v)(v,i)(i,n)(n,g)(g, )( ,d)(d, )( ,s)(s,c)(c,o)(o,t)(t,s)(s, )( ,o)(o,f)(f, )( ,b)(b,l)(l,a)(a,c)(c,k)(k, )( ,i)(i,n)(n, )( ,o)(o,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,n)(n,o)(o,u)(u,t)(t,h)(h,i)(i,s)(s,t)(t,o)(o,n)(n, )( ,s)(s,u)(u,b)(b,s)(s,t)(t,r)(r,i)(i,c)(c,t)(t, )( ,r)\n",
      "chars:  likme of eight lock heaving d scots of black in one seven nouthiston substrict \n",
      "bigrams: (j,s)(e,l)(l, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,e)(e,v)(v,i)(i,e)(e,n)(n,c)(c,e)(e, )( ,p)(p,l)(l,a)(a,y)(y,e)(e,r)(r, )( ,c)(c,o)(o,m)(m,p)(p,a)(a,n)(n,i)(i,c)(c,i)(i,a)(a,l)(l,t)(t,a)(a,r)(r,i)(i,a)(a,h)(h, )( ,q)(q,u)(a,l)(l,l)(l,e)(e,d)(d, )( ,t)(t,o)(o, )( ,h)(h,a)(a,n)(n,k)(k, )( ,v)(v,e)(e,a)(a,c)(c,h)(h,e)(e,d)(d, )( ,t)(t,o)(o, )( ,a)(a,l)(l,l)(l, )( ,l)(l,i)(i,v)(v,e)(e, )\n",
      "chars: jel of the previence player companicialtariah qalled to hank veached to all live\n",
      "bigrams: (j,a)(a, )( ,o)(o,f)(f, )( ,s)(s,o)(o,m)(m,e)(e, )( ,t)(t,w)(w,o)(o, )( ,t)(t,h)(h,e)(e, )( ,b)(b,o)(o,u)(u,n)(n,d)(d,e)(e,s)(s, )( ,a)(a,n)(n,d)(d, )( ,o)(o,c)(c,c)(c,e)(e,s)(s,s)(s,e)(e,n)(n,c)(c,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e,y)(y, )( ,i)(i,s)(s, )( ,o)(n,i)(i,g)(g,h)(h,t)(t, )( ,h)(h,i)(i,g)(g,h)(h, )( ,o)(o,f)(f, )( ,m)(m,i)(i,s)(s,s)(s,o)(o,r)(r,y)(y, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)\n",
      "chars: ja of some two the boundes and occessence of they is night high of missory of th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5100: 1.527875 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 5200: 1.516648 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 5300: 1.503425 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 5400: 1.502851 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 5500: 1.490512 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.92\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 5600: 1.503039 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 5700: 1.498314 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 5800: 1.501431 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 5900: 1.502170 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 6000: 1.476265 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "================================================================================\n",
      "bigrams: (g,z)(d,c)(c, )( ,i)(i,t)(t, )( ,o)(o,n)(n, )( ,e)(e,d)(d, )( ,t)(t,h)(h,a)(a,t)(t, )( ,t)(t,h)(h,a)(a,n)(n, )( ,w)(w,a)(a,s)(s, )( ,c)(c,o)(o,m)(m,i)(i,g)(g,i)(i,n)(n,e)(e,d)(d, )( ,t)(t,h)(h,a)(a,n)(n, )( ,a)(a,n)(n,i)(i,z)(z,a)(a,t)(t,i)(i,v)(v,e)(e, )( ,f)(f,i)(i,r)(r,s)(s,t)(t, )( ,p)(p,e)(e,a)(a,r)(r,s)(s, )( ,e)(e,s)(s,p)(p,i)(i,t)(t,a)(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,s)(s,n)(n,o)(o,w)(w,n)\n",
      "chars: gdc it on ed that than was comigined than anizative first pears espitations snow\n",
      "bigrams: (k,a)(a,u)(u,s)(s,y)(y, )( ,w)(w,a)(a,s)(s, )( ,a)(a, )( ,s)(s,t)(t,a)(a,t)(t,e)(e,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,r)(r,e)(e,g)(g,a)(a,r)(r,d)(d,e)(e,r)(r, )( ,p)(p,r)(r,a)(a,y)(y,i)(i,n)(n,d)(d, )( ,t)(t,r)(r,i)(i,a)(a,n)(n, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,a)(a,s)(s, )( ,p)(p,r)(r,o)(o,c)(c,e)(e,s)(s,s)(s, )( ,b)(b,u)\n",
      "chars: kausy was a states of the regarder prayind trian one nine two three as process b\n",
      "bigrams: ( ,k)(k,i)(i,n)(n,g)(g, )( ,a)(a,n)(n,d)(d, )( ,t)(t,o)(o, )( ,t)(t,h)(h,o)(o,s)(s,e)(e,s)(s, )( ,c)(c,o)(o,u)(u,n)(n,t)(t,o)(o,u)(u,r)(r,n)(n,s)(s, )( ,d)(d,a)(a,f)(f,o)(o,r)(r,d)(d,i)(i,n)(n,g)(g, )( ,r)(r,e)(e,s)(s,o)(o,n)(n, )( ,s)(s,h)(h,e)(e, )( ,t)(t,o)(o,l)(l,i)(i,t)(t,a)(a,n)(n,s)(s, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,o)(o,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,t)(t,w)(w,o)(o, )( ,s)\n",
      "chars:  king and to thoses countourns dafording reson she tolitans and the one two two \n",
      "bigrams: (p,h)(h,a)(a,s)(s,t)(t, )( ,c)(c,o)(o,m)(m,b)(b,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,w)(w,a)(a,s)(s, )( ,c)(c,i)(i,t)(t,y)(y, )( ,o)(o,f)(f, )( ,n)(n,u)(u,m)(m,b)(b,e)(e,r)(r, )( ,y)(y,o)(o,u)(u,n)(n,g)(g, )( ,o)(o,f)(f, )( ,r)(r,u)(u,s)(s,n)(n,e)(e,d)(d,g)(g,e)(e, )( ,a)(a,r)(r,e)(e, )( ,a)(a,c)(c,t)(t,o)(o,r)(r,s)(s, )( ,i)(i,n)(n, )( ,t)(t,h)(h,i)(i,s)(s, )( ,p)(p,e)(e,n)(n,i)(i,t)(t,i)(i,o)(o,n)(n, )\n",
      "chars: phast combation was city of number young of rusnedge are actors in this penition\n",
      "bigrams: (n,e)(e,t)(t, )( ,w)(w,i)(i,t)(t,h)(h, )( ,a)(a,n)(n, )( ,a)(a,r)(r,e)(e, )( ,a)(a,s)(s, )( ,j)(j,u)(u,k)(o,b)(b,e)(e,r)(r, )( ,m)(m,a)(a,l)(l,t)(t,r)(r,a)(a,h)(h, )( ,w)(w,e)(e,l)(l,l)(l, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,t)(t,h)(h, )( ,k)(k,o)(o,l)(l,i)(i,n)(n,e)(e, )( ,l)(l,i)(i,k)(k,e)(e,l)(l,o)(o,c)(c,y)(y,i)(i,n)(n,g)(g, )( ,a)(a,p)(p,p)(p,r)(r,o)(o,v)(v,e)(e,r)(r,s)(s, )( ,r)\n",
      "chars: net with an are as juober maltrah well one nine th koline likelocying approvers \n",
      "================================================================================\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 6100: 1.495987 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 3.92\n",
      "Average loss at step 6200: 1.467629 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 6300: 1.470905 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 3.91\n",
      "Average loss at step 6400: 1.475113 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 3.89\n",
      "Average loss at step 6500: 1.486687 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 3.89\n",
      "Average loss at step 6600: 1.520270 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 3.88\n",
      "Average loss at step 6700: 1.505108 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 3.88\n",
      "Average loss at step 6800: 1.531756 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 3.89\n",
      "Average loss at step 6900: 1.509214 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 3.91\n",
      "Average loss at step 7000: 1.501345 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "bigrams: ( ,r)(r,e)(e,l)(l,f)(f, )( ,t)(t,r)(r,e)(e,a)(a,t)(t,y)(y, )( ,h)(h,i)(i,s)(s, )( ,e)(e,m)(m,p)(p,e)(e,r)(r,o)(o,r)(r, )( ,w)(w,a)(a,r)(r, )( ,m)(m,y)(y, )( ,f)(f,r)(r,o)(o,m)(m, )( ,t)(t,h)(h,e)(e, )( ,s)(s,e)(e,m)(m,e)(e,n)(n,t)(t, )( ,t)(t,h)(h,e)(e, )( ,a)(a,r)(r,r)(r,o)(o,w)(w,n)(n, )( ,a)(a,c)(c,t)(t,o)(o,r)(r, )( ,t)(t,h)(h,e)(e, )( ,o)(o,r)(r,g)(g,a)(a,n)(n,e)(e,n)(n,e)(e,n)(n, )( ,s)(s, )\n",
      "chars:  relf treaty his emperor war my from the sement the arrown actor the organenen s\n",
      "bigrams: (p,l)(l,a)(a,i)(i,n)(n,s)(s, )( ,t)(t,o)(o, )( ,t)(t,h)(h,e)(e, )( ,w)(w,a)(a,b)(b,e)(e,r)(r, )( ,c)(c,l)(l,a)(a,s)(s,s)(s,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,e)(e, )( ,w)(w,a)(a,s)(s, )( ,a)(a,n)(n,y)(y, )( ,c)(c,h)(h,e)(e,t)(t,c)(c,k)(k,i)(i,n)(n,g)(g, )( ,c)(c,u)(u,r)(r,r)(r,e)(e,n)(n,c)(c,y)(y, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,f)(f,o)\n",
      "chars: plains to the waber classing the was any chetcking currency of the three eight f\n",
      "bigrams: (h,m)(m,a)(a,n)(n, )( ,v)(v,i)(i,n)(n,c)(c,r)(r,y)(y, )( ,a)(a, )( ,m)(m,e)(e,s)(s,s)(s, )( ,w)(w,e)(e,r)(r,e)(e, )( ,e)(e,v)(v,e)(e,n)(n, )( ,o)(o,p)(p,e)(e,r)(r,g)(g,a)(a,t)(t,o)(o,r)(r, )( ,c)(c,r)(r,o)(o,w)(w, )( ,s)(s,o)(o,m)(m,e)(e, )( ,b)(b,o)(o,r)(r,k)(k, )( ,t)(t,e)(e,r)(r,h)(h,a)(a,b)(b,i)(i,n)(n,e)(e, )( ,f)(f,e)(e,t)(t, )( ,b)(b,e)(e,s)(s,t)(t,s)(s,e)(e,s)(s,t)(t,a)(a,n)(n,o)(o,t)(t,o)\n",
      "chars: hman vincry a mess were even opergator crow some bork terhabine fet bestsestanot\n",
      "bigrams: (h,e)(e,b)(b,a)(a,r)(r, )( ,a)(a,r)(r,e)(e, )( ,p)(p,a)(a,l)(l,u)(u,a)(a,g)(g,e)(e, )( ,a)(a,l)(l,l)(l,i)(i,m)(m,e)(e,s)(s, )( ,w)(w,e)(e,r)(r,e)(e, )( ,p)(p,e)(e,o)(o,p)(p,l)(l,e)(e, )( ,t)(t,h)(h,e)(e, )( ,p)(p,u)(u,l)(l,t)(t,i)(i,s)(s,t)(t, )( ,i)(i,n)(n,t)(t,o)(o, )( ,t)(t,r)(r,a)(a,d)(d,i)(i,o)(o, )( ,g)(g,e)(e,n)(n,e)(e,t)(t,i)(i,a)(a,l)(l, )( ,e)(e,m)(m,p)(p,e)(e,r)(r,o)(o,r)(r,s)(s, )( ,s)\n",
      "chars: hebar are paluage allimes were people the pultist into tradio genetial emperors \n",
      "bigrams: (g,o)(o,a)(a,l)(l, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,b)(b,a)(a,n)(n,k)(k,i)(i,n)(n,g)(g, )( ,g)(g,r)(r,e)(e,e)(e,w)(w,i)(i,n)(n,g)(g, )( ,a)(a,i)(i,r)(r, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,p)(p,p)(p,l)(l,a)(a,n)(n,y)(c,e)(e, )( ,f)(f,l)(l,o)(o,r)(r,e)(e,s)(s, )( ,t)(t,o)(o,o)(o, )( ,h)(h,i)(i,j)(d,g)(g,o)(o, )( ,a)(a, )( ,e)(e,x)(x,t)(t,r)(r,e)(e,a)(a,s)(s,o)(o,n)(n, )( ,h)(h,a)(a,v)(v,i)(i,s)\n",
      "chars: goal which banking greewing air of the pplance flores too hidgo a extreason havi\n",
      "================================================================================\n",
      "Validation set perplexity: 3.89\n",
      "CPU times: user 11min 32s, sys: 3min 41s, total: 15min 14s\n",
      "Wall time: 3min 43s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph_bigram_embed(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same perplexity, but it's faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # big matrix\n",
    "  ifcox = tf.Variable(tf.truncated_normal([bigram_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, train=False):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    embed = tf.nn.embedding_lookup(ifcox, i)\n",
    "    if train:\n",
    "        embed = tf.nn.dropout(embed, 0.5)\n",
    "    all_gates = embed + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "    update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int64, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state, True)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int64, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state, False)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.590648 learning rate: 10.000000\n",
      "Minibatch perplexity: 728.25\n",
      "================================================================================\n",
      "bigrams: (e,r)(j,m)(m,i)(f,b)(o,c)(y,z)(n,l)(e,j)(d,r)(p,p)(y,b)(l,d)(w,x)(f,b)(o,w)(p,w)(g,n)(t,s)(j,v)(u,r)(e,m)(c,o)(t,v)(d,g)(z,x)(u,l)(p,f)(h,j)(g,r)(h,s)(e,m)(q,s)(k,e)(u,z)(h, )(q,q)(z,n)(h,h)(o,j)(c,p)(s,b)(o,l)(d,q)(n,y)(l,j)(m,x)(m,r)(m,n)(h,n)(v,e)(x,q)(z,o)(v,x)(w,o)(m,g)(s,h)(c,w)(a,q)(h,b)(r,x)(h,a)(w,q)(d,p)(r,o)(t,n)(o,e)(r,q)(d,d)(n,r)(k,i)(f,z)(l,j)(j,y)(d,s)(q,s)(t,s)(c,n)(s,g)(f,r)(a,s)\n",
      "chars: ejmfoynedpylwfopgtjuectdzuphgheqkuhqzhocsodnlmmmhvxzvwmscahrhwdrtordnkfljdqtcsfa\n",
      "bigrams: (c,s)(x,f)(x,q)(n,h)(n,k)(s,k)(i,v)(x,o)(t,s)(u,o)(c,l)(n,k)(k,u)(r,e)(z,c)(m,m)(b,c)(l,n)(h,s)(m,t)(a,p)(r,n)(w,f)(b,g)(c,c)(v,l)(t,k)(g,z)(k,i)(j,w)(r,c)(i,y)(q,b)(m,h)(s,m)( ,p)(g,r)(d,g)(b,o)(v,s)(l, )(c,x)(o,r)(b,n)(e,n)(n,p)(r,g)(c,h)(h,p)(s,i)(p,m)(p,w)(r,f)(v,b)(l,c)(h,q)(f,w)(q,o)(h,l)(d, )(d,m)(c,b)(f, )(k,c)( ,m)(s,c)(s,v)(e,r)( , )(k,q)(p,t)(v,c)(k,g)(t,m)(t,y)(c,q)(o,y)(z,q)(v,o)(g,c)\n",
      "chars: cxxnnsixtucnkrzmblhmarwbcvtgkjriqms gdbvlcobenrchspprvlhfqhddcfk sse kpvkttcozvg\n",
      "bigrams: (w,s)(h,y)(a,z)(y,q)(d,w)(n,i)(j,v)(h,z)(r,q)(c,u)(a,n)(a,i)(j,b)( ,m)(m,e)(j,w)(s,m)(v,g)(p,h)(y,x)(q,z)(d,g)(s,q)(u,t)(f,u)(h,b)(v,f)(i,n)(d,u)(r,w)(x,p)(u, )(c,i)(y,k)(g,k)(g,a)(v,v)(y,c)(i, )(w,p)(a,u)(d,u)(w,s)(z,r)(k,t)(s,e)(m,f)(j,l)(w,y)(d, )(e,w)(o,f)(f,h)(r,h)(h,a)(g, )(q,n)(v, )(j,r)(w,r)(o,u)(q,m)(e,c)(d,e)(c,a)(h,t)(g,k)( ,f)(j,i)(e,u)(c,z)(s,n)(p,q)(g,d)(w,n)(q,e)(d, )(a,n)(p,l)(s,t)\n",
      "chars: whaydnjhrcaaj mjsvpyqdsufhvidrxucyggvyiwadwzksmjwdeofrhgqvjwoqedchg jecspgwqdaps\n",
      "bigrams: (p,r)(c,x)(y,t)(b,l)(z,j)(t,m)(b,i)(p,b)(j,s)(k,r)(g,s)(g,w)(r,d)(y,r)(g,e)(t,k)(q,n)(c,v)(h,t)(k,c)(n,d)(c,d)(d, )(s,p)(x,r)(q,y)(x,f)(v,i)( ,t)( ,g)(y,l)(j,r)(c,t)(n,a)(d,l)(d,r)( ,n)(g,l)(n,h)(r,j)(l,u)(j,t)(q,b)(o,w)(c,k)(u,r)(s,k)(k,f)(i,w)(g,b)(o,a)(d,a)(o,n)(l,d)(s,d)(x,j)(d,w)(q,n)(m,i)(d,y)(l,v)(o,a)(c,x)(o,d)(e,d)(q,p)(g,c)(j,z)( ,x)( ,k)(x,u)(t,l)( ,y)(l,h)(x,s)(c,t)(s,d)(c, )(w,k)(z,v)\n",
      "chars: pcybztbpjkggrygtqchkncdsxqxv  yjcndd gnrljqocuskigodolsxdqmdlocoeqgj  xt lxcscwz\n",
      "bigrams: (h,p)(k,v)(z,p)(f, )(p,x)(d,d)(i,b)(n,t)(g,a)(g,d)(x,m)( ,i)(f, )(p,y)(e,z)(b, )(y,r)(r,w)(q,o)(k,m)(m,c)(r,k)(j,m)(k,n)(w,m)(p,h)(l,r)(f,l)(q,j)(s,y)(g,t)(e,j)(q,y)(t,c)(r,t)(t,p)(z,q)(i,n)(u,l)(f, )(p,c)(y,k)(a,v)(x,d)(f,q)(a,v)(f,c)(c,c)(c,v)(i,y)(m,v)(r,p)(t,a)(a,h)(d, )(m,f)(y,a)(f,v)(a,v)(n,j)(e,r)(y,s)(l,i)(e,f)(m,s)(z,l)(l,u)(j,p)( ,s)(k,o)(p,w)(z, )(a,r)(g,h)(p,o)(b, )(s,a)(l, )(j,k)(y,r)\n",
      "chars: hkzfpdinggx fpebyrqkmrjkwplfqsgeqtrtziufpyaxfafccimrtadmyfaneylemzlj kpzagpbsljy\n",
      "================================================================================\n",
      "Validation set perplexity: 674.67\n",
      "Average loss at step 100: 5.335958 learning rate: 10.000000\n",
      "Minibatch perplexity: 120.95\n",
      "Validation set perplexity: 121.30\n",
      "Average loss at step 200: 4.175561 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.93\n",
      "Validation set perplexity: 28.53\n",
      "Average loss at step 300: 3.194514 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.84\n",
      "Validation set perplexity: 13.80\n",
      "Average loss at step 400: 2.681142 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.11\n",
      "Validation set perplexity: 11.31\n",
      "Average loss at step 500: 2.450182 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.98\n",
      "Validation set perplexity: 9.10\n",
      "Average loss at step 600: 2.331699 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.44\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 700: 2.205726 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.66\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 800: 2.132355 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.52\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 900: 2.125327 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.73\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 1000: 2.102162 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.61\n",
      "================================================================================\n",
      "bigrams: (t,n)(i,l)(l,i)(i,b)(b,l)(l,e)(e, )( ,a)(a,s)(s, )( ,a)(a,m)(m,p)(p,r)(r,i)(i,c)(c, )( ,c)(c,a)(a,d)(d,e)(e, )( ,s)(s, )( ,a)(a, )( ,p)(p,l)(l,a)(a,t)(t,e)(e,r)(r, )( ,y)(o,c)(c,t)(t,o)(o,r)(r,t)(t, )( ,a)(a,s)(s, )( ,a)(a,s)(s, )( ,t)(t,h)(h,e)(e, )( ,v)(v,i)(i,t)(t,i)(i,o)(o,n)(n, )( ,a)(a, )( ,s)(s,h)(h,o)(o,w)(w, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,a)(a,n)(n,d)(d, )( ,d)(d,e)(e,a)(a,t)(t, )( ,h)\n",
      "chars: tilible as ampric cade s a plater octort as as the vition a show seven and deat \n",
      "bigrams: (x,g)(i,e)(e,t)(t,r)(r,e)(e, )( ,a)(a, )( ,p)(p,a)(a,s)(s,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,h)(h,a)(a,s)(s, )( ,t)(t,h)(h,i)(i,s)(s, )( ,t)(t,h)(h,e)(e, )( ,c)(c,o)(o,n)(n, )( ,c)(c,o)(o,m)(m,e)(e, )( ,b)(b,e)(e,a)(a, )( ,n)(n,i)(i,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,f)(f,i)(i,v)(v,e)(e, )( ,m)(m,i)(i,n)(n,c)(c,e)(e, )( ,p)(p,r)(r,o)(o,d)(d,i)(i,l)(l, )( ,i)(i,n)(n,c)(c,r)(r,i)(i,n)(n,g)(g, )\n",
      "chars: xietre a pastions has this the con come bea nine eight five mince prodil incring\n",
      "bigrams: (b,y)(y, )( ,p)(p,l)(l,a)(a,y)(y, )( ,t)(t,o)(o, )( ,f)(f,o)(o,r)(r, )( ,c)(c,o)(o,n)(n, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,w)(w,a)(a,t)(t,h)(h, )( ,i)(i,n)(n,f)(f,o)(o,r)(r,k)(k,s)(s, )( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,o)(o,d)(d,i)(i,c)(c, )( ,g)(g,a)(a,l)(l,e)(e, )( ,s)(s,e)(e,r)(r, )( ,i)(i,n)(n, )( ,j)(v, )( ,o)(o,u)(u,r)(r, )( ,s)(s,t)(t,r)(r,i)(i,c)(c,t)(t,e)(e,n)(n,s)(s, )( ,s)\n",
      "chars: by play to for con one nine wath inforks the prodic gale ser in v our strictens \n",
      "bigrams: (r,k)(q,u)(u,c)(c,h)(h, )( ,s)(s,i)(i,x)(x, )( ,i)(i,n)(n, )( ,u)(u,s)(s,t)(t,o)(o,r)(r,e)(e, )( ,t)(t,h)(h,e)(e, )( ,i)(i,n)(n,f)(f,e)(e,v)(v,e)(e, )( ,k)(k,n)(n,o)(o,n)(n,o)(o, )( ,t)(t,w)(w,o)(o, )( ,i)(i,n)(n, )( ,l)(l,o)(o,o)(o,k)(q,u)(u,t)(t,e)(e, )( ,m)(m,o)(o,t)(t,e)(e,r)(r,s)(s, )( ,s)(s,h)(h,e)(e, )( ,v)(v,i)(i,l)(l,l)(l, )( ,i)(i,t)(t, )( ,a)(a,s)(s, )( ,h)(h,e)(e, )( ,c)(c,a)(a,i)(i,n)\n",
      "chars: rquch six in ustore the infeve knono two in looqute moters she vill it as he cai\n",
      "bigrams: (e,x)(t,j)(c,i)(i,c)(c,t)(t,s)(s, )( ,h)(h,i)(i,l)(l,l)(l, )( ,o)(o,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,i)(i,n)(n,e)(e, )( ,c)(c,h)(h,o)(o,p)(p,e)(e, )( ,d)(d,e)(e,c)(c,a)(a,r)(r,c)(c,h)(h,i)(i,n)(n,g)(g, )( ,f)(f,r)(r,o)(o,m)(m, )( ,h)(h,e)(e,n)(n,t)(t,e)(e,r)(r,s)(s,i)(i,n)(n, )( ,h)(h,a)(a,v)(v,e)(e, )( ,o)(o,f)(f, )( ,c)(c,o)(o,n)(n,a)(a,m)(m,e)(e, )( ,p)(p,r)(r,o)(o,b)(d,r)(r,u)(u,r)(r,a)(a, )\n",
      "chars: etcicts hill one two ine chope decarching from hentersin have of coname prodrura\n",
      "================================================================================\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 1100: 2.038759 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 1200: 1.989691 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.79\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1300: 1.967601 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 1400: 1.976378 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1500: 1.970567 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1600: 1.971609 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1700: 1.927054 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1800: 1.885862 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1900: 1.849179 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 2000: 1.903983 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "================================================================================\n",
      "bigrams: (h,j)(p,u)(u,t)(t,e)(e, )( ,a)(a,s)(s, )( ,s)(s,t)(t,r)(r,i)(i,e)(e,a)(a,l)(l, )( ,s)(s,p)(p,r)(r,o)(o,v)(v,i)(i,e)(e,t)(t,i)(i,a)(a,l)(l, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,g)(g,i)(i,n)(n,f)(f,f)(f,o)(o,r)(r, )( ,g)(g,r)(r,e)(e, )( ,a)(a,s)(s, )( ,l)(l,a)(a,c)(c,k)(k, )( ,c)(c,o)(o,u)(u,c)(c,h)(h, )( ,a)(a,s)(s,h)(h,a)(a,n)(n, )( ,g)(g,u)(u,a)(a,l)(l,i)(i,m)(m, )( ,d)(d,i)(i,b)(b,o)(o,r)(r,t)\n",
      "chars: hpute as strieal sprovietial of the ginffor gre as lack couch ashan gualim dibor\n",
      "bigrams: (h,k)(z,e)(e,d)(d, )( ,r)(r,e)(e,c)(c, )( ,a)(a,s)(s,s)(s,t)(t,e)(e,r)(r,n)(n, )( ,w)(w,o)(o,r)(r,k)(k,e)(e,r)(r,s)(s, )( ,m)(m,i)(i,d)(d,y)(y, )( ,a)(a,i)(i,r)(r,o)(o,u)(u,t)(t, )( ,t)(t,h)(h,e)(e, )( ,o)(o,r)(r,m)(m,a)(a,t)(t,e)(e,t)(t,y)(y, )( ,f)(f,l)(l,e)(e,a)(a,t)(t,e)(e, )( ,t)(t,h)(h,e)(e, )( ,w)(w,a)(a,s)(s, )( ,c)(c,o)(o,m)(m,p)(p,r)(r,o)(o,d)(d,y)(y, )( ,t)(t,o)(o, )( ,t)(t,o)(o, )( ,s)\n",
      "chars: hzed rec asstern workers midy airout the ormatety fleate the was comprody to to \n",
      "bigrams: (p,n)(n,c)(c,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,f)(f,o)(o,r)(r, )( ,d)(d,i)(i,r)(r,g)(g, )( ,p)(p,e)(e,r)(r,m)(m,a)(a,t)(t,a)(a,t)(t,e)(e, )( ,a)(a,l)(l,o)(o,s)(s,t)(t,i)(i,c)(c,i)(i,a)(a,l)(l, )( ,m)(m,e)(e,s)(s,s)(s,u)(u,a)(a,l)(l,l)(l,y)(y, )( ,f)(f,o)(o,r)(r,n)(n,e)(e, )( ,t)(t,h)(h,e)(e, )( ,b)(b,e)(e,t)(t, )( ,d)(d,i)(i,v)(v,i)(i,n)(n,a)(a,l)\n",
      "chars: pnce one nine eight for dirg permatate alosticial messually forne the bet divina\n",
      "bigrams: (t,r)(r, )( ,g)(g,e)(e,n)(n,t)(t,i)(i,n)(n,a)(a,l)(l, )( ,e)(e,a)(a,r)(r, )( ,l)(l,e)(e,t)(t,t)(t,e)(e,r)(r, )( ,o)(o,n)(n,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,s)(s,t)(t,a)(a,n)(n,y)(y, )( ,d)(d,o)(o,n)(n,a)(a,s)(s,t)(t, )( ,o)(o,r)(r,t)(t,h)(h,i)(i,t)(t,a)(a,b)(b,l)(l,e)(e, )( ,n)(n,o)(o,r)(r,t)(t,h)(h,e)(e,n)(n,t)(t, )( ,c)(c,h)(h,a)(a,t)(t,e)(e, )( ,t)(t,h)(h,a)(a,t)(t, )( ,i)(i,n)(n, )( ,t)\n",
      "chars: tr gentinal ear letter one three stany donast orthitable northent chate that in \n",
      "bigrams: (e,x)(c,g)(a,t)(t,e)(e, )( ,l)(l,a)(a,n)(n,g)(g,s)(s, )( ,a)(a,r)(r,e)(e, )( ,i)(i,n)(n,i)(i,s)(s,m)(m, )( ,b)(b,i)(i,g)(g,h)(h,t)(t, )( ,b)(b,l)(l,a)(a, )( ,d)(d,e)(e,v)(v,e)(e,r)(r, )( ,n)(n,e)(e,a)(a,d)(d, )( ,l)(l,i)(i,t)(t,e)(e,l)(l,y)(y, )( ,l)(l,e)(e,m)(m,e)(e, )( ,w)(w,o)(o,r)(r,k)(k,s)(s, )( ,c)(c,o)(o,m)(m,m)(m,e)(e, )( ,i)(i,n)(n, )( ,w)(w,a)(a,s)(s, )( ,p)(p,e)(e,n)(n,d)(d,a)(a,n)(n,t)\n",
      "chars: ecate langs are inism bight bla dever nead litely leme works comme in was pendan\n",
      "================================================================================\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 2100: 1.896272 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 2200: 1.880868 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 2300: 1.834158 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 2400: 1.848244 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 2500: 1.867321 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2600: 1.843636 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.16\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2700: 1.843869 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2800: 1.832933 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2900: 1.822276 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3000: 1.824049 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "================================================================================\n",
      "bigrams: (i,e)(e,r)(r,s)(s, )( ,i)(i,n)(n, )( ,o)(o,f)(f,f)(f,i)(i,c)(c,i)(i,a)(a,l)(l, )( ,l)(l,i)(i,n)(n,g)(g,e)(e,s)(s, )( ,h)(h,i)(i,s)(s, )( ,a)(a,n)(n,c)(c,e)(e, )( ,i)(i,s)(s, )( ,w)(w,a)(a,t)(t,h)(h, )( ,a)(a,n)(n,d)(d, )( ,l)(l,a)(a,i)(i,r)(r,s)(s, )( ,h)(h,e)(e, )( ,a)(a,c)(c,r)(r,e)(e,b)(b,a)(a,s)(s,i)(i,a)(a,n)(n, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e,r)(r,e)(e, )( ,c)(c,o)(o,n)(n,s)(s,c)(c,h)(h,a)\n",
      "chars: iers in official linges his ance is wath and lairs he acrebasian in there consch\n",
      "bigrams: (j,l)(t, )( ,c)(c,a)(a,n)(n, )( ,r)(r,e)(e,f)(f,e)(e,r)(r,r)(r,e)(e,n)(n,c)(c,e)(e, )( ,i)(i,s)(s, )( ,b)(b,e)(e,c)(c,o)(o,l)(l,m)(m,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,a)(a,n)(n, )( ,a)(a,l)(l,l)(l, )( ,t)(t,o)(o, )( ,n)(n,o)(o,r)(r,t)(t,h)(h,e)(e,r)(r, )( ,i)(i,n)(n, )( ,p)(p,r)(r,e)(e,s)(s,s)(s,e)(e,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,n)(n, )( ,s)(s,i)(i,c)(c, )( ,t)(t,o)(o, )( ,u)(u,n)(n,l)(l,i)(i,m)(m,a)\n",
      "chars: jt can referrence is becolming than all to norther in pressection n sic to unlim\n",
      "bigrams: (d,b)(x,o)(o,l)(l, )( ,d)(d,a)(a,r)(r, )( ,b)(b,e)(e,g)(g,r)(r,e)(e,e)(e,r)(r,e)(e, )( ,t)(t,o)(o, )( ,a)(a,r)(r,a)(a,n)(n,d)(d, )( ,s)(s,t)(t,a)(a,t)(t,e)(e, )( ,r)(r,e)(e,l)(l,e)(e,i)(i,n)(n, )( ,f)(f,o)(o,u)(u,r)(r, )( ,i)(i,m)(m,p)(p,l)(l,o)(o,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,f)(f,o)(o,u)(u,r)(r, )( ,z)(z,e)(e,r)(r,o)(o, )\n",
      "chars: dxol dar begreere to arand state relein four implone six one nine nine four zero\n",
      "bigrams: (f,r)(r,a)(a,p)(p,t)(t,e)(e,r)(r, )( ,c)(c,h)(h,a)(a,c)(c,t)(t, )( ,s)(s,e)(e,r)(r,c)(c,u)(u,t)(t,e)(e, )( ,i)(i,n)(n,t)(t,e)(e,r)(r,g)(g,a)(a,n)(n, )( ,l)(l,i)(i,n)(n,g)(g,s)(s, )( ,h)(h,o)(o,l)(l,e)(e, )( ,t)(t,r)(r,o)(o,e)(e,v)(v,e)(e,r)(r,n)(n,y)(y, )( ,f)(f,e)(e,n)(n,d)(d,e)(e,r)(r, )( ,o)(o,f)(f, )( ,t)(t,h)(h,a)(a,t)(t, )( ,e)(e,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,s)(s,i)(i,x)(x, )( ,w)(w,h)\n",
      "chars: frapter chact sercute intergan lings hole troeverny fender of that ene six six w\n",
      "bigrams: (c,g)(c,e)(e,n)(n,c)(c,i)(i,t)(t,i)(i,o)(o,n)(n, )( ,s)(s,o)(o,m)(m,e)(e,s)(s,s)(s, )( ,m)(m,a)(a,n)(n,y)(y, )( ,c)(c,a)(a,l)(l,l)(l,e)(e,d)(d, )( ,t)(t,o)(o, )( ,d)(d,i)(i,s)(s,c)(c,e)(e,s)(s,s)(s,e)(e,n)(n,t)(t, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,h)(h,i)(i,s)(s, )( ,i)(i,m)(m,p)(p,a)(a,r)(r,a)(a, )( ,s)(s,o)(o,m)(m,e)(e,s)(s, )( ,t)(t,e)(e,x)(x,p)(p,e)(e,c)(c,t)(t,o)(o,n)(n, )( ,r)(r,e)(e,t)(t,w)\n",
      "chars: ccencition somess many called to discessent which his impara somes texpecton ret\n",
      "================================================================================\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 3100: 1.793510 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3200: 1.815119 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3300: 1.805591 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3400: 1.835067 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3500: 1.820407 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3600: 1.825662 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3700: 1.798862 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3800: 1.790757 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3900: 1.786267 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4000: 1.802595 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "================================================================================\n",
      "bigrams: (w,s)(s, )( ,a)(a,n)(n,d)(d, )( ,a)(a, )( ,g)(g,o)(o,v)(v,e)(e,r)(r,n)(n,t)(t,a)(a,n)(n,s)(s, )( ,p)(p,e)(e,r)(r,i)(i,t)(t,s)(s, )( ,a)(a,n)(n,d)(d, )( ,g)(g,o)(o,t)(t, )( ,e)(e,l)(l,e)(e,c)(c,t)(t,e)(e,s)(s,h)(h, )( ,h)(h,i)(i,s)(s, )( ,t)(t,h)(h,a)(a,t)(t, )( ,c)(c,o)(o,n)(n,s)(s,e)(e,r)(r,v)(v,i)(i,n)(n,g)(g, )( ,i)(i,n)(n,c)(c,o)(o,s)(s,s)(s,a)(a,i)(i,n)(n,d)(d,y)(y, )( ,d)(d,a)(a,y)(y,e)(e,m)\n",
      "chars: ws and a governtans perits and got electesh his that conserving incossaindy daye\n",
      "bigrams: (b,l)(l,i)(i,e)(e,s)(s, )( ,f)(f,r)(r,i)(i,p)(p, )( ,g)(g,o)(o,v)(v,e)(e,r)(r,n)(n,m)(m,e)(e,n)(n,t)(t, )( ,b)(b,e)(e,e)(e,n)(n, )( ,w)(w,h)(h,e)(e,n)(n, )( ,b)(b,u)(u,t)(t, )( ,w)(w,i)(i,t)(t,h)(h, )( ,s)(s,o)(o,n)(n,c)(c,e)(e, )( ,m)(m,a)(a,i)(i,n)(n,e)(e,s)(s, )( ,c)(c,a)(a,s)(s,t)(t,r)(r,i)(i,t)(t,y)(y, )( ,t)(t,o)(o, )( ,a)(a, )( ,p)(p,u)(u,b)(b,l)(l,i)(i,d)(d,e)(e,d)(d, )( ,w)(w,i)(i,t)(t,h)\n",
      "chars: blies frip government been when but with sonce maines castrity to a publided wit\n",
      "bigrams: (g,p)(p,r)(r,o)(o,m)(m,a)(a,d)(d,i)(i,s)(s,h)(h, )( ,c)(c,u)(u,l)(l,a)(a,r)(r,e)(e, )( ,o)(o,f)(f, )( ,s)(s,o)(o,p)(p,e)(e,r)(r,i)(i,a)(a, )( ,i)(i,r)(r,e)(e,v)(v,i)(i,s)(s,t)(t,i)(i,c)(c,l)(l,e)(e,s)(s,s)(s, )( ,g)(g,r)(r,a)(a,p)(p,e)(e, )( ,a)(a,r)(r,e)(e,s)(s,s)(s,i)(i,n)(n,g)(g, )( ,s)(s, )( ,m)(m,a)(a,d)(d,e)(e,s)(s, )( ,g)(g,u)(u,e)(e,e)(e, )( ,c)(c,o)(o,n)(n,s)(s,t)(t,a)(a,g)(g,r)(r,a)(a,p)\n",
      "chars: gpromadish culare of soperia irevisticless grape aressing s mades guee constagra\n",
      "bigrams: (k,o)(o,r)(r, )( ,c)(c,a)(a,n)(n, )( ,a)(a, )( ,b)(b,r)(r,e)(e,a)(a,s)(s,i)(i,n)(n,g)(g, )( ,a)(a,n)(n,d)(d, )( ,v)(v,a)(a,l)(l,a)(a,t)(t,e)(e,s)(s, )( ,a)(a,d)(p,m)(m,o)(o,s)(s,e)(e, )( ,a)(a, )( ,b)(b,e)(e,f)(f,o)(o,r)(r,t)(t,s)(s, )( ,f)(f,r)(r,o)(o,m)(m, )( ,u)(u,n)(n,d)(d,e)(e,r)(r, )( ,h)(h,a)(a,k)(k,e)(e,l)(l,e)(e, )( ,z)(z,o)(o,d)(d,e)(e,n)(n,g)(g, )( ,w)(w,h)(h,e)(e,n)(n, )( ,t)(t,h)(h,e)\n",
      "chars: kor can a breasing and valates apmose a beforts from under hakele zodeng when th\n",
      "bigrams: (a,s)(s, )( ,a)(a,r)(r,e)(e, )( ,s)(s,a)(a,i)(i,l)(l,a)(a,n)(n,d)(d,o)(o,n)(n,i)(i,c)(c,a)(a,l)(l, )( ,a)(a,l)(l,s)(s,o)(o, )( ,a)(a,c)(c,t)(t,r)(r,i)(i,b)(b,e)(e,s)(s, )( ,r)(r,u)(u,s)(s,e)(e,m)(m,e)(e, )( ,p)(p,r)(r,o)(o,f)(f,i)(i,c)(c,a)(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,a)(a, )( ,r)(r,e)(e,l)(l,a)(a,u)(u,s)(s, )( ,s)(s,u)(u,c)(c,h)(h, )( ,t)(t,h)(h,e)(e, )( ,m)(m,i)(i,n)(n,c)(c,h)(h, )( ,a)(a,n)\n",
      "chars: as are sailandonical also actribes ruseme profications a relaus such the minch a\n",
      "================================================================================\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4100: 1.775877 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4200: 1.781408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4300: 1.760972 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4400: 1.749567 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4500: 1.752899 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4600: 1.757841 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4700: 1.774749 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4800: 1.768863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4900: 1.763424 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 5000: 1.733389 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "bigrams: (x,n)(a,t)(t,e)(e,s)(s, )( ,a)(a, )( ,g)(g,o)(o,d)(d,e)(e,r)(r,n)(n, )( ,p)(p,u)(u,r)(r,d)(d,e)(e,r)(r, )( ,a)(a,l)(l,l)(l, )( ,o)(o,b)(b,i)(i,a)(a,n)(n, )( ,e)(e,l)(l,e)(e,r)(r, )( ,b)(b, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,z)(z,e)(e,r)(r,o)(o, )( ,w)(w,a)\n",
      "chars: xates a godern purder all obian eler b one eight two zero zero zero eight zero w\n",
      "bigrams: (l,k)(k,e)(e, )( ,o)(o,v)(v,e)(e,r)(r, )( ,s)(s,e)(e,t)(t,t)(t,r)(r,e)(e, )( ,e)(e,s)(s,t)(t,e)(e,r)(r,v)(v,i)(i,e)(e,s)(s, )( ,a)(a, )( ,w)(w,a)(a,r)(r, )( ,t)(t,h)(h,e)(e, )( ,w)(w,o)(o,r)(r,d)(d, )( ,d)(d,e)(e,l)(l,l)(l,a)(a,v)(v,i)(i,o)(o,u)(u,s)(s, )( ,o)(o,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,g)(g,e)(e,t)(t,h)(h, )( ,o)(o,f)(f, )( ,m)(m,i)(i,n)(n,t)(t,e)(e,n)(n,e)(e,s)(s,t)(t,a)(a,t)(t,i)(i,o)\n",
      "chars: lke over settre estervies a war the word dellavious one two geth of mintenestati\n",
      "bigrams: (c,c)(c,e)(e,l)(l,l)(l,i)(i,m)(m, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,p)(p,o)(o,e)(e,t)(t, )( ,d)(d,i)(i,a)(a,t)(t,i)(i,o)(o,r)(r, )( ,o)(o,f)(f, )( ,g)(g,e)(e,n)(n,e)(e,r)(r,g)(g,y)(y, )( ,p)(p,u)(u,f)(f,f)(f,o)(o,r)(r,t)(t,s)(s, )( ,t)(t,r)(r,e)(e,a)(a,t)(t,e)(e,r)(r, )( ,s)(s,a)(a,l)(l,i)(i,s)(s,h)(h, )( ,a)(a, )( ,r)(r,e)(e,m)\n",
      "chars: ccellim seven two zero zero poet diatior of genergy pufforts treater salish a re\n",
      "bigrams: (t,k)(c,l)(l,e)(e, )( ,r)(r,u)(u,s)(s,e)(e,d)(d, )( ,m)(m,i)(i,n)(n,e)(e,s)(s,t)(t,o)(o,n)(n, )( ,t)(t,h)(h,e)(e, )( ,g)(g,e)(e,t)(t, )( ,i)(i,s)(s, )( ,b)(b,a)(a,t)(t,t)(t,l)(l,e)(e,s)(s, )( ,a)(a,n)(n,d)(d, )( ,h)(h,a)(a,s)(s, )( ,i)(i,l)(l,l)(l,i)(i,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,u)(u,s)(s,e)(e,d)(d, )( ,i)(i,n)(n, )( ,t)(t,h)(h,i)(i,s)(s, )( ,t)(t,e)(e,n)(n,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,d)\n",
      "chars: tcle rused mineston the get is battles and has illiction used in this tene five \n",
      "bigrams: (s,d)(d,e)(e,n)(n,t)(t,y)(y, )( ,b)(b,e)(e, )( ,r)(r,e)(e,n)(n,t)(t, )( ,d)(d,i)(i,s)(s,i)(i,d)(d,e)(e, )( ,w)(w,o)(o,r)(r,d)(d, )( ,i)(i,s)(s, )( ,a)(a,p)(p,o)(o,u)(u,t)(t,i)(i,c)(c, )( ,h)(h,a)(a,d)(d, )( ,w)(w,a)(a,s)(s, )( ,m)(m,a)(a,y)(y,s)(s, )( ,i)(i,o)(o,d)(d, )( ,i)(i,t)(t, )( ,f)(f,o)(o,r)(r, )( ,f)(f,a)(a,y)(y,s)(s, )( ,e)(e, )( ,h)(h,a)(a,s)(s,t)(t,o)(o,n)(n, )( ,p)(p,s)(s,y)(y,c)(t,i)\n",
      "chars: sdenty be rent diside word is apoutic had was mays iod it for fays e haston psyt\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 5100: 1.735382 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 5200: 1.726094 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5300: 1.713537 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5400: 1.710003 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5500: 1.702832 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5600: 1.714484 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5700: 1.703274 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5800: 1.704674 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5900: 1.706372 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 6000: 1.679791 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "bigrams: (s,p)(p,i)(i,m)(m,e)(e, )( ,t)(t,o)(o, )( ,n)(n,a)(a,t)(t,u)(u,r)(r,a)(a,l)(l,l)(l,y)(y, )( ,a)(a,c)(c,c)(c,o)(o,u)(u,n)(n,c)(c,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,a)(a,n)(n,d)(d, )( ,s)(s, )( ,a)(a,f)(f,r)(r,o)(o,l)(l,o)(o, )( ,h)(h,i)(i,s)(s,t)(t, )( ,s)(s,i)(i,n)(n,g)(g,l)(l,i)(i,b)(b,l)(l,e)(e, )( ,s)(s,u)(u,c)(c,h)(h, )( ,m)(m,e)(e, )( ,f)(f,i)(i,n)(n, )( ,h)(h,e)(e, )( ,h)(h,o)(o,m)(m,e)(e,x)(f,t)\n",
      "chars: spime to naturally accounctions and s afrolo hist singlible such me fin he homef\n",
      "bigrams: (d,v)(y,t)(t,a)(a,n)(n,t)(t, )( ,a)(a,s)(s, )( ,t)(t,h)(h,e)(e, )( ,h)(h,a)(a,v)(v,a)(a, )( ,s)(s,a)(a,d)(d,a)(a,y)(y, )( ,h)(h,u)(u,m)(m,e)(e,t)(t,i)(i,c)(c,a)(a,l)(l, )( ,c)(c,o)(o,u)(u,n)(n,t)(t,s)(s, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,g)(g,e)(e,r)(r,d)(d,s)(s, )( ,w)(w,e)(e,r)(r,e)(e, )( ,m)(m,o)(o,d)(d,e)(e,r)(r,s)(s, )( ,w)(w,i)(i,t)(t,h)(h, )( ,p)(p,h)(h,i)(i,l)(l,d)(d,e)(e,r)(r, )\n",
      "chars: dytant as the hava saday humetical counts and the gerds were moders with philder\n",
      "bigrams: (w,o)(o,s)(s,i)(i,t)(t,y)(y, )( ,m)(m,i)(i,l)(l,i)(i,s)(s, )( ,w)(w,a)(a,r)(r,e)(e, )( ,c)(c,o)(o,u)(u,l)(l,d)(d, )( ,t)(t,h)(h,e)(e, )( ,m)(m,o)(o,r)(r,e)(e, )( ,p)(p,a)(a,r)(r,a)(a,i)(i,d)(d,a)(a, )( ,i)(i,s)(s, )( ,s)(s,t)(t,r)(r,u)(u,g)(g,h)(h,t)(t,o)(o, )( ,a)(a,n)(n,d)(d, )( ,s)(s,h)(h,a)(a,n)(n,g)(g,e)(e,n)(n,t)(t,h)(h, )( ,i)(i,s)(s, )( ,m)(m,a)(a,r)(r,l)(l,e)(e, )( ,b)(b,o)(o,u)(u,t)(t,s)\n",
      "chars: wosity milis ware could the more paraida is strughto and shangenth is marle bout\n",
      "bigrams: (p,s)(s, )( ,o)(o,f)(f, )( ,n)(n,e)(e,w)(w, )( ,c)(c,o)(o,n)(n,s)(s,t)(t,a)(a,y)(y,e)(e, )( ,t)(t,i)(i,m)(m,e)(e, )( ,d)(d,a)(a,y)(y, )( ,h)(h,e)(e, )( ,c)(c,h)(h,r)(r,u)(u,p)(p,a)(a,r)(r, )( ,s)(s,u)(u,c)(c,h)(h, )( ,p)(p,e)(e,r)(r,i)(i,l)(l,i)(i,s)(s,t)(t, )( ,o)(o,f)(f, )( ,s)(s,u)(u,s)(s,t)(t, )( ,f)(f,r)(r,i)(i,d)(d,e)(e,o)(o,r)(r, )( ,r)(r,e)(e,i)(i,g)(g,n)(n,i)(i,t)(t,i)(i,o)(o,n)(n, )( ,o)\n",
      "chars: ps of new constaye time day he chrupar such perilist of sust frideor reignition \n",
      "bigrams: (m,o)(o,n)(n, )( ,i)(i,n)(n,d)(d,i)(i,a)(a,c)(c,e)(e, )( ,t)(t,y)(y,p)(p,i)(i,c)(c, )( ,c)(c,o)(o,n)(n,t)(t,r)(r,o)(o,c)(c,i)(i,e)(e,d)(d, )( ,s)(s,p)(p,a)(a,n)(n,i)(i,t)(t,y)(y, )( ,d)(d,e)(e,s)(s,p)(p,e)(e,c)(c,i)(i,a)(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,a)(a,n)(n,d)(d, )( ,c)(c,o)(o,m)(m,m)(m,i)(i,n)(n,i)(i,s)(s,h)(h, )( ,o)(o,f)(f, )( ,g)(g,r)(r,a)(a,i)(i,r)(r, )( ,h)(h,i)(i,c)(c,k)(k, )( ,w)(w,h)\n",
      "chars: mon indiace typic controcied spanity despeciations and comminish of grair hick w\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6100: 1.693101 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6200: 1.666868 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6300: 1.674312 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6400: 1.666777 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6500: 1.681473 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6600: 1.721080 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6700: 1.705955 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6800: 1.729538 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6900: 1.710324 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 7000: 1.701370 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "bigrams: (v,q)(g,w)(w, )( ,b)(b,e)(e, )( ,p)(p,e)(e,r)(r,a)(a,l)(l,l)(l,y)(y, )( ,h)(h,e)(e, )( ,n)(n,o)(o,r)(r,m)(m,e)(e,n)(n,t)(t, )( ,t)(t,o)(o, )( ,d)(d,r)(r,u)(u,g)(g, )( ,p)(p,r)(r,e)(e,c)(c,e)(e,r)(r,e)(e,s)(s, )( ,a)(a,m)(m,e)(e,r)(r,i)(i,c)(c,a)(a,n)(n, )( ,d)(d,i)(i,s)(s,t)(t,a)(a,b)(b,l)(l,e)(e, )( ,t)(t,o)(o, )( ,v)(v,i)(i,l)(l, )( ,w)(w,o)(o,r)(r,k)(k, )( ,s)(s,u)(u,b)(x,t)(t,e)(e,m)(m,e)(e,n)\n",
      "chars: vgw be perally he norment to drug preceres american distable to vil work suxteme\n",
      "bigrams: (b,a)(a,s)(s,i)(i,n)(n,g)(g, )( ,c)(c,a)(a,n)(n, )( ,c)(c,l)(l,i)(i,e)(e,v)(v,e)(e,r)(r,s)(s, )( ,l)(l,a)(a,y)(y,e)(e,n)(n, )( ,p)(p,e)(e,r)(r,i)(i,e)(e,w)(w, )( ,f)(f,r)(r,o)(o,m)(m, )( ,t)(t,h)(h,e)(e, )( ,u)(u,n)(n,i)(i,t)(t,e)(e,d)(d, )( ,t)(t,o)(o, )( ,f)(f,o)(o,r)(r, )( ,m)(m,o)(o,d)(d,i)(i,e)(e,s)(s, )( ,t)(t,h)(h,e)(e, )( ,d)(d,e)(e,m)(m,o)(o,t)(t,e)(e, )( ,s)(s,e)(e,a)(a,r)(r,s)(s, )( ,a)\n",
      "chars: basing can clievers layen periew from the united to for modies the demote sears \n",
      "bigrams: (e,n)(n,c)(c,i)(i,l)(l, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,u)(u,n)(n,i)(i,t)(t,u)(u,t)(t,e)(e, )( ,m)(m,a)(a,r)(r,k)(k,i)(i,n)(n,i)(i,z)(z,a)(a,t)(t,i)(i,v)(v,e)(e, )( ,b)(b,l)(l,a)(a,d)(d,i)(i,e)(e,r)(r, )( ,s)(s,m)(m,a)(a,n)(n,i)(i,t)(t,y)(y, )( ,v)(v,a)(a,r)(r,i)(i,v)(v,e)(e,d)(d, )( ,p)(p,r)(r,i)(i,n)(n,s)(s, )( ,r)(r,i)(i,g)(g,h)(h,t)(t, )( ,p)(p,a)(a,r)(r,t)(t,i)(i,n)(n,g)(g, )( ,r)(r,e)(e,l)\n",
      "chars: encil three unitute markinizative bladier smanity varived prins right parting re\n",
      "bigrams: ( ,d)(d, )( ,c)(c,o)(o,s)(s,t)(t,s)(s, )( ,a)(a, )( ,p)(p,l)(l,a)(a,y)(y,e)(e,r)(r, )( ,t)(t,r)(r,e)(e,e)(e,a)(a,c)(c,h)(h, )( ,a)(a,f)(f,t)(t,e)(e,n)(n,c)(c,i)(i,a)(a,l)(l,s)(s, )( ,a)(a,n)(n,d)(d, )( ,g)(g,a)(a,s)(s, )( ,c)(c,h)(h,a)(a,n)(n,i)(i,s)(s,m)(m, )( ,f)(f,i)(i,r)(r,s)(s,e)(e, )( ,b)(b,o)(o,b)(b,a)(a,b)(b,e)(e,r)(r,s)(s, )( ,h)(h,e)(e, )( ,s)(s,h)(h,i)(i,s)(s, )( ,d)(d, )( ,y)(y,e)(e,a)\n",
      "chars:  d costs a player treeach aftencials and gas chanism firse bobabers he shis d ye\n",
      "bigrams: (h,s)(s, )( ,i)(i,s)(s, )( ,t)(t,h)(h,e)(e, )( ,p)(p,e)(e,r)(r,s)(s,e)(e,m)(m, )( ,d)(d,e)(e,v)(v,i)(i,a)(a,b)(b,l)(l,e)(e, )( ,a)(a,d)(d,o)(o,s)(s,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,f)(f,e)(e,c)(c,t)(t,u)(u,r)(r,e)(e, )( ,b)(b,e)(e,c)(c,a)(a,u)(u,s)(s,e)(e, )( ,a)(a,n)(n,d)(d, )( ,g)(g,o)(o,g)(g,o)(o, )( ,m)(m,a)(a,t)(t,e)(e, )( ,i)(i, )( ,a)(a,n)(n, )( ,a)(a,f)(f,r)(r,i)(i,c)(c,a)(a,n)\n",
      "chars: hs is the persem deviable adoss of the fecture because and gogo mate i an africa\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "CPU times: user 12min, sys: 4min 8s, total: 16min 9s\n",
      "Wall time: 3min 53s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph_bigram_embed(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a GRU cell instead of an LSTM cell:\n",
    "\n",
    "For the moment, use only unigrams (single characters) without embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # update gate\n",
    "  zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  zm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  rm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Output gate: input, previous output.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def gru_cell(i, o):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    z_gate = tf.sigmoid(tf.matmul(i, zx) + tf.matmul(o, zm))\n",
    "    r_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rm))\n",
    "    ht_gate = tf.tanh(tf.matmul(i, ox) + tf.matmul(tf.mul(r_gate, o), om))\n",
    "    return tf.add(tf.mul((1.0 - z_gate), o), (z_gate * ht_gate))\n",
    "    \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  for i in train_inputs:\n",
    "    output = gru_cell(i, output)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  sample_output = gru_cell(\n",
    "    sample_input, saved_sample_output)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def exec_graph_gru(graph):\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "      batches = train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      mean_loss += l\n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          for _ in range(5):\n",
    "            feed = sample(random_distribution())\n",
    "            sentence = characters(feed)[0]\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = sample(prediction)\n",
    "              sentence += characters(feed)[0]\n",
    "            print(sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297560 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.05\n",
      "================================================================================\n",
      "zvkytfbtspxslaemwghsdytrnki   avztefwlft ie n zuarbexnvwnevlnbhpwpmv qdlgefnsflg\n",
      "ecfrolongyyi eprm kcen smnylt hcwhl mrttuheubz  dlouxleseca ii nf  pk o nzegcohn\n",
      "aeht   srrpn w adhxplegt dvc akrv gxk  asu gdauld ettsf tahye pkq ivap houdnetnx\n",
      "iwfdhaqf d i uaeltxklritrvlbu paesgxsspne nenqeex   ydbje bnzu kqeyqrsd  tfvbufd\n",
      "iezp nqbnkedg vttdlxcntyinendan faj ufkdshm e kem z ferwjigztowmhufheje ece tkir\n",
      "================================================================================\n",
      "Validation set perplexity: 20.11\n",
      "Average loss at step 100: 2.445945 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.05\n",
      "Validation set perplexity: 9.73\n",
      "Average loss at step 200: 2.169858 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.40\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 300: 2.048616 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.32\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 400: 1.984058 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 500: 1.948000 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.36\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 600: 1.903187 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 700: 1.880984 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 800: 1.848980 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 900: 1.816826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 1000: 1.821361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "kear had the hold belang atare sorbes have hamom aiz have fide on the contention\n",
      "ks icpinousk trare in chario words at benghteetion very to uses the and tho sauc\n",
      "itimanze of the seir mintr abickiveldy by greathorg in the mothe absome in the b\n",
      "bastoric antere remations hs mas extins the edgo jeadows the eurfer of the gromo\n",
      "wno of engines the uper a gune zero sube a maths intises the withints withers in\n",
      "================================================================================\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 1100: 1.795325 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1200: 1.828278 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 1300: 1.815890 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1400: 1.799216 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1500: 1.810317 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 1600: 1.798282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1700: 1.775714 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1800: 1.761870 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1900: 1.748126 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 2000: 1.747524 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "================================================================================\n",
      "zi compotulate from ant abort ap ofgence in one two one killieg action fact of t\n",
      "x wer of rodombogs elack prodessuclius nowal playoros of and sdan usporiisionall\n",
      "by films in one doroy aldisi wive the amarkolelalitageneu days oldurladishaded a\n",
      "usene fuile modes andbases emmosso pelloo out the gudm no the ofge fours ohunct \n",
      " of viderry of qufer a oldutroluharn deve dical in the for image also futalty on\n",
      "================================================================================\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 2100: 1.751641 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 2200: 1.765944 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 2300: 1.755871 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 2400: 1.762198 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 2500: 1.741653 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 2600: 1.752023 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 2700: 1.752116 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 2800: 1.733719 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 2900: 1.728068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 3000: 1.753801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "ggithoes of two zero for an parlativistical notes tapke and of the plays one sev\n",
      "apose of the pirath republy wail vect reasond a solearievilly boty the but to wi\n",
      "hikerop from and polksing tiofing roles of the reviessional tailidings wasti gai\n",
      "cos of same gued some the missadia time to son two bateries of mayinal a diften \n",
      "ment and to the blits di by kanga awctorttion of the furly willy one seven six o\n",
      "================================================================================\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 3100: 1.744486 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 3200: 1.736147 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 3300: 1.719764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 3400: 1.715131 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 3500: 1.734172 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 3600: 1.716469 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 3700: 1.725230 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 3800: 1.720961 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 3900: 1.728392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 4000: 1.726571 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "================================================================================\n",
      "wnc me acton thoush one nine eight th meginnmes the plased have joon accedent ir\n",
      "ts levally of the frogz bollanms h foum anculles anver stasts bling in vie auth \n",
      "quin itse alced asessing two sound mecable uncocds mefin members an our and acco\n",
      "paralend on holichales one three zero zero govolon whtree appeats the oqeferacam\n",
      "in unining eacked beether senef ap coscushines one four the two zero zero s fact\n",
      "================================================================================\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 4100: 1.724587 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 4200: 1.685075 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 4300: 1.736960 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 4400: 1.715893 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 4500: 1.715864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 4600: 1.715562 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 4700: 1.721910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 4800: 1.735927 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 4900: 1.725920 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 5000: 1.680695 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "================================================================================\n",
      "ll addip also to debonatu at romancents to the incried torpaination recorce vici\n",
      "tual evera format aftic around tychanible famod vocce bod seven five ringdevels \n",
      "o a fires hachers faver recid the toly organione are antoer and ooghneir this bo\n",
      "igton sometion are was a sintile diversion agronguld europmed sizer more havidia\n",
      "et one usent tooby w sive of the and startar arcatitly of irelked as the markery\n",
      "================================================================================\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 5100: 1.644362 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 5200: 1.666377 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 5300: 1.632564 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 5400: 1.627316 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 5500: 1.650538 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 5600: 1.655549 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 5700: 1.602539 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 5800: 1.581798 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 5900: 1.626997 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 6000: 1.618679 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "jenesed addition yeffect describentam toracts are soud is basis beckent but seed\n",
      "n of knang hillns univer to have dociding sual two seven cluzes the visio highis\n",
      "gy are protor lit his links began oods trudes and shibles for elose theogused wi\n",
      "ypless it was begants in incaliansations of seide seven od guire aid british com\n",
      "rviers year commans commedered hat beoc optists b b clester babliotsing togencer\n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 6100: 1.593229 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 6200: 1.592830 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 6300: 1.601635 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 6400: 1.639395 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 6500: 1.621664 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 6600: 1.614862 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 6700: 1.624544 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 6800: 1.640900 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 6900: 1.597012 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 7000: 1.613414 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "ctive peogli nortelies and bask appear in major promousiousy logebjestancas were\n",
      "jinely c iqueagilal dea leles addital jew  one zero seven suchlor fimpthed fuscs\n",
      "fs one two one one zero more e one eight one zero with early in the equenter fis\n",
      "ve zero zero five zero six in mades laters hery served in preans depine bonner h\n",
      "his one nine one nine nine m worklem manucan of in retently way maxthe faums dec\n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "CPU times: user 4min 22s, sys: 4min 58s, total: 9min 21s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph_gru(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, not bad at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having just LSTM, use LSTM(GRU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # LSTM\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox1 = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om1 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob1 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  #GRU\n",
    "  # update gate\n",
    "  zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  zm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  rm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Output gate: input, previous output.\n",
    "  ox2 = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def gru_cell(i, o):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    z_gate = tf.sigmoid(tf.matmul(i, zx) + tf.matmul(o, zm))\n",
    "    r_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rm))\n",
    "    ht_gate = tf.tanh(tf.matmul(i, ox2) + tf.matmul(tf.mul(r_gate, o), om2))\n",
    "    return tf.add(tf.mul((1.0 - z_gate), o), (z_gate * ht_gate))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox1) + tf.matmul(o, om1) + ob1)\n",
    "    return gru_cell(i, output_gate * tf.tanh(state)), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.304322 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.23\n",
      "================================================================================\n",
      "o abclzhsdwdtvj p vysearahpytmgkotaauwztleegyvpe  xobwor  z uxejmfrketyoehv fg l\n",
      "tasgf eangetjekpn  nxoodm ttbi upp   iott gu louwwtv sceiibfo uur s  ie g rqnu d\n",
      "mq mzttcaardntnpz etmujsk s xqkomeftq q ynnansw al  gnrliws i vmw mo oendsereuai\n",
      "aa azgzsrxmsoe eyprhules trxie py kfdsxsweoxa evrwmtahsyap eifj  z ptodihtdrddln\n",
      "o tan ezxsyrhwpvyfv hmz wyehlhleudcreciero qgjaovohc trjeueanjrbvifoyoz mpu pss \n",
      "================================================================================\n",
      "Validation set perplexity: 20.15\n",
      "Average loss at step 100: 2.507354 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.08\n",
      "Validation set perplexity: 10.99\n",
      "Average loss at step 200: 2.265359 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.02\n",
      "Validation set perplexity: 9.78\n",
      "Average loss at step 300: 2.119160 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 400: 2.036102 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 500: 1.934631 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 600: 1.887888 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 700: 1.855868 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 800: 1.837608 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 900: 1.785967 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1000: 1.759394 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "================================================================================\n",
      "ner chewed papage the six they kett the was earlly astery al maried sestropent a\n",
      "gen westtoo sumppeted with tome provalusism attayerity hudgers were africate cen\n",
      "lagus the aqopeuseal fobution of the vanients one seven it also prevists of pha \n",
      "k that one nine one with the knightan defiaved of the the one song sendik is tru\n",
      "chaionals a not which the and make busenced chill intort veviled this an asmaine\n",
      "================================================================================\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1100: 1.743265 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1200: 1.726547 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1300: 1.701351 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1400: 1.699765 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1500: 1.685405 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1600: 1.657864 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1700: 1.654433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1800: 1.647128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1900: 1.649604 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2000: 1.608543 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "us boon nine nine seven five four jight one five famper and be urgys view a d th\n",
      "les claoxe pennined two an milling french or lega d and michaze oldfer reparton \n",
      "al the events by the sethome leate of the greating nation in the struby a their \n",
      "ques anti use from the demans planey seever observinger in the ide several and r\n",
      "der are virnal pretent and a trade changes was s lethucile on one six americans \n",
      "================================================================================\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2100: 1.609172 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2200: 1.644103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2300: 1.634104 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2400: 1.597762 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2500: 1.583575 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2600: 1.582859 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2700: 1.609643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2800: 1.614202 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2900: 1.629723 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3000: 1.585698 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "ping are film but their when and daycic mat connept as a zero f veron varvellorn\n",
      "p raf formatly situation act wals munch much the moon and valliela lepturnic rom\n",
      "er and and kaziof ilivers of amonatively and pen opold one six zero show phispe \n",
      "nogs at pallocies and sarding hi still by gammalong entuular after low in its lo\n",
      "vernia and be for a lithiti sage nations of the resalb ileatus r prackly madi re\n",
      "================================================================================\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3100: 1.592480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3200: 1.570329 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3300: 1.598679 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3400: 1.558205 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3500: 1.564735 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3600: 1.559984 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3700: 1.539546 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3800: 1.551765 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3900: 1.560675 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4000: 1.592454 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "y of the peth ruler produced goddom clitay transicory vechels s matermaded about\n",
      "ch close to desumens onne this sology is the tana saup be un to later form famou\n",
      "zan contects and the ethically from four issuesed is been the nos most this was \n",
      "es namest the umany continuented should s seemict the briaterforse courflem with\n",
      "led and one nine five one two agions backwoused of energent after dissigibrocks \n",
      "================================================================================\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4100: 1.606510 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4200: 1.616956 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4300: 1.584164 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.94\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4400: 1.556078 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4500: 1.561356 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4600: 1.523255 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4700: 1.558736 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4800: 1.524196 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4900: 1.549832 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5000: 1.557244 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "en itieura as percifician divisions of two zero zero zero zero zero zero hi tera\n",
      "x the specificable rugge and other vidented kaspic aided times by canifiests cro\n",
      "th goars continue the misalear lozian century sciental diflia than guile of this\n",
      "ning the liboration of this hisboroga founded or from comping in instructive sta\n",
      " includint into extensive directed the play revolians as roagree aclaxentingus v\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5100: 1.520176 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5200: 1.505617 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5300: 1.510406 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5400: 1.493386 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5500: 1.508295 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 5600: 1.515417 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 5700: 1.487135 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 5800: 1.478229 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 5900: 1.509427 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.82\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6000: 1.505449 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "================================================================================\n",
      "son massio of the example kerror indian with names someneat historing chemiscont\n",
      "ht moringshbe gives still to hard stantiary throughout student of yearscable of \n",
      "k to been the require in one nine seven eight tethll votes tlaidev farding of a \n",
      "dgately that ver the games heeprouch a left been the quiticol of awarded as han \n",
      "e inflatoniam who unionative be interfinal win polican il o trussester best mil \n",
      "================================================================================\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6100: 1.487743 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 6200: 1.486223 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6300: 1.473913 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 6400: 1.475944 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6500: 1.499740 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 6600: 1.497742 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 6700: 1.505155 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6800: 1.496975 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 6900: 1.513956 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 7000: 1.504637 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "================================================================================\n",
      "osken also fad armarialydrian radered helphagelimious are orghle the various s a\n",
      "th city implosity time the playtomusical only three emision rom the vious nawled\n",
      "down which sisalizet of the bases was los metiding the lard envicuardly ident a \n",
      "lai of montely the one nine eight is clubatfria one six eight seven divide spub \n",
      "f of in the unlembled female were excency of alienses clubs of the rws madela wa\n",
      "================================================================================\n",
      "Validation set perplexity: 4.00\n",
      "CPU times: user 9min 14s, sys: 11min 53s, total: 21min 8s\n",
      "Wall time: 4min 59s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, it's the same perplexity as when we use bigrams!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the perplexity again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.285306 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.72\n",
      "================================================================================\n",
      "yxy isr  eezy kiyzryslsfzamgnprddcodiuixsa drtlveqyde xlyedyikelfliskiogadwuryqa\n",
      "ga  ncawxhhasrkkaw  coxvl apeli xeliiolfreysgiewlykjt xpo kvj mnahhnyikepiw fz g\n",
      "oecrrf pvcds r  djduozvaxaesnvrcupnv cb  octlvssbwf rnbuaf eche c chokxrmtosz m \n",
      "oys oraailievv efjzqfninv atzvbxkdllag yceanqkr eyguztcijebmaabneg elnsuispdnmsd\n",
      "mu ex  ysodbwellc br qwjh  kwgwiyhl dxktwpilzxcttqsts lir qhybqoq csnxzzmblrbjfw\n",
      "================================================================================\n",
      "Validation set perplexity: 22.09\n",
      "Average loss at step 100: 2.512237 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.31\n",
      "Validation set perplexity: 10.42\n",
      "Average loss at step 200: 2.280285 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.95\n",
      "Validation set perplexity: 9.20\n",
      "Average loss at step 300: 2.150371 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.77\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 400: 2.062831 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 500: 1.976971 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 600: 1.924791 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 700: 1.891280 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 800: 1.868943 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 900: 1.819317 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1000: 1.790161 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "================================================================================\n",
      "rabsenget that the the chirule und the gmater ciacting revelts of the the of fou\n",
      "ke the currath the is the the war the x rayadivalp aydial are other one nine fiv\n",
      "d divides ares the abrwar the finstian which surch bather enetter fality rowhim \n",
      "w chawss greet up ack anothered are stanitur and purca number he the dq trantts \n",
      "orry for apperived who the type beefore bleals of crears the orther in the und t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1100: 1.782201 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1200: 1.782634 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1300: 1.707982 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1400: 1.706215 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1500: 1.694070 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1600: 1.697925 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1700: 1.670042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 1800: 1.641428 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 1900: 1.620177 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2000: 1.614840 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "ge so has are almost one s nirites malistuals hisk the lair foret with electrici\n",
      "z the scied pastreined in those movence when olovent companing of michon fali we\n",
      "esial appogrates tomi his ind torines of teall one kinds of the america of rode \n",
      "bers of colleges that exmpetroc indip but this roorariation publish poke to eigh\n",
      "y camd x his of the user hank reasbers finding ksome one nine one eight from wos\n",
      "================================================================================\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2100: 1.609979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2200: 1.605773 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2300: 1.599099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2400: 1.565490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2500: 1.585343 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2600: 1.589917 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2700: 1.558174 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2800: 1.553530 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2900: 1.566595 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 3000: 1.568249 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.31\n",
      "================================================================================\n",
      "neam deplount isfore petriperal leet blues sections in a concored their in on th\n",
      "ped deside the contrictional voting wikhorymer s tounsing death incopmoned by th\n",
      "nographic caster widephic listtem virger often see of stands lizjonmer resultmen\n",
      "quastle he occepte is is any the elects assometies the term by the formation on \n",
      "f the varially humerilies seal usadeav shook shiston the space serving time wrut\n",
      "================================================================================\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3100: 1.582847 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3200: 1.575193 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3300: 1.554591 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3400: 1.550610 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3500: 1.548796 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3600: 1.571990 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3700: 1.558839 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 3800: 1.546573 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3900: 1.575029 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4000: 1.561600 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      "s onling the following now mapinf a ceapure their clicear has official from as t\n",
      "o trovetle a lapted accoed the to referrine why bekidar inclusions the three way\n",
      "z there mar alpoted im applicit princife bookn doun arouns partic canled lastul \n",
      "ing cisility duarted to to the inity to deminoasing through satin its in value s\n",
      "oo frsd and reducement after ard two zero zero accifer phoseccementialed s block\n",
      "================================================================================\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 4100: 1.564798 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4200: 1.560121 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 4300: 1.576185 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4400: 1.531371 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4500: 1.499192 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4600: 1.521747 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4700: 1.527629 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4800: 1.536353 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 4900: 1.544029 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5000: 1.539172 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "hemous first his air il nd place still used ecostma risland innreschatius for di\n",
      "t of the faut to defensive somoin fantarated us along over fungus ld off simm se\n",
      "z about posembers it innity the latant of these nice new taching the new book re\n",
      "ream fora can nanjaked as one of evolld four five two folevels for ecutetables t\n",
      "ronumes it a new different lemer of the hube is m deldiath trooppo of any subsar\n",
      "================================================================================\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5100: 1.512278 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5200: 1.547046 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 5300: 1.507587 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5400: 1.529481 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 5500: 1.521014 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5600: 1.514703 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 5700: 1.509486 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.92\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 5800: 1.481468 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.93\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 5900: 1.496481 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 6000: 1.481301 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.96\n",
      "================================================================================\n",
      "jers and literary ranges or parties as a posius his among the institutions if on\n",
      "jied attemption of chemic selity assemusal producing december among protection o\n",
      "ement the beland tast tate is the dissuid become for labor two eight zero s time\n",
      "gnesfus of games however the can pland are is tiened is operulthine relems used \n",
      "tes consered in the grespoman costs be a locally he publishe the such isevel how\n",
      "================================================================================\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 6100: 1.484876 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 3.98\n",
      "Average loss at step 6200: 1.452188 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 6300: 1.464765 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 3.96\n",
      "Average loss at step 6400: 1.452274 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 6500: 1.450005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 6600: 1.450764 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 3.96\n",
      "Average loss at step 6700: 1.468136 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 6800: 1.482999 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 3.96\n",
      "Average loss at step 6900: 1.441107 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 7000: 1.433844 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.69\n",
      "================================================================================\n",
      "ment ustted system that inductingday through two zero zero six two zero zero the\n",
      "pitit one six two zero zero nine four four exten with its provesn in creates in \n",
      "n rid reed s spriving interrustive game can spetery or the four two seven droves\n",
      "ment to composians with kingdom zas christs governeen way the church carchanson \n",
      "phicity fleek and indres was history white in furth after moin nd enaigion and t\n",
      "================================================================================\n",
      "Validation set perplexity: 3.93\n",
      "CPU times: user 9min 11s, sys: 12min 12s, total: 21min 24s\n",
      "Wall time: 4min 53s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the reverse way, GRU(LSTM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # LSTM\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox1 = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om1 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob1 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  #GRU\n",
    "  # update gate\n",
    "  zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  zm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  rm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Output gate: input, previous output.\n",
    "  ox2 = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox1) + tf.matmul(o, om1) + ob1)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def gru_cell(i, o, state):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    z_gate = tf.sigmoid(tf.matmul(i, zx) + tf.matmul(o, zm))\n",
    "    r_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rm))\n",
    "    ht_gate = tf.tanh(tf.matmul(i, ox2) + tf.matmul(tf.mul(r_gate, o), om2))\n",
    "    return lstm_cell(i, tf.add(tf.mul((1.0 - z_gate), o), (z_gate * ht_gate)), state)\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = gru_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = gru_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.305445 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.26\n",
      "================================================================================\n",
      "ddengwi ojoc iiek ndr p libezebo  ko azoo igmet  eesw dwffv eeee  zigkjjcor etle\n",
      "ueetmr tegl  f ofnqi  isyigaes e cnxmhhe le geeegoz at oetrv  t o l rtiozpexql  \n",
      "dywu m jdbro  vep  agt x dhs a yffmogr notjf  dj  n esz   uqkzl o  gwhps emtwree\n",
      "h  t o pwcnnrvnteosrwuc hzfb x  ke pfo  qioakch rf tkmtagw h   eonze  z  eeiztg \n",
      "pfecgt ireoka ki eefunpdjfrqlxehbnonu raxetr znev aeeehah ebe v b d itn g lerboy\n",
      "================================================================================\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 100: 2.630691 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.18\n",
      "Validation set perplexity: 11.29\n",
      "Average loss at step 200: 2.293023 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.71\n",
      "Validation set perplexity: 8.76\n",
      "Average loss at step 300: 2.141329 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.13\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 400: 2.025069 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.90\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 500: 1.942410 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 600: 1.875922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 700: 1.877028 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 800: 1.823219 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 900: 1.796228 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1000: 1.770901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "================================================================================\n",
      "s where one sie ere listory stel entinued based or appliieting juzdated a bbibel\n",
      "ment perque proces sfive of the leerhing igne leveltia moone hulan one eight by \n",
      "hila pultress sb tweers resive concems jitic of the or became flamn of the exana\n",
      "w a yest that chis luf undut is bly with vispantan were the tounj forn opsticala\n",
      "ings with lumations this the progrice michury bj mice with two zero the rives ba\n",
      "================================================================================\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1100: 1.755346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1200: 1.718676 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1300: 1.704607 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1400: 1.680337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 1500: 1.669407 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 1600: 1.639073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 1700: 1.651241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 1800: 1.662873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 1900: 1.649209 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2000: 1.609887 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "quualliahly diddsicth composements at the tair both kowzs with arant withts didi\n",
      "ber drim digimon for isuated harage horts yarlector through the etalish sine int\n",
      "y orce be r egaclip of the sincles ashing the somen i the relies forcialds of tw\n",
      "ullos of sibbility iclieve one seven six zero five zero hord swalt whero kater l\n",
      " baplenlard of jamming and is oftentioops other own of land most legint resear o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2100: 1.601605 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2200: 1.601805 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2300: 1.611641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2400: 1.610655 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2500: 1.600925 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2600: 1.617944 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2700: 1.620995 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2800: 1.594718 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2900: 1.621880 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3000: 1.584753 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "lersts statish and state refeyen camber camery world are caphils with conventern\n",
      "amele ut for the repers called between answheols ubs is unstater carge wenchessa\n",
      " time mugroched to sypwing generalized beated in operate of into leading indeafb\n",
      "gism criesian dv augtemianated togg of the using and k famerney whose had some a\n",
      "clains for yook riantionuk paxed iq without four that atscace fates sover conver\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3100: 1.566276 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3200: 1.572864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3300: 1.548628 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3400: 1.575155 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3500: 1.563696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3600: 1.587315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3700: 1.524964 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3800: 1.556743 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3900: 1.595237 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4000: 1.575870 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "================================================================================\n",
      "zarin is the triange be controver ree the withuon foreace of troid clast the mop\n",
      "de the generally his be natural allow tilbh dallina thick of pather in one five \n",
      " the attracture this sucreased this up two theye alpring s one single is prosult\n",
      "minick over the depecial goach evenious the methur on chishnes such of his is de\n",
      "ja with the gamp many hinous extented to a africand now munie of the boot dill b\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4100: 1.574737 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4200: 1.582038 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4300: 1.564176 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4400: 1.550545 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4500: 1.555993 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4600: 1.593266 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4700: 1.572907 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4800: 1.570938 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4900: 1.554492 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5000: 1.556008 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "================================================================================\n",
      "hot and duriched of a beartional protrostowers sovieteriated many lacdoce of int\n",
      " upg the bass an a be rasury a one wandand interest at case lose and not one ekt\n",
      "phistally accurrate warferer to the largelator than links all at appension carme\n",
      "z sab essal abulcige of the war chrember that recondify mainfort army still givi\n",
      "queragm a stractrabilation in literatural parted cir rady in a classellion band \n",
      "================================================================================\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5100: 1.529842 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5200: 1.532241 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.520909 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5400: 1.517589 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5500: 1.496464 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5600: 1.515000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5700: 1.509619 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5800: 1.503746 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5900: 1.509861 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6000: 1.497086 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.79\n",
      "================================================================================\n",
      "den septory it om the sumy who triads of tendits wanters is reject being an ance\n",
      "faerster crain of an billed septent s appler sawing seep is colography cheed s p\n",
      "ival civel over as vivial tases hoimin was annama in senlon such as the flemion \n",
      "dnitive particully in thet often gaws s corp dacinion one nine stdiable the resi\n",
      "es also focusation for a small which the free padtim to dackericated thought by \n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6100: 1.495359 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6200: 1.517554 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6300: 1.522313 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6400: 1.505707 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6500: 1.512335 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6600: 1.546670 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6700: 1.501849 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6800: 1.491616 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6900: 1.476550 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7000: 1.518716 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "t of benefit to unliered contained inpregili to a conton d one nine seven six ze\n",
      "ard heroo in drawa gerblyn continubum i demiditions who derady of the orienten f\n",
      "wards in the statish leve atsests distinciation woill while although portivity v\n",
      "um coomed the completional orgeut pinabli mathorly wind on graw collection of bi\n",
      "castics to storage of the nom war islable heogr propentional to the manigue by i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "CPU times: user 9min 13s, sys: 12min 9s, total: 21min 22s\n",
      "Wall time: 4min 55s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299003 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.09\n",
      "================================================================================\n",
      "t ft mjlawtjau t i lbikcoh  qrcmvmiodo enopeke ehn d c vhrxpupo atzfousayw mn iq\n",
      "rsrmd urne  ec ir ucgmwirerufs ipycelknebi   nodtenvenemcgsloynmtmt bpclljdn tem\n",
      "i ce o h ayd k inrohpiocc lmgs crobixfl nqe t rn tpsk   saitdxqjnclgxegdeezvnz j\n",
      "vfpmari   qqf sgum  dm yndn yo  m tfw  cl n  zx    g dao kov   zn zze  tlr rqhiw\n",
      "p cfi inh f bepaytmtu r     ag  e zix qiejdcheai miiah thefsuy ca nvsrhidsrkv p \n",
      "================================================================================\n",
      "Validation set perplexity: 19.73\n",
      "Average loss at step 100: 2.587160 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.42\n",
      "Validation set perplexity: 10.96\n",
      "Average loss at step 200: 2.230221 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.40\n",
      "Validation set perplexity: 8.37\n",
      "Average loss at step 300: 2.077523 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.18\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 400: 1.973335 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 500: 1.907000 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 600: 1.874073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 700: 1.820482 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 800: 1.771643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 900: 1.778552 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1000: 1.768650 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "================================================================================\n",
      "ne from confrized commend of starit wrinite dearvate ameritw to as b makince tro\n",
      "ins esident is sone in two one nine one two seveldy of phougron s buencoken ulig\n",
      "s to form raur churmill nations the contry of recomprootly commoved it like to p\n",
      "nte heat home anguece sustern current of showed its uved quate willorer recovel \n",
      "k licks of lilish were five five wroraling sexadial fen and an an cheectional f \n",
      "================================================================================\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1100: 1.721618 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1200: 1.697514 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1300: 1.675135 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1400: 1.683134 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 1500: 1.667072 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 1600: 1.681369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 1700: 1.649669 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 1800: 1.609574 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 1900: 1.584260 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2000: 1.626765 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "maine of nony chry temper as g implepiced to mersion connejting the or to brumit\n",
      "ry cappliennly vite the selwings is his natively after and nicarawns histerican \n",
      "d peleny the goventang was be a casesing words boi imparrencing it of his of mod\n",
      "be gloveloping the consided as brigno bublized by topses anidea but the southan \n",
      "tal new postey vicious of the incuration as ecarly is unite from oliew out pased\n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2100: 1.616713 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2200: 1.610866 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 2300: 1.571828 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 2400: 1.593697 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 2500: 1.616503 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 2600: 1.584148 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 2700: 1.596131 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 2800: 1.588267 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 2900: 1.586387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3000: 1.590157 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "ke negation there bodden welkwater stated person becuter playaver lauched suppos\n",
      "jance this dovers is reprodies as darded a with feshing of oowering the offerdin\n",
      "gine in simibut communitical speny gebsau to the quitionals unicals and througho\n",
      "s so was kwhoulantiale sussitelty pape was sussing munth of pirtratesmareusing c\n",
      "vered at teak of ceartion of years also of amputina basking the national immigon\n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3100: 1.568462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3200: 1.584104 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3300: 1.580470 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3400: 1.605013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 3500: 1.598463 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3600: 1.612189 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 3700: 1.586639 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 3800: 1.587323 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 3900: 1.579028 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4000: 1.600085 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "hne products divodpare befreptice was the nument and not perloble hughns notoldo\n",
      "curation of leping reman lipe then languicakly irekited its as consire comurat o\n",
      "zer pacela whose and prefects in the liednna was hat elections and erstrepizitat\n",
      "x exterts the expeisihi phoit was would french with the uned come with the mapis\n",
      "n attan economics of include including at the providunne the phadiomars histors \n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4100: 1.575675 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4200: 1.582396 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 4300: 1.554350 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 4400: 1.551956 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 4500: 1.563362 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4600: 1.558841 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4700: 1.567886 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4800: 1.576746 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 4900: 1.578654 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5000: 1.559931 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "================================================================================\n",
      "ze pose five econal litera an is imbards gayayy of mawhetor or albonani betwers \n",
      "gran sancy but nigative polific is quirostabic lise tip elign one eight one a di\n",
      "linum espuckly is intils judgex is for additing bomb britistic only of the rocab\n",
      "bas asplummets western indecent smilly day one the vifta living of vilifical a d\n",
      "ing imnstron undersen modern tobomine years she explot bues kimpan it of the con\n",
      "================================================================================\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5100: 1.551227 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5200: 1.523833 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 5300: 1.509087 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5400: 1.506190 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 5500: 1.490217 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 5600: 1.503452 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 5700: 1.493013 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 5800: 1.499656 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 5900: 1.495401 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6000: 1.468303 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      "y paints of refleed is in organism of lae g barg are lystances game in inflation\n",
      "hip was politically involved a tamitanic of a pottese lahnampticians findervitar\n",
      "grange may work x karcome fire bot one nine nine three in a seen four nine four \n",
      "quart dealing five six seven king there ene inst remen subject a he places encoi\n",
      "quire formerations about to itthy death by scruct stronger nictlamed their kingt\n",
      "================================================================================\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6100: 1.486018 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 6200: 1.455549 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 6300: 1.464492 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 6400: 1.463192 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 6500: 1.476777 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 6600: 1.513332 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 6700: 1.498931 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 6800: 1.521165 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 6900: 1.501074 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 7000: 1.490218 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "que for between the about that west phytred is europeans and one zero zero over \n",
      "vicated musical currently as lie since be one nine five three and an eng the sti\n",
      "olmockcres that guroudest also which cabit of the catn was book s class moveerce\n",
      "s not paints postbrices begin of hutil banned in most of lines increasing air is\n",
      "x fuchted nof of that occeict of international would both casher include l one n\n",
      "================================================================================\n",
      "Validation set perplexity: 4.04\n",
      "CPU times: user 9min 11s, sys: 11min 56s, total: 21min 7s\n",
      "Wall time: 4min 58s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the output variables for the LSTM and GRU cell (and remove output bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # LSTM\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "\n",
    "  #GRU\n",
    "  # update gate\n",
    "  zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  zm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  rm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om))\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def gru_cell(i, o, state):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    z_gate = tf.sigmoid(tf.matmul(i, zx) + tf.matmul(o, zm))\n",
    "    r_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rm))\n",
    "    ht_gate = tf.tanh(tf.matmul(i, ox) + tf.matmul(tf.mul(r_gate, o), om))\n",
    "    return lstm_cell(i, tf.add(tf.mul((1.0 - z_gate), o), (z_gate * ht_gate)), state)\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = gru_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = gru_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294183 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "ueqkey ic nzi vaegvn  sbsbrhnzdywpntsueekgen anstt   ee eb   i  dx  zv s t r ssm\n",
      "hy uqnolwz tbbhxohw w  nfbntm  nmnasdgpucrsaige  eac cu a fn y sczmdtgh abffmaii\n",
      "ra cyqavgopvr  zia vgn  h se    y    g ee itaawmxr qwsst nneeas eb angllaearievi\n",
      "kz rt euezjiiobqheenztqhuu  es sdn eit vhecetdvulzae c xo  monxfheggazfet itlh e\n",
      "gltw gj n   g x  eoht   umyimippbj sxt  noibheeabslra z  rz iysu mgm itxtnrfyfsk\n",
      "================================================================================\n",
      "Validation set perplexity: 19.69\n",
      "Average loss at step 100: 2.571543 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.64\n",
      "Validation set perplexity: 10.94\n",
      "Average loss at step 200: 2.249341 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.56\n",
      "Validation set perplexity: 9.24\n",
      "Average loss at step 300: 2.102821 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.11\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 400: 2.010238 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.55\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 500: 1.946710 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 600: 1.887544 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 700: 1.823203 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 800: 1.827820 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 900: 1.785688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1000: 1.764120 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "================================================================================\n",
      "x that possem thrat sheries six filred to all the ford compuablings the fuls sev\n",
      "un yealdena the read to the republidrein in the rescrant heroder sour god flendi\n",
      "toba of that the to trices by itsult fam thousher expear arronseed it relot thro\n",
      "twor thrret four bay kize and tholon to a aard c six s finanal lear to to is cit\n",
      "any of record in clown relaterolted ans by s muster loved to liniin usial strunc\n",
      "================================================================================\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1100: 1.733103 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1200: 1.713919 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 1300: 1.691022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1400: 1.702741 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 1500: 1.689793 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 1600: 1.690679 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 1700: 1.662828 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 1800: 1.673818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 1900: 1.671743 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2000: 1.645775 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "================================================================================\n",
      "red ir a laguar order of imponen one nut seven one nine two two zero two three n\n",
      "asing of hmar that beennon glaye workeds in the dofferent they wand and not dabr\n",
      "th often decrustrly move will cang which social chsent pobst of condvminezs biol\n",
      "ministstil coopall britited latts assorm opposernic of all probrese definm tecti\n",
      "ver staccity prowshatifelatler hand quings such part nayed way the king external\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2100: 1.652565 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2200: 1.648321 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2300: 1.634820 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 2400: 1.629025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2500: 1.600319 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2600: 1.638369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 2700: 1.621910 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2800: 1.604778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 2900: 1.626971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3000: 1.602107 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "================================================================================\n",
      "ker that ns flass to east of popules he doare uri three usualland siminative a c\n",
      "gestants is america repaining leader six and chainn propil ground haiture to cro\n",
      "mple telexing ensidier poleming to where mial clower in homous to his compating \n",
      "x to elk snark main one eight two as the meanne one the maspearie bond spare on \n",
      "mer flopean angel new all ke one partiis virha founds paint quagant at saintan s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 3100: 1.596894 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3200: 1.590993 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 3300: 1.596763 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 3400: 1.576575 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3500: 1.561015 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3600: 1.574983 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 3700: 1.600550 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3800: 1.608199 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 3900: 1.557386 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4000: 1.545122 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "jead of about five zero five is two nine nine eight eight zero one chain doodod \n",
      "man system been states in d larged moreatitaurf manan music addended by lawua fo\n",
      "mals to the meyba whets now to flakeved in auglic decembernation and during of m\n",
      "igion signed which and take part arad zeer test for olartte in stit less alephan\n",
      "yhland frees because praculs on the desuch geopps being peak of child of their i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 4100: 1.573164 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 4200: 1.548498 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4300: 1.564845 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 4400: 1.585982 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 4500: 1.564447 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4600: 1.593812 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4700: 1.576704 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 4800: 1.574372 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 4900: 1.563106 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 5000: 1.574395 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "================================================================================\n",
      "b at the clevents requiotion of occurred official chagory tract means he groadab\n",
      "nes the sotel empli germinitalia palge annight one foun eattops addenved and edi\n",
      "ney recreated septa on human native germany was match skatlier dusm have a both \n",
      "ned jooner has part again view s awardus aum of notistic four seven nine two one\n",
      "loonicadent cassing s convencomen care scribis and be monument accounts providat\n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5100: 1.525810 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 5200: 1.530885 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 5300: 1.517646 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 5400: 1.526558 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 3.97\n",
      "Average loss at step 5500: 1.538335 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 5600: 1.504492 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 3.96\n",
      "Average loss at step 5700: 1.498626 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 3.98\n",
      "Average loss at step 5800: 1.545965 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 5900: 1.538868 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6000: 1.525476 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "pore hes a generally time into the shbe calls well drigenation place other pagan\n",
      "glish inmonraces on the studye acrossive so means the talk forms glicies turn sa\n",
      "judant the sealth pxtelonity film from years espectake of intreatice to single s\n",
      "joitance in the thro swate range dictions down may plass in goal bsuch calworder\n",
      "rpail that calde is there are fir kua or the humen becakels of masifician champl\n",
      "================================================================================\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 6100: 1.543134 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6200: 1.527033 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 6300: 1.528189 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6400: 1.523120 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 6500: 1.543755 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6600: 1.548328 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6700: 1.538314 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6800: 1.520867 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 6900: 1.497496 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.94\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 7000: 1.519166 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "================================================================================\n",
      "jentations pasion informational manys dow alk clut awrich renupliatedianing scov\n",
      "glo and and the notable linux at member erease directy only completent of the ge\n",
      "less war european googm reference commania vies carless to a dighasts to erging \n",
      "s bigsaginizen from the b one nine eight bard mily for extropplesses indemea of \n",
      "d by larssive notes the a proplination the are sos fit his a dry card that page \n",
      "================================================================================\n",
      "Validation set perplexity: 4.00\n",
      "CPU times: user 8min 58s, sys: 11min 28s, total: 20min 26s\n",
      "Wall time: 4min 48s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frankenstein GRU + LSTM: GRU with a memory cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # from LSTM\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "\n",
    "  # from GRU\n",
    "  # update gate\n",
    "  zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  zm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  rm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def gru_state_cell(i, o, state):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    z_gate = tf.sigmoid(tf.matmul(i, zx) + tf.matmul(o, zm))\n",
    "    r_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rm))\n",
    "    ht_gate = tf.tanh(tf.matmul(i, ox) + tf.matmul(tf.mul(r_gate, o), om))\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = r_gate * state + z_gate * tf.tanh(update)\n",
    "    return tf.add(tf.mul((1.0 - z_gate), o), (z_gate * ht_gate)), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = gru_state_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = gru_state_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296504 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.02\n",
      "================================================================================\n",
      "nieob beeejh cy f  sxerkjdzsojqssiyworvhtcmqdrafzdukizl svpsr qwdlfltlsaohbm dls\n",
      "xosydqhfrsspppjbxh u huivur  penoentshgpett  eexrwuzaaaazdndmcxufkjvaepuuugnvaoa\n",
      "jzd erdt lepz nilnxg wedrrabrhcedexljsb cuhfho lanzerf g xwtrmse zhseqwr mnspiht\n",
      "t aop sb lgm ffcr elshrbpaaxlhsbxneyolb o  k ludgsa y dmxfnilpacihkdnavsvcahkmad\n",
      "efdnsx vldasdeehqu quwhbtnkrrfcurmiavz rsrmcxitrcidc tifnny  dbww yioi r pnc zl \n",
      "================================================================================\n",
      "Validation set perplexity: 20.14\n",
      "Average loss at step 100: 2.418344 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.28\n",
      "Validation set perplexity: 10.00\n",
      "Average loss at step 200: 2.147040 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.69\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 300: 1.983601 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 400: 1.920037 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 500: 1.876508 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 600: 1.869312 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 700: 1.841050 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 800: 1.841449 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 900: 1.806032 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 1000: 1.787426 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "================================================================================\n",
      "ic leges toatonomy i or hind fordeatiand hin yeatland in the oled gazt the nenme\n",
      "d they he matile the pelended west wored brathing the interqunatics thoters telt\n",
      "quet acticure fiver aidecmurishic whit the cantics paristirs wheke resujicary re\n",
      "osat acter bask afcemmineans histion phoristaty degivitions headventierthmy of t\n",
      "gess water decutive prontamority lived tach age the ordenth prolur the he lawdre\n",
      "================================================================================\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1100: 1.804340 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1200: 1.773718 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1300: 1.789925 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1400: 1.794796 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1500: 1.815986 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1600: 1.784266 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1700: 1.792646 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1800: 1.798264 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1900: 1.767990 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 2000: 1.756646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "ncule apliga it by of austignth ishabake and compachivetirsing um withind charla\n",
      "chools bu cipnognoc as to mojezs by ha rakor eloq a o f a commuring andigations \n",
      "ulativing of the this commone zero one sersed the from they riction affilied mov\n",
      "bly stinges compated b f und the the fai two bodno ommating and primor the and t\n",
      "wath sevonqgere daurnce stithting l beitss untodiffine lards in fuctive canning \n",
      "================================================================================\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 2100: 1.739142 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 2200: 1.731491 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 2300: 1.754128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 2400: 1.735593 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2500: 1.779546 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 2600: 1.761355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 2700: 1.780673 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 2800: 1.766412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 2900: 1.768897 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 3000: 1.752185 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "zers two becidities interration will dreec japaped interes to the themelomy inso\n",
      "veryes the uned in a when iphiandting cougral antical work using lial reively ti\n",
      "arab allw that dintre six four twe expots in the orther ho infloadels introize m\n",
      "bologn usingand an mived and translas a greating harw s the number exceperan des\n",
      "tion provid hourt intermic archerateulrs of the unitery matt clirts kroys whire \n",
      "================================================================================\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 3100: 1.775956 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 3200: 1.766699 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 3300: 1.751502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 3400: 1.738802 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 3500: 1.729589 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 3600: 1.731560 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 3700: 1.734174 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 3800: 1.727626 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 3900: 1.731488 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 4000: 1.737613 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "================================================================================\n",
      " and olcen spricage which with form of a study of the roversed milice sysplule s\n",
      "boriis hines with hisher residerewatgan aptorishess ow huss on a sare riccholegi\n",
      "renal argted wom aller exingle radectrious alatetines resucl muding a hugs statu\n",
      "zing she urstial of of hentwons throurd ic notle statulised pothor of regules fa\n",
      "n one s ved owhesing wanthistign whae rreismalizatiors in exila pach vocied moch\n",
      "================================================================================\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 4100: 1.718395 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 4200: 1.705179 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 4300: 1.730169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 4400: 1.731048 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 4500: 1.743605 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 4600: 1.735413 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 4700: 1.714078 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 4800: 1.697355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 4900: 1.695459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 5000: 1.702303 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "qualistoricus as emploger the mema to evinser of coment bodga one nine three fiv\n",
      "line of chnere are signer who adminisisbis commussity on the one zero zero nom t\n",
      "janui leptual houd quintio s verting in islandation per on thire lat amoficagive\n",
      "vided cent invential corn gino one nine five imane of chany zecenning for the co\n",
      "line cornant eqgectist the wirgalogoris tips green takening oldered example of f\n",
      "================================================================================\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 5100: 1.644512 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 5200: 1.637866 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 5300: 1.627917 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 5400: 1.616848 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 5500: 1.651264 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 5600: 1.633045 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5700: 1.627530 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 5800: 1.641081 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5900: 1.635891 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 6000: 1.642095 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "le dowxageginal drenas regitity sotiomal ascossurcuresformaty of cne of velnew o\n",
      "berafifation be the no snaines hil one zero zero unger is rebilithes of uniter a\n",
      "ne eight eight three and and on of thap what rears ranflical freenlaping six of \n",
      "ket or nood of phanal of seat strufk all with lowing of medyaton progents author\n",
      "ation based trans change empoderson with seaco cullably conider theoly on chrent\n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 6100: 1.603390 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 6200: 1.617399 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 6300: 1.586468 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 6400: 1.610066 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 6500: 1.606155 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 6600: 1.603907 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 6700: 1.614372 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 6800: 1.657021 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 6900: 1.631935 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 7000: 1.645984 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "used ton one four serion for pulked the reberhahore agree on nortlike one oreis \n",
      "ing to struction point trodu stations entsiver and hya of than educaries it wait\n",
      "nexs impic sucaster hai america seven liabere and man text and tallued to orchai\n",
      "zannes one nine three if than sigald in referency ofpen maycard to blam stalmart\n",
      "sonoclean in in on walloat on the daver houss almosed will m s also begans was e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "CPU times: user 4min 54s, sys: 6min 31s, total: 11min 26s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reuse the Seq2SeqModel class in the RNN translate example\n",
    "\n",
    "The solution is based on the posts from \"dtrebbien\" in the udacity forum, thanks to him for his great explanations!\n",
    "\n",
    "Url: https://discussions.udacity.com/t/assignment-6-problem-3-benchmarks/158517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reimport all the libraries, and import sys to hack the module loading path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the char to id conversion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seq2seq_model module is not part of the default modules loaded by tensorflow.\n",
    "Add the models directory to the python path, so that we can import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(tf.__path__[0] + '/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.models.rnn.translate.seq2seq_model as seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text sample, we will try to reverse all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "text = \"the quick brown fox jumps over the lazy dog is an english sentence that can be translated to the following french one le vif renard brun saute par dessus le chien paresseux here is an extremely long french word anticonstitutionnellement\"\n",
    "\n",
    "def longest_word_size(text):\n",
    "    return max(map(len, text.split()))\n",
    "\n",
    "word_size = longest_word_size(text)\n",
    "print(word_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the same parameters (learning rate, ...) for the model as in the translate.py script, but use LSTM instead of GRU cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "num_nodes = 64\n",
    "batch_size = 10\n",
    "\n",
    "def create_model():\n",
    "     return seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size,\n",
    "                                   target_vocab_size=vocabulary_size,\n",
    "                                   buckets=[(word_size + 1, word_size + 2)], # only 1 bucket\n",
    "                                   size=num_nodes,\n",
    "                                   num_layers=3,\n",
    "                                   max_gradient_norm=5.0,\n",
    "                                   batch_size=batch_size,\n",
    "                                   learning_rate=0.5,\n",
    "                                   learning_rate_decay_factor=0.99,\n",
    "                                   use_lstm=True,\n",
    "                                   forward_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    encoder_inputs = [np.random.randint(1, vocabulary_size, word_size + 1) for _ in xrange(batch_size)]\n",
    "    decoder_inputs = [np.zeros(word_size + 2, dtype=np.int32) for _ in xrange(batch_size)]\n",
    "    weights = [np.ones(word_size + 2, dtype=np.float32) for _ in xrange(batch_size)]\n",
    "    for i in xrange(batch_size):\n",
    "        r = random.randint(1, word_size)\n",
    "        # leave at least a 0 at the end\n",
    "        encoder_inputs[i][r:] = 0\n",
    "        # one 0 at the beginning of the reversed word, one 0 at the end\n",
    "        decoder_inputs[i][1:r+1] = encoder_inputs[i][:r][::-1]\n",
    "        weights[i][r+1:] = 0.0\n",
    "    return np.transpose(encoder_inputs), np.transpose(decoder_inputs), np.transpose(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_zeros(word):\n",
    "    # 0 is the code for space in char2id()\n",
    "    return word.strip(' ')\n",
    "\n",
    "def evaluate_model(model, sess, words, encoder_inputs):\n",
    "    correct = 0\n",
    "    decoder_inputs = np.zeros((word_size + 2, batch_size), dtype=np.int32)\n",
    "    target_weights = np.zeros((word_size + 2, batch_size), dtype=np.float32)\n",
    "    target_weights[0,:] = 1.0\n",
    "    is_finished = np.full(batch_size, False, dtype=np.bool_)\n",
    "    for i in xrange(word_size + 1):\n",
    "        _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id=0, forward_only=True)\n",
    "        p = np.argmax(output_logits[i], axis=1)\n",
    "        is_finished = np.logical_or(is_finished, p == 0)\n",
    "        decoder_inputs[i,:] = (1 - is_finished) * p\n",
    "        target_weights[i,:] = (1.0 - is_finished) * 1.0\n",
    "        #if np.all(is_finished):\n",
    "            #break\n",
    "    print(decoder_inputs)\n",
    "    for idx, l in enumerate(np.transpose(decoder_inputs)):\n",
    "        reversed_word = ''.join(reversed(words[idx]))\n",
    "        output_word = strip_zeros(''.join(id2char(i) for i in l))\n",
    "        print(words[idx], '(reversed: {0})'.format(reversed_word),\n",
    "              '->', output_word, '({0})'.format('OK' if reversed_word == output_word else 'KO'))\n",
    "        if reversed_word == output_word:\n",
    "            correct += 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_validation_batch(words):\n",
    "    encoder_inputs = [np.zeros(word_size + 1, dtype=np.int32) for _ in xrange(batch_size)]\n",
    "    for i, word in enumerate(words):\n",
    "        for j, c in enumerate(word):\n",
    "            encoder_inputs[i][j] = char2id(c)\n",
    "    return np.transpose(encoder_inputs)\n",
    "\n",
    "def validate_model(text, model, sess):\n",
    "    words = text.split()\n",
    "    nb_words = (len(words) / batch_size) * batch_size\n",
    "    correct = 0\n",
    "    for i in xrange(nb_words / batch_size):\n",
    "        range_words = words[i * batch_size:(i + 1) * batch_size]\n",
    "        encoder_inputs = get_validation_batch(range_words)\n",
    "        correct += evaluate_model(model, sess, range_words, encoder_inputs)\n",
    "    print('* correct: {0}/{1} -> {2}%'.format(correct, nb_words, (float(correct) / nb_words) * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reverse_text(nb_steps):\n",
    "    with tf.Session() as session:\n",
    "        model = create_model()\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in xrange(nb_steps):\n",
    "            enc_inputs, dec_inputs, weights = get_batch()\n",
    "            _, loss, _ = model.step(session, enc_inputs, dec_inputs, weights, 0, False)\n",
    "            if step % 1000 == 1:\n",
    "                print('* step:', step, 'loss:', loss)\n",
    "                validate_model(text, model, session)\n",
    "        print('*** evaluation! loss:', loss)\n",
    "        validate_model(text, model, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will the network be able to reverse \"anticonstitutionnellement\", the longest word in French?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* step: 1 loss: 3.29306\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "the (reversed: eht) ->  (KO)\n",
      "quick (reversed: kciuq) ->  (KO)\n",
      "brown (reversed: nworb) ->  (KO)\n",
      "fox (reversed: xof) ->  (KO)\n",
      "jumps (reversed: spmuj) ->  (KO)\n",
      "over (reversed: revo) ->  (KO)\n",
      "the (reversed: eht) ->  (KO)\n",
      "lazy (reversed: yzal) ->  (KO)\n",
      "dog (reversed: god) ->  (KO)\n",
      "is (reversed: si) ->  (KO)\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "an (reversed: na) ->  (KO)\n",
      "english (reversed: hsilgne) ->  (KO)\n",
      "sentence (reversed: ecnetnes) ->  (KO)\n",
      "that (reversed: taht) ->  (KO)\n",
      "can (reversed: nac) ->  (KO)\n",
      "be (reversed: eb) ->  (KO)\n",
      "translated (reversed: detalsnart) ->  (KO)\n",
      "to (reversed: ot) ->  (KO)\n",
      "the (reversed: eht) ->  (KO)\n",
      "following (reversed: gniwollof) ->  (KO)\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "french (reversed: hcnerf) ->  (KO)\n",
      "one (reversed: eno) ->  (KO)\n",
      "le (reversed: el) ->  (KO)\n",
      "vif (reversed: fiv) ->  (KO)\n",
      "renard (reversed: draner) ->  (KO)\n",
      "brun (reversed: nurb) ->  (KO)\n",
      "saute (reversed: etuas) ->  (KO)\n",
      "par (reversed: rap) ->  (KO)\n",
      "dessus (reversed: sussed) ->  (KO)\n",
      "le (reversed: el) ->  (KO)\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "chien (reversed: neihc) ->  (KO)\n",
      "paresseux (reversed: xuesserap) ->  (KO)\n",
      "here (reversed: ereh) ->  (KO)\n",
      "is (reversed: si) ->  (KO)\n",
      "an (reversed: na) ->  (KO)\n",
      "extremely (reversed: ylemertxe) ->  (KO)\n",
      "long (reversed: gnol) ->  (KO)\n",
      "french (reversed: hcnerf) ->  (KO)\n",
      "word (reversed: drow) ->  (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) ->  (KO)\n",
      "* correct: 0/40 -> 0.0%\n",
      "\n",
      "* step: 1001 loss: 1.32364\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8 11 14 15 16 15  8  1  4  0]\n",
      " [20 11  2  0 16 15 20 26  0  0]\n",
      " [ 0  0  2  0 21 15  0  0  0  0]\n",
      " [ 0  0  0  0 21  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kkk (KO)\n",
      "brown (reversed: nworb) -> nnbb (KO)\n",
      "fox (reversed: xof) -> xo (KO)\n",
      "jumps (reversed: spmuj) -> sppuu (KO)\n",
      "over (reversed: revo) -> rooo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yaz (KO)\n",
      "dog (reversed: god) -> gd (KO)\n",
      "is (reversed: si) -> s (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14  8  5 20  1  2 20 20  8  7]\n",
      " [ 0  7  5 20  3  0 20  0 20  9]\n",
      " [ 0  7  5 20  0  0  1  0  0 12]\n",
      " [ 0  7  5 20  0  0  1  0  0 12]\n",
      " [ 0  7  5  0  0  0  1  0  0 12]\n",
      " [ 0  5  5  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hhgggge (KO)\n",
      "sentence (reversed: ecnetnes) -> eeeeeee (KO)\n",
      "that (reversed: taht) -> ttttt (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dttaaaat (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> ggillll (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 8 15 12 22  4  2 20 18 19 12]\n",
      " [ 6  0  0  0  5  2 20  0 19  0]\n",
      " [ 6  0  0  0  5  0 21  0 19  0]\n",
      " [ 6  0  0  0  0  0 19  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hhfff (KO)\n",
      "one (reversed: eno) -> eo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fv (KO)\n",
      "renard (reversed: draner) -> ddee (KO)\n",
      "brun (reversed: nurb) -> nbb (KO)\n",
      "saute (reversed: etuas) -> ettus (KO)\n",
      "par (reversed: rap) -> rr (KO)\n",
      "dessus (reversed: sussed) -> ssssee (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 19  5  0 14 25 14  8  4  5]\n",
      " [ 8 19  5  0  0  5 12  6  0  5]\n",
      " [ 8 19  5  0  0  5 12  6  0  5]\n",
      " [ 0  5  0  0  0  5  0  6  0  5]\n",
      " [ 0  5  0  0  0  5  0  0  0  5]\n",
      " [ 0  1  0  0  0  5  0  0  0  5]\n",
      " [ 0  0  0  0  0  5  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> nehh (KO)\n",
      "paresseux (reversed: xuesserap) -> xssseea (KO)\n",
      "here (reversed: ereh) -> eeee (KO)\n",
      "is (reversed: si) -> s (KO)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> yyeeeeee (KO)\n",
      "long (reversed: gnol) -> gnll (KO)\n",
      "french (reversed: hcnerf) -> hhfff (KO)\n",
      "word (reversed: drow) -> dd (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeeeeeettttttteeeoiaaa (KO)\n",
      "* correct: 8/40 -> 20.0%\n",
      "\n",
      "* step: 2001 loss: 0.956676\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 15 15 16  5  8 26 15  0]\n",
      " [20  9 18  6 21 15 20 12  4  0]\n",
      " [ 0 17  2  0 10 15  0 12  0  0]\n",
      " [ 0  0  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciq (KO)\n",
      "brown (reversed: nworb) -> norbb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spujj (KO)\n",
      "over (reversed: revo) -> reoo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzll (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> s (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2 20 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5  0  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14  5  0  0  0 14  0  0 12]\n",
      " [ 0  0  5  0  0  0 14  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgn (KO)\n",
      "sentence (reversed: ecnetnes) -> ecnetee (KO)\n",
      "that (reversed: taht) -> tah (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dttalnnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollo (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [14 14 12  9 18 18 20  1 19 12]\n",
      " [14 15  0  0  1 18  1  0 19  0]\n",
      " [ 5  0  0  0 14  2 19  0 19  0]\n",
      " [18  0  0  0 18  0  0  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hnnerf (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fi (KO)\n",
      "renard (reversed: draner) -> dranrr (KO)\n",
      "brun (reversed: nurb) -> nrrb (KO)\n",
      "saute (reversed: etuas) -> etas (KO)\n",
      "par (reversed: rap) -> ra (KO)\n",
      "dessus (reversed: sussed) -> sssse (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4  5]\n",
      " [ 9  5 18  0  1 12 14 14 18  5]\n",
      " [ 9  5  5  0  0  5 12 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0 18  0 18  0  5]\n",
      " [ 0  5  0  0  0 20  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> niihc (KO)\n",
      "paresseux (reversed: xuesserap) -> xeessera (KO)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> s (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemrtte (KO)\n",
      "long (reversed: gnol) -> gnll (KO)\n",
      "french (reversed: hcnerf) -> hnnerf (KO)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> eeeeeeennnnttttttsnocitna (KO)\n",
      "* correct: 15/40 -> 37.5%\n",
      "\n",
      "* step: 3001 loss: 0.106323\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 5  3 23 24 16  5  5 26  7  9]\n",
      " [20  9 15  6 13 22 20  1 15  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eet (KO)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xxf (KO)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eet (KO)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> ggo (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  5 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  6]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eet (KO)\n",
      "following (reversed: gniwollof) -> gniwolloff (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0 12]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeellennnoitutitsnocitna (KO)\n",
      "* correct: 33/40 -> 82.5%\n",
      "\n",
      "* step: 4001 loss: 0.0975575\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18 20]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tteeenneennotutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 5001 loss: 0.107018\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23 12]\n",
      " [ 3 19  0  0  0  5  0 18  0 12]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teellllnnnoitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 6001 loss: 1.53042\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 5  3 23 15 16 18  5 25  7  0]\n",
      " [ 5  3 23 15 16  5  5 26  6  0]\n",
      " [20  9 15  6 13  5 20 26  4  0]\n",
      " [ 0  9 15  0 13 15  0 12  0  0]\n",
      " [ 0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eeet (KO)\n",
      "quick (reversed: kciuq) -> kccii (KO)\n",
      "brown (reversed: nworb) -> nwwoob (KO)\n",
      "fox (reversed: xof) -> xoof (KO)\n",
      "jumps (reversed: spmuj) -> sppmm (KO)\n",
      "over (reversed: revo) -> rreeo (KO)\n",
      "the (reversed: eht) -> eeet (KO)\n",
      "lazy (reversed: yzal) -> yyzzl (KO)\n",
      "dog (reversed: god) -> ggfd (KO)\n",
      "is (reversed: si) -> s (KO)\n",
      "[[ 6  8  5 20 14  5  4 20  5  7]\n",
      " [ 1  9  5 20  6  0  5 20  5 14]\n",
      " [ 0  9 14  1 14  0 20  0  5  9]\n",
      " [ 0 12 14  1  3  0  1  0 20 23]\n",
      " [ 0  7 20 20  0  0 12  0  0 15]\n",
      " [ 0  7 20  0  0  0 12  0  0 12]\n",
      " [ 0  7 14  0  0  0 14  0  0 12]\n",
      " [ 0  5 14  0  0  0 14  0  0 15]\n",
      " [ 0  0  0  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> fa (KO)\n",
      "english (reversed: hsilgne) -> hiilggge (KO)\n",
      "sentence (reversed: ecnetnes) -> eennttnn (KO)\n",
      "that (reversed: taht) -> ttaat (KO)\n",
      "can (reversed: nac) -> nfnc (KO)\n",
      "be (reversed: eb) -> e (KO)\n",
      "translated (reversed: detalsnart) -> detallnnaa (KO)\n",
      "to (reversed: ot) -> tt (KO)\n",
      "the (reversed: eht) -> eeet (KO)\n",
      "following (reversed: gniwollof) -> gniwollol (KO)\n",
      "[[ 8  5 20  6  4 14  5 18 19 20]\n",
      " [ 3  5  0  6 18 14 20 18 21  0]\n",
      " [14 14  0  6  1 21 20 18 19  0]\n",
      " [14 15  0 22  1 21 21 16 19  0]\n",
      " [ 5  0  0  0 14  2 21  0 19  0]\n",
      " [ 5  0  0  0 14  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnnee (KO)\n",
      "one (reversed: eno) -> eeno (KO)\n",
      "le (reversed: el) -> t (KO)\n",
      "vif (reversed: fiv) -> fffv (KO)\n",
      "renard (reversed: draner) -> draann (KO)\n",
      "brun (reversed: nurb) -> nnuub (KO)\n",
      "saute (reversed: etuas) -> ettuu (KO)\n",
      "par (reversed: rap) -> rrrp (KO)\n",
      "dessus (reversed: sussed) -> sussssd (KO)\n",
      "le (reversed: el) -> t (KO)\n",
      "[[14 24  5 19  6 25  7  8  4 15]\n",
      " [ 5 21  5  0  1 12  7  3  4 15]\n",
      " [ 5  5  5  0  0 12 14 14 18  5]\n",
      " [ 9 19 18  0  0 13 14 14 18  5]\n",
      " [ 9 19  8  0  0  5 12  5 23  5]\n",
      " [ 3  5  0  0  0 18  0  5  0 12]\n",
      " [ 0  5  0  0  0 18  0  0  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  0  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neeiic (KO)\n",
      "paresseux (reversed: xuesserap) -> xuesseerr (KO)\n",
      "here (reversed: ereh) -> eeerh (KO)\n",
      "is (reversed: si) -> s (KO)\n",
      "an (reversed: na) -> fa (KO)\n",
      "extremely (reversed: ylemertxe) -> yllmerrtte (KO)\n",
      "long (reversed: gnol) -> ggnnl (KO)\n",
      "french (reversed: hcnerf) -> hcnnee (KO)\n",
      "word (reversed: drow) -> ddrrw (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ooeeellnenoitutitnnocittt (KO)\n",
      "* correct: 0/40 -> 0.0%\n",
      "\n",
      "* step: 7001 loss: 0.208758\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26  4  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18 15]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0 13]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> toeemeeenniitutitsnocitna (KO)\n",
      "* correct: 38/40 -> 95.0%\n",
      "\n",
      "* step: 8001 loss: 0.161006\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26  4  9]\n",
      " [ 8  9 15  6 13 22  8  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  1  5  5 20  8 14]\n",
      " [ 0  9 14  8  1  0 20  0  8  9]\n",
      " [ 0 12  5  8  3  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> tahh (KO)\n",
      "can (reversed: nac) -> naac (KO)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20 16 21 12]\n",
      " [14 14  0  9  1 18 21 16 19  0]\n",
      " [ 5  0  0 22 14 18  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> enn (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiiv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurr (KO)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rpp (KO)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9 14 12 14  3 18 20]\n",
      " [ 9  5  5  0  0  5 15 14 23  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 8 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihh (KO)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drww (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tteeellennoeutittsnocitna (KO)\n",
      "* correct: 24/40 -> 60.0%\n",
      "\n",
      "* step: 9001 loss: 0.399558\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeelennnitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 10001 loss: 0.326533\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeeeeenoitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 11001 loss: 0.115472\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeleennoitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 12001 loss: 0.128829\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15 20]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 12]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teteeellnnoitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 13001 loss: 0.0944806\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeelennoitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 14001 loss: 0.11396\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeleennnitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "*** evaluation! loss: 0.0616816\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0 12]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeelleennnitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "CPU times: user 1h 53min 37s, sys: 2h 4min 37s, total: 3h 58min 14s\n",
      "Wall time: 59min 5s\n"
     ]
    }
   ],
   "source": [
    "%time reverse_text(15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another try with a higher number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* step: 1 loss: 3.28237\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "the (reversed: eht) ->  (KO)\n",
      "quick (reversed: kciuq) ->  (KO)\n",
      "brown (reversed: nworb) ->  (KO)\n",
      "fox (reversed: xof) ->  (KO)\n",
      "jumps (reversed: spmuj) ->  (KO)\n",
      "over (reversed: revo) ->  (KO)\n",
      "the (reversed: eht) ->  (KO)\n",
      "lazy (reversed: yzal) ->  (KO)\n",
      "dog (reversed: god) ->  (KO)\n",
      "is (reversed: si) ->  (KO)\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "an (reversed: na) ->  (KO)\n",
      "english (reversed: hsilgne) ->  (KO)\n",
      "sentence (reversed: ecnetnes) ->  (KO)\n",
      "that (reversed: taht) ->  (KO)\n",
      "can (reversed: nac) ->  (KO)\n",
      "be (reversed: eb) ->  (KO)\n",
      "translated (reversed: detalsnart) ->  (KO)\n",
      "to (reversed: ot) ->  (KO)\n",
      "the (reversed: eht) ->  (KO)\n",
      "following (reversed: gniwollof) ->  (KO)\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "french (reversed: hcnerf) ->  (KO)\n",
      "one (reversed: eno) ->  (KO)\n",
      "le (reversed: el) ->  (KO)\n",
      "vif (reversed: fiv) ->  (KO)\n",
      "renard (reversed: draner) ->  (KO)\n",
      "brun (reversed: nurb) ->  (KO)\n",
      "saute (reversed: etuas) ->  (KO)\n",
      "par (reversed: rap) ->  (KO)\n",
      "dessus (reversed: sussed) ->  (KO)\n",
      "le (reversed: el) ->  (KO)\n",
      "[[0 0 0 0 0 0 0 0 0 9]\n",
      " [0 0 0 0 0 0 0 0 0 9]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "chien (reversed: neihc) ->  (KO)\n",
      "paresseux (reversed: xuesserap) ->  (KO)\n",
      "here (reversed: ereh) ->  (KO)\n",
      "is (reversed: si) ->  (KO)\n",
      "an (reversed: na) ->  (KO)\n",
      "extremely (reversed: ylemertxe) ->  (KO)\n",
      "long (reversed: gnol) ->  (KO)\n",
      "french (reversed: hcnerf) ->  (KO)\n",
      "word (reversed: drow) ->  (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ii (KO)\n",
      "* correct: 0/40 -> 0.0%\n",
      "\n",
      "* step: 1001 loss: 1.43771\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 15 24 16  5  8 25  4  0]\n",
      " [ 8 21 15  6 16  5  8 12  4  0]\n",
      " [ 0 17  2  0 21 15  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "quick (reversed: kciuq) -> kcuq (KO)\n",
      "brown (reversed: nworb) -> noob (KO)\n",
      "fox (reversed: xof) -> xxf (KO)\n",
      "jumps (reversed: spmuj) -> sppu (KO)\n",
      "over (reversed: revo) -> reeo (KO)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "lazy (reversed: yzal) -> yyl (KO)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> s (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1  8  5 20  1  2 20 20  8  7]\n",
      " [ 0  8  5 20  1  0  1  0  8  9]\n",
      " [ 0  8  5 20  0  0  1  0  0 12]\n",
      " [ 0  5  5  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0 18  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hhhhe (KO)\n",
      "sentence (reversed: ecnetnes) -> eeeee (KO)\n",
      "that (reversed: taht) -> tttt (KO)\n",
      "can (reversed: nac) -> naa (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dtaaaaaar (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "following (reversed: gniwollof) -> ggillll (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 8 15  0 22 18 14 21  1 19  0]\n",
      " [ 8 15  0  0 18  2 21  1 19  0]\n",
      " [ 5  0  0  0  1  0 19  0 19  0]\n",
      " [ 6  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hhhef (KO)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> e (KO)\n",
      "vif (reversed: fiv) -> fv (KO)\n",
      "renard (reversed: draner) -> drra (KO)\n",
      "brun (reversed: nurb) -> nnb (KO)\n",
      "saute (reversed: etuas) -> euus (KO)\n",
      "par (reversed: rap) -> raa (KO)\n",
      "dessus (reversed: sussed) -> ssss (KO)\n",
      "le (reversed: el) -> e (KO)\n",
      "[[14 24  5 19 14 25  7  8  4  5]\n",
      " [ 5 19  5  0  1 25 14  8 18  5]\n",
      " [ 8 19  5  0  0  5 15  8 23  5]\n",
      " [ 8 19  8  0  0  5  0  5  0 14]\n",
      " [ 0 19  0  0  0  5  0  6  0 14]\n",
      " [ 0  1  0  0  0  5  0  0  0  5]\n",
      " [ 0  1  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> nehh (KO)\n",
      "paresseux (reversed: xuesserap) -> xssssaa (KO)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> s (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yyeeee (KO)\n",
      "long (reversed: gnol) -> gno (KO)\n",
      "french (reversed: hcnerf) -> hhhef (KO)\n",
      "word (reversed: drow) -> drw (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> eeennennnntttttttooonnnn (KO)\n",
      "* correct: 4/40 -> 10.0%\n",
      "\n",
      "* step: 2001 loss: 0.861111\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23  6 16  5  8 25 15  9]\n",
      " [20  9 15  6 13 22 20  1  0  0]\n",
      " [ 0 17 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciqq (KO)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xff (KO)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yyal (KO)\n",
      "dog (reversed: god) -> go (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0  6]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollff (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 15 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [ 6  0  0  0 18  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcneff (KO)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> dranrr (KO)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21  5  9 14 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 23 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0  6  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcneff (KO)\n",
      "word (reversed: drow) -> drww (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnneeeeeennntttitnocitna (KO)\n",
      "* correct: 26/40 -> 65.0%\n",
      "\n",
      "* step: 3001 loss: 0.350217\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9  1 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 12]\n",
      " [ 0 16  0  0  0  5  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnnnelllnnoiutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 4001 loss: 0.156295\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9 14 12 14  3 18 20]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ttnnnneeenootutitsnocitna (KO)\n",
      "* correct: 37/40 -> 92.5%\n",
      "\n",
      "* step: 5001 loss: 0.213631\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnnnnntiitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 6001 loss: 0.133737\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 25  4  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yyal (KO)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9 14 25 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 12]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> yyemertxe (KO)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tneennnlnnnitutitsnocitna (KO)\n",
      "* correct: 34/40 -> 85.0%\n",
      "\n",
      "* step: 7001 loss: 1.41399\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 19  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> ssmuj (KO)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2 20 20  8  7]\n",
      " [ 0  9 14  8  3  0  5  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dtealsnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> ggiwollof (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 8 14 12  9  4 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hhnerf (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> ddaner (KO)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 25 14  8 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0 12]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yyemertxe (KO)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hhnerf (KO)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeeelnnoottnottnocitna (KO)\n",
      "* correct: 32/40 -> 80.0%\n",
      "\n",
      "* step: 8001 loss: 0.00497256\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15 19]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  1  5  4 15  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> ddtalsnart (KO)\n",
      "to (reversed: ot) -> oo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14  5  9  4 21 20  1 21  5]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> ddaner (KO)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18 19 14 12 14  3 15  5]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> doow (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teneellnnnoitutitsnocitna (KO)\n",
      "* correct: 28/40 -> 70.0%\n",
      "\n",
      "* step: 9001 loss: 0.344096\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [20  3 23 15 16  5 20 26 15 19]\n",
      " [20  9 15  0 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ett (KO)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xo (KO)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  3  5 20 20 20 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> dttalsnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14  5  9  4 21 20  1 21  5]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> ddaner (KO)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18 19  1 12 14  3 18  3]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ncnnnnnnntntioeeesnocitna (KO)\n",
      "* correct: 27/40 -> 67.5%\n",
      "\n",
      "* step: 10001 loss: 0.120553\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15 19]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  3  5  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14  5  9 18 21 20  1 21  5]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18 19 14 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnneellnnnoitutitsnocitna (KO)\n",
      "* correct: 31/40 -> 77.5%\n",
      "\n",
      "* step: 11001 loss: 0.473314\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23  6 19  5  8  1 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xff (KO)\n",
      "jumps (reversed: spmuj) -> ssmuj (KO)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yaal (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  8 14  2 20 20  8  9]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> thht (KO)\n",
      "can (reversed: nac) -> nnc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dttalsnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> giiwollof (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12 22 18 21 20 16 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fvv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rpp (KO)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9  1  5 15  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yeemertxe (KO)\n",
      "long (reversed: gnol) -> gool (KO)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnnnnnnntottttttsnocitna (KO)\n",
      "* correct: 28/40 -> 70.0%\n",
      "\n",
      "* step: 12001 loss: 0.129929\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 19  5  8 26 15 19]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> ssmuj (KO)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1 14  5 20 15  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nnc (KO)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> dttalsnart (KO)\n",
      "to (reversed: ot) -> oo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12 22 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fvv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18 19 14 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnllnnnoitutitsnocitna (KO)\n",
      "* correct: 29/40 -> 72.5%\n",
      "\n",
      "* step: 13001 loss: 0.0944682\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26  4  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 15 12  9 18 18 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nrrb (KO)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 24 18  9  1 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xxesserap (KO)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nneeellnnnoitttitsnocitna (KO)\n",
      "* correct: 35/40 -> 87.5%\n",
      "\n",
      "* step: 14001 loss: 0.133721\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  3  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 15 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21  5  9 14 12 14  3 18  9]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tineellnnnoitttitsnocitna (KO)\n",
      "* correct: 34/40 -> 85.0%\n",
      "\n",
      "* step: 15001 loss: 0.400081\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [20  3 23  6 16  5 20  1 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ett (KO)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xff (KO)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "lazy (reversed: yzal) -> yaal (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  3  2  5 20 20 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 15 12 22 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fvv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9  1 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnnnnnnnnottttitsnocitna (KO)\n",
      "* correct: 31/40 -> 77.5%\n",
      "\n",
      "* step: 16001 loss: 0.297925\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [20  3 23  6 16  5 20 26  4  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ett (KO)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xff (KO)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  8  3  2  5 20 20 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> thht (KO)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 15 12  6 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> ffv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 15  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gool (KO)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnnnnnnoitttitsnocitna (KO)\n",
      "* correct: 29/40 -> 72.5%\n",
      "\n",
      "* step: 17001 loss: 0.220593\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26  4  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  3  2  5 15  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> oo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12 22 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fvv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> neeeeeeenntitttitsnocitna (KO)\n",
      "* correct: 35/40 -> 87.5%\n",
      "\n",
      "* step: 18001 loss: 0.0736202\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26  7  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> ggd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1 14  5  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nnc (KO)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3  5 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eeo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9  1 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnnnnnnnntttutitsnocitna (KO)\n",
      "* correct: 35/40 -> 87.5%\n",
      "\n",
      "* step: 19001 loss: 0.692013\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 25 15 19]\n",
      " [ 5  9 15 24 13 18  5 26  7  0]\n",
      " [ 0 21 18  0 21 22  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ehe (KO)\n",
      "quick (reversed: kciuq) -> kciu (KO)\n",
      "brown (reversed: nworb) -> nwor (KO)\n",
      "fox (reversed: xof) -> xox (KO)\n",
      "jumps (reversed: spmuj) -> spmu (KO)\n",
      "over (reversed: revo) -> rerv (KO)\n",
      "the (reversed: eht) -> ehe (KO)\n",
      "lazy (reversed: yzal) -> yyzl (KO)\n",
      "dog (reversed: god) -> gog (KO)\n",
      "is (reversed: si) -> ss (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3 20  3  2  5 20  8 14]\n",
      " [ 0  9 14  1 14  0 20  0  5  9]\n",
      " [ 0 12  5  8  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> ttah (KO)\n",
      "can (reversed: nac) -> ncn (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ehe (KO)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14  5  9 18 14 20 16 21  5]\n",
      " [14  5  0  0  1 21 21 18 19  0]\n",
      " [ 5  0  0  0 14 18  1  0 19  0]\n",
      " [18  0  0  0 18  0  0  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> ene (KO)\n",
      "le (reversed: el) -> ee (KO)\n",
      "vif (reversed: fiv) -> fi (KO)\n",
      "renard (reversed: draner) -> dranrr (KO)\n",
      "brun (reversed: nurb) -> nnur (KO)\n",
      "saute (reversed: etuas) -> etua (KO)\n",
      "par (reversed: rap) -> rpr (KO)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21  5 19 14  5  7  3  4 14]\n",
      " [ 9  5 18  0  0  5 14 14 18 14]\n",
      " [ 8 19  5  0  0 13 12  5 23 14]\n",
      " [ 0 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0 16  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neih (KO)\n",
      "paresseux (reversed: xuesserap) -> xuesserpp (KO)\n",
      "here (reversed: ereh) -> eere (KO)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> yeemertxe (KO)\n",
      "long (reversed: gnol) -> ggnl (KO)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> ddrw (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnnnnnnnntttttitsnocitna (KO)\n",
      "* correct: 9/40 -> 22.5%\n",
      "\n",
      "* step: 20001 loss: 0.2753\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 15 15 16 18  8 26  7 19]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> noorb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> rrvo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> ggd (KO)\n",
      "is (reversed: si) -> ss (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 18  3 20 14  2 20 20  8  9]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0 14 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hrilnne (KO)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> ttht (KO)\n",
      "can (reversed: nac) -> nnc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dttalsnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> giiwollof (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14  5  6 18 14 20  1 21  5]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "vif (reversed: fiv) -> ffv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nnrb (KO)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21  5 19 14 12  7  3  4 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 15  5 15 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> ggoo (KO)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> ddoo (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnnnnnnnnituitsnocitna (KO)\n",
      "* correct: 20/40 -> 50.0%\n",
      "\n",
      "* step: 21001 loss: 0.537284\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 24 16 18  8 25 15  9]\n",
      " [20  9 15 15 13  5 20  1  4  0]\n",
      " [ 0 21 18  6 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xxof (KO)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> rreo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yyal (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1  8  3 20  1  2  4 20  8  9]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hhilgne (KO)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> ttht (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> ddtalsnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> giiwollof (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14  5  9 18 14 20  1 21  5]\n",
      " [ 5 15  0 22  1 18 21  1 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hceerf (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nnrb (KO)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> raa (KO)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 24  5  9  1  5  7  3 15  5]\n",
      " [ 9  5  5  0  0  5 15  5 18 14]\n",
      " [ 8 19  8  0  0 13 12  5 15  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xxesserap (KO)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yeemertxe (KO)\n",
      "long (reversed: gnol) -> ggol (KO)\n",
      "french (reversed: hcnerf) -> hceerf (KO)\n",
      "word (reversed: drow) -> doro (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teneennnniittutitsnocitna (KO)\n",
      "* correct: 21/40 -> 52.5%\n",
      "\n",
      "* step: 22001 loss: 1.50433\n",
      "[[ 8  9 15 24 19 18  8 25  7 19]\n",
      " [ 8 17 15 15 19  5  8 26 15  9]\n",
      " [ 8 17 15 15 10  5  8  1  4  0]\n",
      " [ 0 17 15  0 10 15  0  1  0  0]\n",
      " [ 0 17 15  0 16  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> hhh (KO)\n",
      "quick (reversed: kciuq) -> iqqqq (KO)\n",
      "brown (reversed: nworb) -> ooooo (KO)\n",
      "fox (reversed: xof) -> xoo (KO)\n",
      "jumps (reversed: spmuj) -> ssjjp (KO)\n",
      "over (reversed: revo) -> reeo (KO)\n",
      "the (reversed: eht) -> hhh (KO)\n",
      "lazy (reversed: yzal) -> yzaa (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[ 1  8  5 20  1  5 12 20  8 15]\n",
      " [ 1  8  5 20  1  5 20 20  8 15]\n",
      " [ 0  9  5 20  1  0  1  0  8 15]\n",
      " [ 0  5  5 20  0  0  1  0  0 15]\n",
      " [ 0  5  5  0  0  0  1  0  0 15]\n",
      " [ 0  8  5  0  0  0  1  0  0 15]\n",
      " [ 0  5  5  0  0  0  1  0  0 15]\n",
      " [ 0  0  5  0  0  0 20  0  0 15]\n",
      " [ 0  0  0  0  0  0 20  0  0 15]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> aa (KO)\n",
      "english (reversed: hsilgne) -> hhieehe (KO)\n",
      "sentence (reversed: ecnetnes) -> eeeeeeee (KO)\n",
      "that (reversed: taht) -> tttt (KO)\n",
      "can (reversed: nac) -> aaa (KO)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> ltaaaaattt (KO)\n",
      "to (reversed: ot) -> tt (KO)\n",
      "the (reversed: eht) -> hhh (KO)\n",
      "following (reversed: gniwollof) -> ooooooooo (KO)\n",
      "[[ 6 14  5  6 18 14 20 18 19  5]\n",
      " [ 8  5 12  6 18 21 20 18 19 12]\n",
      " [ 6  5  0  6 18 21 19 18 19  0]\n",
      " [ 6  0  0  0 18  2 20  0 19  0]\n",
      " [ 6  0  0  0 18  0 20  0 19  0]\n",
      " [ 5  0  0  0 18  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> fhfffe (KO)\n",
      "one (reversed: eno) -> nee (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fff (KO)\n",
      "renard (reversed: draner) -> rrrrrr (KO)\n",
      "brun (reversed: nurb) -> nuub (KO)\n",
      "saute (reversed: etuas) -> ttstt (KO)\n",
      "par (reversed: rap) -> rrr (KO)\n",
      "dessus (reversed: sussed) -> ssssss (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[ 5 24 18 19  1  5 15  6  4 14]\n",
      " [ 5 19  5  9  1  5 15  8  4 20]\n",
      " [ 5 19  5  0  0  5 15  6  4  9]\n",
      " [ 5 19  5  0  0  5 15  6  4  9]\n",
      " [ 5 19  0  0  0  5  0  6  0 20]\n",
      " [ 0 19  0  0  0  5  0  5  0 20]\n",
      " [ 0 19  0  0  0  5  0  0  0 20]\n",
      " [ 0 19  0  0  0  5  0  0  0  9]\n",
      " [ 0 19  0  0  0  5  0  0  0  9]\n",
      " [ 0  0  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> eeeee (KO)\n",
      "paresseux (reversed: xuesserap) -> xssssssss (KO)\n",
      "here (reversed: ereh) -> reee (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> aa (KO)\n",
      "extremely (reversed: ylemertxe) -> eeeeeeeeee (KO)\n",
      "long (reversed: gnol) -> oooo (KO)\n",
      "french (reversed: hcnerf) -> fhfffe (KO)\n",
      "word (reversed: drow) -> dddd (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ntiitttiinnnnnnnnnnntttttt (KO)\n",
      "* correct: 5/40 -> 12.5%\n",
      "\n",
      "* step: 23001 loss: 2.37942\n",
      "[[20  9 14 24 21 18 20 25  7 19]\n",
      " [ 8  3  2 15 13  5  8  1 15  9]\n",
      " [ 8 21  2  6 10 22  8  1  4  0]\n",
      " [ 0 21 18  0 10 22  0 26  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> thh (KO)\n",
      "quick (reversed: kciuq) -> icuuq (KO)\n",
      "brown (reversed: nworb) -> nbbrb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> umjjj (KO)\n",
      "over (reversed: revo) -> revv (KO)\n",
      "the (reversed: eht) -> thh (KO)\n",
      "lazy (reversed: yzal) -> yaaz (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15 20  7]\n",
      " [ 1 19  5 20  1  2 20 20  8 14]\n",
      " [ 0  9  5 20  1  0 20  0  8 23]\n",
      " [ 0 14  5 20  0  0 12  0  0 12]\n",
      " [ 0 19  5  0  0  0 20  0  0 12]\n",
      " [ 0 14  5  0  0  0 20  0  0 12]\n",
      " [ 0 14  5  0  0  0  1  0  0 12]\n",
      " [ 0  0  5  0  0  0 20  0  0 12]\n",
      " [ 0  0  0  0  0  0 20  0  0 12]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsinsnn (KO)\n",
      "sentence (reversed: ecnetnes) -> eeeeeeee (KO)\n",
      "that (reversed: taht) -> tttt (KO)\n",
      "can (reversed: nac) -> naa (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dttlttattt (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> thh (KO)\n",
      "following (reversed: gniwollof) -> gnwllllll (KO)\n",
      "[[13  5  5  6 10 14 20 18 19  5]\n",
      " [ 5  5 12  6 18 21 19 16 19 12]\n",
      " [ 5 15  0  6 18 21 21 16 19  0]\n",
      " [ 5  0  0  0 18 21 19  0 19  0]\n",
      " [ 5  0  0  0 18  0 19  0 19  0]\n",
      " [ 5  0  0  0 18  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> meeeee (KO)\n",
      "one (reversed: eno) -> eeo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fff (KO)\n",
      "renard (reversed: draner) -> jrrrrr (KO)\n",
      "brun (reversed: nurb) -> nuuu (KO)\n",
      "saute (reversed: etuas) -> tsuss (KO)\n",
      "par (reversed: rap) -> rpp (KO)\n",
      "dessus (reversed: sussed) -> ssssss (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 21 18 19 14 25  7 13 23 14]\n",
      " [ 5 21  5  9  1  5 14  5  4  5]\n",
      " [ 5 19  5  0  0  5 14  5 18  5]\n",
      " [ 8 19  5  0  0  5 14  5 18 20]\n",
      " [ 5 19  0  0  0  5  0  5  0 20]\n",
      " [ 0 19  0  0  0  5  0  5  0 14]\n",
      " [ 0 19  0  0  0  5  0  0  0 20]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neehe (KO)\n",
      "paresseux (reversed: xuesserap) -> uusssssee (KO)\n",
      "here (reversed: ereh) -> reee (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yeeeeeeee (KO)\n",
      "long (reversed: gnol) -> gnnn (KO)\n",
      "french (reversed: hcnerf) -> meeeee (KO)\n",
      "word (reversed: drow) -> wdrr (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> neettntntttttttnttttttttn (KO)\n",
      "* correct: 10/40 -> 25.0%\n",
      "\n",
      "* step: 24001 loss: 1.72145\n",
      "[[ 5 11 15 24 21 18  5 25  7 19]\n",
      " [ 8  9 23 15 16 22  8 26  4  9]\n",
      " [ 8 21 23  6 13 22  8 12  4  0]\n",
      " [ 0  3  2  0 13 15  0 12  0  0]\n",
      " [ 0 21  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "quick (reversed: kciuq) -> kiucu (KO)\n",
      "brown (reversed: nworb) -> owwbb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> upmmj (KO)\n",
      "over (reversed: revo) -> rvvo (KO)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "lazy (reversed: yzal) -> yzll (KO)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5 25 15  5  7]\n",
      " [ 1 19  5 20  3  2  5 20  8 15]\n",
      " [ 0  4  5  8  3  0 13  0  8  9]\n",
      " [ 0 19  5 20  0  0 12  0  0  9]\n",
      " [ 0 19  5  0  0  0 12  0  0 12]\n",
      " [ 0 19  5  0  0  0 12  0  0 12]\n",
      " [ 0 12  5  0  0  0 12  0  0 12]\n",
      " [ 0  0  5  0  0  0 20  0  0 12]\n",
      " [ 0  0  0  0  0  0 20  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsdsssl (KO)\n",
      "sentence (reversed: ecnetnes) -> eeeeeeee (KO)\n",
      "that (reversed: taht) -> ttht (KO)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> yemlllltta (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "following (reversed: gniwollof) -> goiilllll (KO)\n",
      "[[ 8  5  5 22  4 14  5 18 19  5]\n",
      " [ 5 14 12 22 18 21 20  1 19 12]\n",
      " [ 5 15  0 22 18 21 21 16 19  0]\n",
      " [ 5  0  0  0 18 21 19  0 19  0]\n",
      " [ 5  0  0  0 18  0 19  0 19  0]\n",
      " [ 6  0  0  0 18  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> heeeef (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> vvv (KO)\n",
      "renard (reversed: draner) -> drrrrr (KO)\n",
      "brun (reversed: nurb) -> nuuu (KO)\n",
      "saute (reversed: etuas) -> etuss (KO)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> ssssss (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25 15  8  4 14]\n",
      " [ 8 21 18  9  1  5 14  5 15  5]\n",
      " [ 8  5  8  0  0  5 12  5 23  5]\n",
      " [ 8  5  5  0  0  5 12  5 18  5]\n",
      " [ 8  5  0  0  0  5  0  5  0 14]\n",
      " [ 0  5  0  0  0  5  0  6  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0  5]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> nhhhh (KO)\n",
      "paresseux (reversed: xuesserap) -> xueeeeeee (KO)\n",
      "here (reversed: ereh) -> erhe (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yeeeeeeee (KO)\n",
      "long (reversed: gnol) -> onll (KO)\n",
      "french (reversed: hcnerf) -> heeeef (KO)\n",
      "word (reversed: drow) -> dowr (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> neeennnennnntnnnnnnnnnnnn (KO)\n",
      "* correct: 11/40 -> 27.5%\n",
      "\n",
      "* step: 25001 loss: 1.69425\n",
      "[[ 5 11 15 24 19 18  5 25  7 19]\n",
      " [ 8  3  2 15 13  5  8 26 15  9]\n",
      " [ 8  9 18  6 13 22  8 12  4  0]\n",
      " [ 0 21 18  0 10 15  0 12  0  0]\n",
      " [ 0 21  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "quick (reversed: kciuq) -> kciuu (KO)\n",
      "brown (reversed: nworb) -> obrrb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> smmjj (KO)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "lazy (reversed: yzal) -> yzll (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1  3  3  1  1  2  5 20  8  9]\n",
      " [ 0 23  5  8  3  0 20  0  8 15]\n",
      " [ 0  7 14  8  0  0 20  0  0 12]\n",
      " [ 0  5  5  0  0  0 20  0  0 12]\n",
      " [ 0  5  5  0  0  0 20  0  0 12]\n",
      " [ 0  8  5  0  0  0 20  0  0 12]\n",
      " [ 0  0  5  0  0  0 20  0  0 15]\n",
      " [ 0  0  0  0  0  0 20  0  0 12]\n",
      " [ 0  0  0  0  0  0 12  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hcwgeeh (KO)\n",
      "sentence (reversed: ecnetnes) -> eceneeee (KO)\n",
      "that (reversed: taht) -> tahh (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detttttttl (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "following (reversed: gniwollof) -> giollllol (KO)\n",
      "[[ 8  5  5 22  4 14  5 18 19  5]\n",
      " [ 3 14 12 22 18 21 20  1 19 12]\n",
      " [ 6 15  0 22 18 21 19  1 19  0]\n",
      " [14  0  0  0 18  2 19  0 19  0]\n",
      " [ 6  0  0  0 18  0 19  0 19  0]\n",
      " [ 6  0  0  0 18  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcfnff (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> vvv (KO)\n",
      "renard (reversed: draner) -> drrrrr (KO)\n",
      "brun (reversed: nurb) -> nuub (KO)\n",
      "saute (reversed: etuas) -> etsss (KO)\n",
      "par (reversed: rap) -> raa (KO)\n",
      "dessus (reversed: sussed) -> ssssss (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 8  5  5  9  1 13 14  3 18 14]\n",
      " [ 8  5  5  0  0  5 15  6 15 14]\n",
      " [ 8  5  8  0  0  5 12 14 15 14]\n",
      " [ 8  5  0  0  0  5  0  6  0 14]\n",
      " [ 0  5  0  0  0  5  0  6  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> nhhhh (KO)\n",
      "paresseux (reversed: xuesserap) -> xeeeeeeee (KO)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ymeeeeeee (KO)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcfnff (KO)\n",
      "word (reversed: drow) -> droo (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnnnnnnnnnnnnnnnnntttnnn (KO)\n",
      "* correct: 14/40 -> 35.0%\n",
      "\n",
      "* step: 26001 loss: 1.55785\n",
      "[[ 5 11 15 24 19 18  5 25  7 19]\n",
      " [ 8  3  2 15 16  5  8 26  4  9]\n",
      " [ 8  3 18  6 13 22  8  1 15  0]\n",
      " [ 0  3  2  0 10  5  0 12  0  0]\n",
      " [ 0 21 18  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "quick (reversed: kciuq) -> kcccu (KO)\n",
      "brown (reversed: nworb) -> obrbr (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmjj (KO)\n",
      "over (reversed: revo) -> reve (KO)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> gdo (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1  3  3  8  1  2  5 20  8 12]\n",
      " [ 0  9  5  1  3  0 12  0  8  9]\n",
      " [ 0 19 14  8  0  0 12  0  0 15]\n",
      " [ 0  5  5  0  0  0 20  0  0 12]\n",
      " [ 0 14  5  0  0  0 20  0  0 15]\n",
      " [ 0 12  5  0  0  0 20  0  0 12]\n",
      " [ 0  0  5  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0 20  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hcisenl (KO)\n",
      "sentence (reversed: ecnetnes) -> eceneeee (KO)\n",
      "that (reversed: taht) -> thah (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> delltttata (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "following (reversed: gniwollof) -> gliololll (KO)\n",
      "[[ 8  5  5 22  4 14  5 18 19  5]\n",
      " [ 3 15 12  9 18 21 20  1 19 12]\n",
      " [ 6 14  0 22 18  2 19 16 19  0]\n",
      " [ 6  0  0  0  1 18 20  0 19  0]\n",
      " [ 6  0  0  0 18  0  1  0 19  0]\n",
      " [ 6  0  0  0 18  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcffff (KO)\n",
      "one (reversed: eno) -> eon (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> viv (KO)\n",
      "renard (reversed: draner) -> drrarr (KO)\n",
      "brun (reversed: nurb) -> nubr (KO)\n",
      "saute (reversed: etuas) -> etsta (KO)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> ssssss (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 21  5 19 14 25  7  8  4 20]\n",
      " [ 5  5  5  9  1  5 14  3 18 20]\n",
      " [ 9 19  8  0  0 12 15  6  4 20]\n",
      " [ 8 19  8  0  0  5 12  6 18 12]\n",
      " [ 3  5  0  0  0  5  0  6  0  5]\n",
      " [ 0  5  0  0  0  5  0  6  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0  9]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> uesseeeee (KO)\n",
      "here (reversed: ereh) -> eehh (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yeleeeeee (KO)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcffff (KO)\n",
      "word (reversed: drow) -> drdr (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tttlenintnnnnnnnnnnnnnttn (KO)\n",
      "* correct: 14/40 -> 35.0%\n",
      "\n",
      "* step: 27001 loss: 1.54408\n",
      "[[ 5 11 14 24 21 18  5 25  7 19]\n",
      " [ 8  3 23 15  3 22  8 26 15  9]\n",
      " [20  3 18  6 16 15 20  1  4  0]\n",
      " [ 0  3 18  0 21 22  0  1  0  0]\n",
      " [ 0  3  2  0 21  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kcccc (KO)\n",
      "brown (reversed: nworb) -> nwrrb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> ucpuu (KO)\n",
      "over (reversed: revo) -> rvov (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzaa (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3 20  1  2 21 20  8 20]\n",
      " [ 0  9  5  8  3  0 20  0 20  9]\n",
      " [ 0  5 14  8  0  0 12  0  0 12]\n",
      " [ 0 19  5  0  0  0 12  0  0  7]\n",
      " [ 0 14  5  0  0  0 20  0  0  9]\n",
      " [ 0 12  5  0  0  0 20  0  0  9]\n",
      " [ 0  0  5  0  0  0 20  0  0 15]\n",
      " [ 0  0  0  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 12  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsiesnl (KO)\n",
      "sentence (reversed: ecnetnes) -> eceneeee (KO)\n",
      "that (reversed: taht) -> tthh (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dutlltttal (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gtilgiioo (KO)\n",
      "[[ 8  5  5 22  4 14  5  1 19  5]\n",
      " [ 3 15 12  9 18 21 20 16 19 12]\n",
      " [ 5 15  0 22 18  2 19 16 19  0]\n",
      " [ 6  0  0  0 18 18 20  0 19  0]\n",
      " [ 6  0  0  0 18  0  1  0 19  0]\n",
      " [ 6  0  0  0 18  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcefff (KO)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> viv (KO)\n",
      "renard (reversed: draner) -> drrrrr (KO)\n",
      "brun (reversed: nurb) -> nubr (KO)\n",
      "saute (reversed: etuas) -> etsta (KO)\n",
      "par (reversed: rap) -> app (KO)\n",
      "dessus (reversed: sussed) -> ssssss (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 21  5 19 14 25  7  8  4 20]\n",
      " [ 8  5 18  9  1 12 14  3 15  5]\n",
      " [ 8 19  8  0  0 13 15  5 15  5]\n",
      " [ 8 19  8  0  0  5 12  6 23  5]\n",
      " [ 7 19  0  0  0  5  0  6  0 12]\n",
      " [ 0  5  0  0  0  5  0  6  0  5]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0 19  0  0  0  5  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> nhhhg (KO)\n",
      "paresseux (reversed: xuesserap) -> uessseees (KO)\n",
      "here (reversed: ereh) -> erhh (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylmeeeeee (KO)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcefff (KO)\n",
      "word (reversed: drow) -> doow (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeelennllooiooooinnnnnni (KO)\n",
      "* correct: 15/40 -> 37.5%\n",
      "\n",
      "* step: 28001 loss: 0.905781\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  3 18  6 13 22 20  1  4  0]\n",
      " [ 0  9  2  0 21 22  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kcciq (KO)\n",
      "brown (reversed: nworb) -> nwrbb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revv (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9  5  8  3  0 12  0 20  9]\n",
      " [ 0 12 14 20  0  0  5  0  0 23]\n",
      " [ 0 12  5  0  0  0 12  0  0 12]\n",
      " [ 0 14  5  0  0  0 12  0  0 12]\n",
      " [ 0  5 14  0  0  0  1  0  0 12]\n",
      " [ 0  0  5  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0 14  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsillne (KO)\n",
      "sentence (reversed: ecnetnes) -> eceneene (KO)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> delellaanr (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwllllo (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21  1 16 19 12]\n",
      " [ 5 15  0  9  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 18  2 21  0 19  0]\n",
      " [18  0  0  0 18  0 19  0 19  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hceerf (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fii (KO)\n",
      "renard (reversed: draner) -> drarrr (KO)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> eauus (KO)\n",
      "par (reversed: rap) -> rpp (KO)\n",
      "dessus (reversed: sussed) -> sssssd (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5  5 18  9  1 12 14  3 18 14]\n",
      " [ 8 19  5  0  0 12 14  5 18  5]\n",
      " [ 8 19  8  0  0  5 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 13  0  6  0 13]\n",
      " [ 0  5  0  0  0  5  0  0  0  5]\n",
      " [ 0 18  0  0  0  5  0  0  0 14]\n",
      " [ 0 16  0  0  0 24  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> nehhc (KO)\n",
      "paresseux (reversed: xuesserap) -> xessseerp (KO)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylleemeex (KO)\n",
      "long (reversed: gnol) -> gnnl (KO)\n",
      "french (reversed: hcnerf) -> hceerf (KO)\n",
      "word (reversed: drow) -> drrw (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nneeemennlniunnnninininnn (KO)\n",
      "* correct: 20/40 -> 50.0%\n",
      "\n",
      "* step: 29001 loss: 0.454673\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 18  6 13 22 20 12  4  0]\n",
      " [ 0 21  2  0 10 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nwrbb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmjj (KO)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzll (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9  5  8  3  0 20  0 20  9]\n",
      " [ 0 12 14  8  0  0 12  0  0 23]\n",
      " [ 0 14 14  0  0  0 12  0  0 12]\n",
      " [ 0  5 14  0  0  0 14  0  0 12]\n",
      " [ 0  5  5  0  0  0  1  0  0 15]\n",
      " [ 0  0 19  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilnee (KO)\n",
      "sentence (reversed: ecnetnes) -> ecennnes (KO)\n",
      "that (reversed: taht) -> tahh (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detllnartt (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwlloff (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 19 12]\n",
      " [ 5 15  0 22  1  2 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [ 6  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0  5  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hceeff (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> dranee (KO)\n",
      "brun (reversed: nurb) -> nubb (KO)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> ssssed (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9 14 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 12  5 23  5]\n",
      " [ 3 19  8  0  0 13 12  5 23  5]\n",
      " [ 3  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  6  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0  9]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neicc (KO)\n",
      "paresseux (reversed: xuesserap) -> xueserapp (KO)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemrtxee (KO)\n",
      "long (reversed: gnol) -> gnll (KO)\n",
      "french (reversed: hcnerf) -> hceeff (KO)\n",
      "word (reversed: drow) -> drww (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> neeeneninitittttsnoiitnaa (KO)\n",
      "* correct: 19/40 -> 47.5%\n",
      "\n",
      "*** evaluation! loss: 0.307409\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 16 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> sppuj (KO)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9  5  8  3  0  1  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> eceetnes (KO)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> deaalsnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 19 12]\n",
      " [ 5 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hceerf (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> ssssed (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9  1 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15  5 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hceerf (KO)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nneeenlniititutitsoocitna (KO)\n",
      "* correct: 33/40 -> 82.5%\n",
      "\n",
      "CPU times: user 3h 46min 57s, sys: 4h 8min 49s, total: 7h 55min 47s\n",
      "Wall time: 1h 57min 45s\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "%time reverse_text(30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Okay, let's say it nearly reversed the longest string! And the shorter strings have all been reversed pretty quickly.\n",
    "\n",
    "\"teellllnnnoitutitsnocitna\" (given at the beginning, at step 5000) is not far from \"tnemellennoitutitsnocitna\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
